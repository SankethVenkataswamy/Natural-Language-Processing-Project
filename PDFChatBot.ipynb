{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "828b865bb610413aa6fbe90f1a2c855e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2ac5fc8470947a9806b3dcbfe406335",
              "IPY_MODEL_9dc2af6bd5dd477da8dd764f0e6f474f",
              "IPY_MODEL_86fa167015894523a83e51a6dc252d15"
            ],
            "layout": "IPY_MODEL_44676d4167f84c7183f5aa6d83d6799c"
          }
        },
        "b2ac5fc8470947a9806b3dcbfe406335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba5c18c2bdd4a75aaa44985bc6eb394",
            "placeholder": "​",
            "style": "IPY_MODEL_f36cdeb6471b49c3942e2c378bfbcbaa",
            "value": "Map: 100%"
          }
        },
        "9dc2af6bd5dd477da8dd764f0e6f474f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a121d945a8d94f4fb67e3a2c3175d4fc",
            "max": 3453,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c59e291e21464c03a0251c7cff66cb12",
            "value": 3453
          }
        },
        "86fa167015894523a83e51a6dc252d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f7a8d8c51142f682f18321eff4d830",
            "placeholder": "​",
            "style": "IPY_MODEL_d4c29b4c253342498796f3ef440b8712",
            "value": " 3453/3453 [00:20&lt;00:00, 132.74 examples/s]"
          }
        },
        "44676d4167f84c7183f5aa6d83d6799c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba5c18c2bdd4a75aaa44985bc6eb394": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f36cdeb6471b49c3942e2c378bfbcbaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a121d945a8d94f4fb67e3a2c3175d4fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c59e291e21464c03a0251c7cff66cb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0f7a8d8c51142f682f18321eff4d830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c29b4c253342498796f3ef440b8712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six pdf2image pillow_heif unstructured==0.13.0 unstructured-inference==0.7.25 pikepdf unstructured_pytesseract pypdf\n"
      ],
      "metadata": {
        "id": "7d86rwUwdyJO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4630696-9ebc-4702-c113-c4d8fabe19d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Collecting pillow_heif\n",
            "  Downloading pillow_heif-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured==0.13.0\n",
            "  Downloading unstructured-0.13.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured-inference==0.7.25\n",
            "  Downloading unstructured_inference-0.7.25-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pikepdf\n",
            "  Downloading pikepdf-8.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured_pytesseract\n",
            "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff==2.2.1 (from unstructured==0.13.0)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (4.12.3)\n",
            "Requirement already satisfied: certifi==2024.2.2 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (2024.2.2)\n",
            "Requirement already satisfied: chardet==5.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (3.3.2)\n",
            "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (8.1.7)\n",
            "Collecting dataclasses-json==0.6.4 (from unstructured==0.13.0)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting dataclasses-json-speakeasy==0.5.11 (from unstructured==0.13.0)\n",
            "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
            "Collecting emoji==2.10.1 (from unstructured==0.13.0)\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filetype==1.2.0 (from unstructured==0.13.0)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting idna==3.6 (from unstructured==0.13.0)\n",
            "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib==1.3.2 (from unstructured==0.13.0)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-python==1.0.6 (from unstructured==0.13.0)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting langdetect==1.0.9 (from unstructured==0.13.0)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lxml==5.1.0 (from unstructured==0.13.0)\n",
            "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow==3.20.2 (from unstructured==0.13.0)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions==1.0.0 (from unstructured==0.13.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (3.8.1)\n",
            "Collecting numpy==1.26.4 (from unstructured==0.13.0)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==23.2 (from unstructured==0.13.0)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (2.8.2)\n",
            "Collecting python-iso639==2024.2.7 (from unstructured==0.13.0)\n",
            "  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-magic==0.4.27 (from unstructured==0.13.0)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Collecting rapidfuzz==3.6.1 (from unstructured==0.13.0)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex==2023.12.25 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (2023.12.25)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (2.31.0)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (1.16.0)\n",
            "Requirement already satisfied: soupsieve==2.5 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (2.5)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (0.9.0)\n",
            "Requirement already satisfied: tqdm==4.66.2 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.13.0) (4.66.2)\n",
            "Collecting typing-extensions==4.9.0 (from unstructured==0.13.0)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting typing-inspect==0.9.0 (from unstructured==0.13.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting unstructured-client==0.18.0 (from unstructured==0.13.0)\n",
            "  Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
            "Collecting urllib3==1.26.18 (from unstructured==0.13.0)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt==1.16.0 (from unstructured==0.13.0)\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting layoutparser[layoutmodels,tesseract] (from unstructured-inference==0.7.25)\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured-inference==0.7.25)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.25) (0.20.3)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.25) (4.8.0.76)\n",
            "Collecting onnx (from unstructured-inference==0.7.25)\n",
            "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime<1.16 (from unstructured-inference==0.7.25)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.25) (4.40.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Collecting pillow (from pdf2image)\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pikepdf)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Collecting coloredlogs (from onnxruntime<1.16->unstructured-inference==0.7.25)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.25) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.25) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.25) (1.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.25) (3.13.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.25) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.25) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.25) (0.4.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.25) (2023.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (2.0.3)\n",
            "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (0.17.1+cu121)\n",
            "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.25)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm>=0.9.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (2.0.7)\n",
            "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (2024.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Downloading pypdfium2-4.29.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.25) (1.3.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.25) (3.1.2)\n",
            "Building wheels for collected packages: langdetect, iopath, antlr4-python3-runtime\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=6c93316d984b63b29b8d21210678c8259c149e1d90e269848d17559700df8c0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=4a56e251a46cc0cf7d835b15d8ee8e53d5afe4ef61cc4b303bee03f5c9a00221\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=055f0b07b3b890a3d584e97a90a190dfb4c6766143577883c2a51f4fac86b1ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built langdetect iopath antlr4-python3-runtime\n",
            "Installing collected packages: filetype, antlr4-python3-runtime, wrapt, urllib3, typing-extensions, rapidfuzz, python-multipart, python-magic, python-iso639, pypdfium2, portalocker, pillow, packaging, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mypy-extensions, lxml, langdetect, jsonpath-python, joblib, idna, humanfriendly, emoji, backoff, unstructured_pytesseract, typing-inspect, pytesseract, pypdf, pillow_heif, pdf2image, onnx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, iopath, Deprecated, coloredlogs, pikepdf, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, dataclasses-json-speakeasy, dataclasses-json, unstructured-client, pdfplumber, unstructured, layoutparser, timm, effdet, unstructured-inference\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.9.4\n",
            "    Uninstalling lxml-4.9.4:\n",
            "      Successfully uninstalled lxml-4.9.4\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.0\n",
            "    Uninstalling joblib-1.4.0:\n",
            "      Successfully uninstalled joblib-1.4.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 antlr4-python3-runtime-4.9.3 backoff-2.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.4 dataclasses-json-speakeasy-0.5.11 effdet-0.4.1 emoji-2.10.1 filetype-1.2.0 humanfriendly-10.0 idna-3.6 iopath-0.1.10 joblib-1.3.2 jsonpath-python-1.0.6 langdetect-1.0.9 layoutparser-0.3.4 lxml-5.1.0 marshmallow-3.20.2 mypy-extensions-1.0.0 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 onnx-1.16.0 onnxruntime-1.15.1 packaging-23.2 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.0 pikepdf-8.15.1 pillow-10.3.0 pillow_heif-0.16.0 portalocker-2.8.2 pypdf-4.2.0 pypdfium2-4.29.0 pytesseract-0.3.10 python-iso639-2024.2.7 python-magic-0.4.27 python-multipart-0.0.9 rapidfuzz-3.6.1 timm-0.9.16 typing-extensions-4.9.0 typing-inspect-0.9.0 unstructured-0.13.0 unstructured-client-0.18.0 unstructured-inference-0.7.25 unstructured_pytesseract-0.3.12 urllib3-1.26.18 wrapt-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pydevd_plugins"
                ]
              },
              "id": "e65f152fa7204b5c81e9b3e89e7994d5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "# Returns a List[Element] present in the pages of the parsed pdf document\n",
        "\n",
        "elements = partition_pdf(\"/content/16.pdf\", strategy=\"fast\")\n",
        "\n",
        "\n",
        "# Applies the English and Swedish language pack for ocr. OCR is only applied\n",
        "# if the text is not available in the PDF.\n",
        "\n",
        "# elements = partition_pdf(\"example-docs/layout-parser-paper-fast.pdf\", ocr_languages=\"eng+swe\")\n",
        "# print(elements)"
      ],
      "metadata": {
        "id": "wRCX0LfqcOq-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(elements)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA9dVtSfZ99Q",
        "outputId": "206f2f38-7d08-48c7-c93f-80ac82fabbea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<unstructured.documents.elements.Header object at 0x7ab5bb548160>, <unstructured.documents.elements.Header object at 0x7ab5bb548b50>, <unstructured.documents.elements.Header object at 0x7ab5bb54a350>, <unstructured.documents.elements.Title object at 0x7ab5bb54a530>, <unstructured.documents.elements.Title object at 0x7ab5bb54a800>, <unstructured.documents.elements.Title object at 0x7ab5bb86bc40>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb54a7a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5e3790>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb54a650>, <unstructured.documents.elements.Title object at 0x7ab5bb5e1150>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5489d0>, <unstructured.documents.elements.Title object at 0x7ab5bb5e2aa0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5480a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5494e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb54bb50>, <unstructured.documents.elements.Header object at 0x7ab5bb549f30>, <unstructured.documents.elements.Title object at 0x7ab5bb8cee90>, <unstructured.documents.elements.ListItem object at 0x7ab5bb8cf520>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bdfcf880>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bba17700>, <unstructured.documents.elements.Title object at 0x7ab5bba15870>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb8cd840>, <unstructured.documents.elements.Text object at 0x7ab5bb86ba90>, <unstructured.documents.elements.Title object at 0x7ab5bba15450>, <unstructured.documents.elements.Title object at 0x7ab5bb632770>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb632980>, <unstructured.documents.elements.Title object at 0x7ab5bba17040>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bba17730>, <unstructured.documents.elements.Title object at 0x7ab5bba15420>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bba15f90>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bba153f0>, <unstructured.documents.elements.Title object at 0x7ab5bba15120>, <unstructured.documents.elements.Title object at 0x7ab5bba15510>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bba16ce0>, <unstructured.documents.elements.Title object at 0x7ab5bb7959f0>, <unstructured.documents.elements.Title object at 0x7ab5bb7c4c40>, <unstructured.documents.elements.Title object at 0x7ab5bb7c53c0>, <unstructured.documents.elements.Title object at 0x7ab5bb797e80>, <unstructured.documents.elements.Title object at 0x7ab5bb795a20>, <unstructured.documents.elements.Title object at 0x7ab5bb7971c0>, <unstructured.documents.elements.Title object at 0x7ab5bdfbfe50>, <unstructured.documents.elements.Text object at 0x7ab5bb796050>, <unstructured.documents.elements.ListItem object at 0x7ab5bb795a50>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7958d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb796dd0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb796920>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb795b10>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb795d80>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb795990>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb796f50>, <unstructured.documents.elements.Title object at 0x7ab5bb2c22f0>, <unstructured.documents.elements.ListItem object at 0x7ab5bb99f850>, <unstructured.documents.elements.Title object at 0x7ab5bb99f8e0>, <unstructured.documents.elements.Title object at 0x7ab5bb632530>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99f910>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb64d1e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb64f6a0>, <unstructured.documents.elements.Title object at 0x7ab5bb64fa90>, <unstructured.documents.elements.Title object at 0x7ab5bb7c58d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb64f9a0>, <unstructured.documents.elements.Title object at 0x7ab5bb64f760>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb64fc40>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7c5f90>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be01b340>, <unstructured.documents.elements.Title object at 0x7ab5be01b310>, <unstructured.documents.elements.Title object at 0x7ab5be10c910>, <unstructured.documents.elements.Title object at 0x7ab5be01be50>, <unstructured.documents.elements.Title object at 0x7ab5bb797cd0>, <unstructured.documents.elements.Title object at 0x7ab5bb794dc0>, <unstructured.documents.elements.Title object at 0x7ab5bb797b80>, <unstructured.documents.elements.Title object at 0x7ab5bb5e3070>, <unstructured.documents.elements.Title object at 0x7ab5bb5490c0>, <unstructured.documents.elements.Title object at 0x7ab5bb54abf0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e2230>, <unstructured.documents.elements.ListItem object at 0x7ab5bb5e2da0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb54ba30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb54ba00>, <unstructured.documents.elements.Title object at 0x7ab5bb54b970>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb54b610>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bba500d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb54b550>, <unstructured.documents.elements.Title object at 0x7ab5bb54b940>, <unstructured.documents.elements.Text object at 0x7ab5bb54b790>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb54b340>, <unstructured.documents.elements.Text object at 0x7ab5bb54a5f0>, <unstructured.documents.elements.Title object at 0x7ab5bb7978b0>, <unstructured.documents.elements.Title object at 0x7ab5bb190850>, <unstructured.documents.elements.ListItem object at 0x7ab5bb797c10>, <unstructured.documents.elements.Title object at 0x7ab5bb1910c0>, <unstructured.documents.elements.Title object at 0x7ab5bb191240>, <unstructured.documents.elements.Title object at 0x7ab5bb191e10>, <unstructured.documents.elements.Title object at 0x7ab5bb191f90>, <unstructured.documents.elements.Title object at 0x7ab5bb191990>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb797a60>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7967d0>, <unstructured.documents.elements.Text object at 0x7ab5bb190340>, <unstructured.documents.elements.Title object at 0x7ab5bb190910>, <unstructured.documents.elements.Text object at 0x7ab5bb1913f0>, <unstructured.documents.elements.Title object at 0x7ab5bb191c90>, <unstructured.documents.elements.Text object at 0x7ab5bb191630>, <unstructured.documents.elements.Text object at 0x7ab5bb190fa0>, <unstructured.documents.elements.Text object at 0x7ab5bb1901f0>, <unstructured.documents.elements.Text object at 0x7ab5bb1901c0>, <unstructured.documents.elements.Text object at 0x7ab5bb1904c0>, <unstructured.documents.elements.Title object at 0x7ab5bb794070>, <unstructured.documents.elements.Text object at 0x7ab5bb190cd0>, <unstructured.documents.elements.Text object at 0x7ab5bb1928c0>, <unstructured.documents.elements.Title object at 0x7ab5bb193970>, <unstructured.documents.elements.Text object at 0x7ab5bb1907f0>, <unstructured.documents.elements.Text object at 0x7ab5bb190760>, <unstructured.documents.elements.Text object at 0x7ab5bb190af0>, <unstructured.documents.elements.Text object at 0x7ab5bb190190>, <unstructured.documents.elements.Text object at 0x7ab5bb1920b0>, <unstructured.documents.elements.Text object at 0x7ab5bb1927d0>, <unstructured.documents.elements.Text object at 0x7ab5bb193220>, <unstructured.documents.elements.Text object at 0x7ab5bb191db0>, <unstructured.documents.elements.Text object at 0x7ab5bb193310>, <unstructured.documents.elements.Text object at 0x7ab5bb192c50>, <unstructured.documents.elements.Text object at 0x7ab5bb192d40>, <unstructured.documents.elements.Title object at 0x7ab5bb1925f0>, <unstructured.documents.elements.Text object at 0x7ab5bb193130>, <unstructured.documents.elements.Text object at 0x7ab5bb191660>, <unstructured.documents.elements.Text object at 0x7ab5bb192e90>, <unstructured.documents.elements.Title object at 0x7ab5bb192050>, <unstructured.documents.elements.Text object at 0x7ab5bb192560>, <unstructured.documents.elements.Text object at 0x7ab5bb190b20>, <unstructured.documents.elements.Text object at 0x7ab5bb1906a0>, <unstructured.documents.elements.Text object at 0x7ab5bb191de0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb1905b0>, <unstructured.documents.elements.Title object at 0x7ab5bb190a30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb191900>, <unstructured.documents.elements.Text object at 0x7ab5bb192950>, <unstructured.documents.elements.Text object at 0x7ab5bb190eb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde25180>, <unstructured.documents.elements.Title object at 0x7ab5bde240d0>, <unstructured.documents.elements.Title object at 0x7ab5bb796c50>, <unstructured.documents.elements.Text object at 0x7ab5bde25870>, <unstructured.documents.elements.ListItem object at 0x7ab5bde25840>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde24eb0>, <unstructured.documents.elements.Text object at 0x7ab5bb04d210>, <unstructured.documents.elements.Text object at 0x7ab5bde24580>, <unstructured.documents.elements.Text object at 0x7ab5bb04d180>, <unstructured.documents.elements.Text object at 0x7ab5bb04cdc0>, <unstructured.documents.elements.Text object at 0x7ab5bb04d3c0>, <unstructured.documents.elements.Title object at 0x7ab5bb04d090>, <unstructured.documents.elements.Text object at 0x7ab5bb7cd690>, <unstructured.documents.elements.Text object at 0x7ab5bb04d000>, <unstructured.documents.elements.Title object at 0x7ab5bb04d480>, <unstructured.documents.elements.Text object at 0x7ab5bb7cd3f0>, <unstructured.documents.elements.Text object at 0x7ab5bde258d0>, <unstructured.documents.elements.Text object at 0x7ab5bde254e0>, <unstructured.documents.elements.Text object at 0x7ab5bde24640>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde25630>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde252a0>, <unstructured.documents.elements.Text object at 0x7ab5bde25090>, <unstructured.documents.elements.Title object at 0x7ab5bde24ac0>, <unstructured.documents.elements.Text object at 0x7ab5bde24e20>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde25c90>, <unstructured.documents.elements.Text object at 0x7ab5bde25d80>, <unstructured.documents.elements.Text object at 0x7ab5bde25bd0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde24310>, <unstructured.documents.elements.Title object at 0x7ab5bde25a20>, <unstructured.documents.elements.Text object at 0x7ab5bde24160>, <unstructured.documents.elements.Text object at 0x7ab5bde24400>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde25e70>, <unstructured.documents.elements.Title object at 0x7ab5bde24f10>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde24460>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7969e0>, <unstructured.documents.elements.Text object at 0x7ab5bb7967a0>, <unstructured.documents.elements.Text object at 0x7ab5bb7978e0>, <unstructured.documents.elements.Text object at 0x7ab5bb796980>, <unstructured.documents.elements.Text object at 0x7ab5bde24f70>, <unstructured.documents.elements.Title object at 0x7ab5bb1902e0>, <unstructured.documents.elements.ListItem object at 0x7ab5bb193a60>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde24cd0>, <unstructured.documents.elements.Title object at 0x7ab5be05e3e0>, <unstructured.documents.elements.Text object at 0x7ab5be05c520>, <unstructured.documents.elements.Text object at 0x7ab5be05e5f0>, <unstructured.documents.elements.Title object at 0x7ab5be05c7c0>, <unstructured.documents.elements.Title object at 0x7ab5be05c370>, <unstructured.documents.elements.Text object at 0x7ab5be05c3d0>, <unstructured.documents.elements.Title object at 0x7ab5be05d7e0>, <unstructured.documents.elements.Title object at 0x7ab5be05e320>, <unstructured.documents.elements.Text object at 0x7ab5be05e5c0>, <unstructured.documents.elements.Title object at 0x7ab5bb797130>, <unstructured.documents.elements.Text object at 0x7ab5be05c460>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb1936a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb1930d0>, <unstructured.documents.elements.Title object at 0x7ab5bb192aa0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb192110>, <unstructured.documents.elements.Title object at 0x7ab5bb193d60>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb192fb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb1921a0>, <unstructured.documents.elements.Text object at 0x7ab5bb193b80>, <unstructured.documents.elements.Text object at 0x7ab5bb193dc0>, <unstructured.documents.elements.Title object at 0x7ab5bb193a90>, <unstructured.documents.elements.Text object at 0x7ab5bb192a70>, <unstructured.documents.elements.Title object at 0x7ab5bb1924d0>, <unstructured.documents.elements.Text object at 0x7ab5bb190b80>, <unstructured.documents.elements.Title object at 0x7ab5bb192f20>, <unstructured.documents.elements.Text object at 0x7ab5be05c1c0>, <unstructured.documents.elements.Title object at 0x7ab5bb193ee0>, <unstructured.documents.elements.Text object at 0x7ab5be05d060>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05cdc0>, <unstructured.documents.elements.Title object at 0x7ab5bb1920e0>, <unstructured.documents.elements.Title object at 0x7ab5bb99ff40>, <unstructured.documents.elements.Title object at 0x7ab5bb99ecb0>, <unstructured.documents.elements.Title object at 0x7ab5bb99edd0>, <unstructured.documents.elements.Text object at 0x7ab5bb04c9a0>, <unstructured.documents.elements.ListItem object at 0x7ab5bb99d9c0>, <unstructured.documents.elements.Title object at 0x7ab5bb99f2e0>, <unstructured.documents.elements.Title object at 0x7ab5bb99fc70>, <unstructured.documents.elements.Title object at 0x7ab5bb99c2b0>, <unstructured.documents.elements.Title object at 0x7ab5bb99c850>, <unstructured.documents.elements.Title object at 0x7ab5bb99fe50>, <unstructured.documents.elements.Title object at 0x7ab5bb99d660>, <unstructured.documents.elements.Text object at 0x7ab5bb99e680>, <unstructured.documents.elements.Title object at 0x7ab5bb99fdc0>, <unstructured.documents.elements.Title object at 0x7ab5bb99da50>, <unstructured.documents.elements.Title object at 0x7ab5bb99f1f0>, <unstructured.documents.elements.Title object at 0x7ab5bb99e560>, <unstructured.documents.elements.Text object at 0x7ab5bb99e740>, <unstructured.documents.elements.Title object at 0x7ab5bb99d2d0>, <unstructured.documents.elements.Title object at 0x7ab5bb99cf70>, <unstructured.documents.elements.Text object at 0x7ab5bb99ef50>, <unstructured.documents.elements.Title object at 0x7ab5bb99c640>, <unstructured.documents.elements.Title object at 0x7ab5bb99fd30>, <unstructured.documents.elements.Title object at 0x7ab5bb99d060>, <unstructured.documents.elements.Title object at 0x7ab5bb99ef20>, <unstructured.documents.elements.Title object at 0x7ab5bb99d300>, <unstructured.documents.elements.Title object at 0x7ab5bb99c820>, <unstructured.documents.elements.Title object at 0x7ab5bb99d1e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb797940>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99f940>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99f4f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99f970>, <unstructured.documents.elements.Text object at 0x7ab5bb99c0a0>, <unstructured.documents.elements.Text object at 0x7ab5bb99f430>, <unstructured.documents.elements.Text object at 0x7ab5bb99c1f0>, <unstructured.documents.elements.Text object at 0x7ab5bb99ff10>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99caf0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99cd60>, <unstructured.documents.elements.Text object at 0x7ab5bb99d480>, <unstructured.documents.elements.Text object at 0x7ab5bb99dd50>, <unstructured.documents.elements.Text object at 0x7ab5bb99df90>, <unstructured.documents.elements.Title object at 0x7ab5bb99e020>, <unstructured.documents.elements.Text object at 0x7ab5bb64fa60>, <unstructured.documents.elements.Text object at 0x7ab5bb99e1d0>, <unstructured.documents.elements.Text object at 0x7ab5bb99d150>, <unstructured.documents.elements.Text object at 0x7ab5bb99e6e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99d1b0>, <unstructured.documents.elements.Title object at 0x7ab5bb99f0a0>, <unstructured.documents.elements.Text object at 0x7ab5bb99ca00>, <unstructured.documents.elements.Title object at 0x7ab5bb99e3b0>, <unstructured.documents.elements.Text object at 0x7ab5bb99cdf0>, <unstructured.documents.elements.Text object at 0x7ab5bb99d5d0>, <unstructured.documents.elements.Text object at 0x7ab5bb99d4b0>, <unstructured.documents.elements.Text object at 0x7ab5bb99e830>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99ebc0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99d7b0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99e5f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99ffa0>, <unstructured.documents.elements.Title object at 0x7ab5bb549900>, <unstructured.documents.elements.ListItem object at 0x7ab5bb549660>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5497e0>, <unstructured.documents.elements.Text object at 0x7ab5bb548760>, <unstructured.documents.elements.Title object at 0x7ab5bb5486d0>, <unstructured.documents.elements.Text object at 0x7ab5bb5485b0>, <unstructured.documents.elements.Text object at 0x7ab5bb5484c0>, <unstructured.documents.elements.Title object at 0x7ab5bb5482e0>, <unstructured.documents.elements.Title object at 0x7ab5bb5484f0>, <unstructured.documents.elements.Title object at 0x7ab5bb548370>, <unstructured.documents.elements.Title object at 0x7ab5bb54a020>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5498a0>, <unstructured.documents.elements.Text object at 0x7ab5bb548820>, <unstructured.documents.elements.Text object at 0x7ab5bb54b910>, <unstructured.documents.elements.Title object at 0x7ab5bb549d50>, <unstructured.documents.elements.Text object at 0x7ab5bb549930>, <unstructured.documents.elements.Text object at 0x7ab5bb5483d0>, <unstructured.documents.elements.Text object at 0x7ab5bb549d20>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0ee8f0>, <unstructured.documents.elements.Text object at 0x7ab5bb549b70>, <unstructured.documents.elements.Text object at 0x7ab5bb5487f0>, <unstructured.documents.elements.Text object at 0x7ab5bb65b700>, <unstructured.documents.elements.Text object at 0x7ab5bb5e0040>, <unstructured.documents.elements.Title object at 0x7ab5bb5e3a30>, <unstructured.documents.elements.Text object at 0x7ab5bb5e0340>, <unstructured.documents.elements.Text object at 0x7ab5bb5e2560>, <unstructured.documents.elements.Text object at 0x7ab5bb5e2b30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5e2fb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5e2650>, <unstructured.documents.elements.Text object at 0x7ab5bb5e35e0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e3400>, <unstructured.documents.elements.Title object at 0x7ab5bb5e2770>, <unstructured.documents.elements.Text object at 0x7ab5bb5e3b80>, <unstructured.documents.elements.Text object at 0x7ab5bb5e03a0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e0700>, <unstructured.documents.elements.Text object at 0x7ab5bb5e04f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5e3550>, <unstructured.documents.elements.Title object at 0x7ab5bb5e3d30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5e0490>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5e23b0>, <unstructured.documents.elements.Text object at 0x7ab5bb54a260>, <unstructured.documents.elements.Text object at 0x7ab5bb5e22c0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e25c0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e0850>, <unstructured.documents.elements.Title object at 0x7ab5bb1936d0>, <unstructured.documents.elements.Title object at 0x7ab5bb74bf10>, <unstructured.documents.elements.Text object at 0x7ab5bb1935b0>, <unstructured.documents.elements.ListItem object at 0x7ab5bb193bb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb549ba0>, <unstructured.documents.elements.Title object at 0x7ab5be0ecc10>, <unstructured.documents.elements.Title object at 0x7ab5be0ed750>, <unstructured.documents.elements.Title object at 0x7ab5be0ed450>, <unstructured.documents.elements.Title object at 0x7ab5be0ee410>, <unstructured.documents.elements.Title object at 0x7ab5be0ed2a0>, <unstructured.documents.elements.Title object at 0x7ab5be0ef4c0>, <unstructured.documents.elements.Title object at 0x7ab5be0ed7b0>, <unstructured.documents.elements.Title object at 0x7ab5be0ee740>, <unstructured.documents.elements.Title object at 0x7ab5be0edd20>, <unstructured.documents.elements.Title object at 0x7ab5be0ed4e0>, <unstructured.documents.elements.Title object at 0x7ab5be0ef700>, <unstructured.documents.elements.Title object at 0x7ab5be0ef6d0>, <unstructured.documents.elements.Title object at 0x7ab5be0ee380>, <unstructured.documents.elements.Title object at 0x7ab5be0edb10>, <unstructured.documents.elements.Text object at 0x7ab5be0eec50>, <unstructured.documents.elements.Title object at 0x7ab5be0eda50>, <unstructured.documents.elements.Title object at 0x7ab5be0ee6b0>, <unstructured.documents.elements.Title object at 0x7ab5be0ee1d0>, <unstructured.documents.elements.Title object at 0x7ab5be0edae0>, <unstructured.documents.elements.Title object at 0x7ab5be0ef610>, <unstructured.documents.elements.Title object at 0x7ab5be0ee260>, <unstructured.documents.elements.Text object at 0x7ab5be0ef4f0>, <unstructured.documents.elements.Title object at 0x7ab5be0ef970>, <unstructured.documents.elements.Text object at 0x7ab5be0ee1a0>, <unstructured.documents.elements.Title object at 0x7ab5be0edfc0>, <unstructured.documents.elements.Title object at 0x7ab5be0ed900>, <unstructured.documents.elements.Text object at 0x7ab5be0edf60>, <unstructured.documents.elements.Title object at 0x7ab5be0ee6e0>, <unstructured.documents.elements.Text object at 0x7ab5be0ed540>, <unstructured.documents.elements.Title object at 0x7ab5be0ed030>, <unstructured.documents.elements.Title object at 0x7ab5be0ed870>, <unstructured.documents.elements.Title object at 0x7ab5be0ef190>, <unstructured.documents.elements.Title object at 0x7ab5be0ed090>, <unstructured.documents.elements.Title object at 0x7ab5be0ec550>, <unstructured.documents.elements.Title object at 0x7ab5be0ee5c0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb193550>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7492a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0ec730>, <unstructured.documents.elements.Text object at 0x7ab5be0eca00>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce680>, <unstructured.documents.elements.Title object at 0x7ab5bb99f220>, <unstructured.documents.elements.Title object at 0x7ab5bb99f100>, <unstructured.documents.elements.Title object at 0x7ab5be0ef340>, <unstructured.documents.elements.Title object at 0x7ab5be0efa90>, <unstructured.documents.elements.Title object at 0x7ab5be0ec7c0>, <unstructured.documents.elements.Title object at 0x7ab5be0ec2b0>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce350>, <unstructured.documents.elements.Title object at 0x7ab5bb99d120>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce3e0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cdd50>, <unstructured.documents.elements.Title object at 0x7ab5bb5cebc0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cc9d0>, <unstructured.documents.elements.Title object at 0x7ab5be0ed720>, <unstructured.documents.elements.Text object at 0x7ab5bb5cf580>, <unstructured.documents.elements.Title object at 0x7ab5bb5cee90>, <unstructured.documents.elements.Title object at 0x7ab5bb99f580>, <unstructured.documents.elements.Title object at 0x7ab5bb5cc700>, <unstructured.documents.elements.Title object at 0x7ab5bb5ceec0>, <unstructured.documents.elements.Title object at 0x7ab5bb99d570>, <unstructured.documents.elements.Title object at 0x7ab5bb5cf490>, <unstructured.documents.elements.Text object at 0x7ab5bb5cfd60>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0efca0>, <unstructured.documents.elements.Title object at 0x7ab5be0eff40>, <unstructured.documents.elements.Title object at 0x7ab5bb5ccf10>, <unstructured.documents.elements.Title object at 0x7ab5bb5cd960>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce440>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce020>, <unstructured.documents.elements.Text object at 0x7ab5bb5cd780>, <unstructured.documents.elements.Title object at 0x7ab5be0ee2c0>, <unstructured.documents.elements.Title object at 0x7ab5be0efe50>, <unstructured.documents.elements.Title object at 0x7ab5bb5ccdc0>, <unstructured.documents.elements.Title object at 0x7ab5be0efee0>, <unstructured.documents.elements.Text object at 0x7ab5bb5cd9c0>, <unstructured.documents.elements.Text object at 0x7ab5bb5cec50>, <unstructured.documents.elements.Title object at 0x7ab5bb04c4f0>, <unstructured.documents.elements.Title object at 0x7ab5be0efcd0>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce290>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce800>, <unstructured.documents.elements.Title object at 0x7ab5bb5cdb40>, <unstructured.documents.elements.Title object at 0x7ab5bb5cfcd0>, <unstructured.documents.elements.Title object at 0x7ab5bb99d390>, <unstructured.documents.elements.Text object at 0x7ab5bb99cee0>, <unstructured.documents.elements.Title object at 0x7ab5be0ef5e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0ec9a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0eca30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0ecaf0>, <unstructured.documents.elements.Title object at 0x7ab5bb5ccd30>, <unstructured.documents.elements.Title object at 0x7ab5bb5cf1c0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cca30>, <unstructured.documents.elements.Title object at 0x7ab5bb5cca90>, <unstructured.documents.elements.Title object at 0x7ab5bb5cf400>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce860>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce770>, <unstructured.documents.elements.Title object at 0x7ab5bb5cee60>, <unstructured.documents.elements.Title object at 0x7ab5bb5cc2e0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cc490>, <unstructured.documents.elements.Title object at 0x7ab5bb5cd150>, <unstructured.documents.elements.Title object at 0x7ab5bb5cc4f0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cfb50>, <unstructured.documents.elements.Title object at 0x7ab5bb5cdba0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cff10>, <unstructured.documents.elements.Title object at 0x7ab5bb5ccb20>, <unstructured.documents.elements.Title object at 0x7ab5bb5ceef0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cdd20>, <unstructured.documents.elements.Title object at 0x7ab5bb5cd4b0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cf730>, <unstructured.documents.elements.Title object at 0x7ab5bb5cc1c0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cf550>, <unstructured.documents.elements.Title object at 0x7ab5bb5cd5a0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cf2b0>, <unstructured.documents.elements.Title object at 0x7ab5bb5ce710>, <unstructured.documents.elements.Text object at 0x7ab5bb5cc370>, <unstructured.documents.elements.Title object at 0x7ab5bb5ceb60>, <unstructured.documents.elements.Title object at 0x7ab5bb5cf6d0>, <unstructured.documents.elements.Title object at 0x7ab5bb5cdb70>, <unstructured.documents.elements.Title object at 0x7ab5bb5cc610>, <unstructured.documents.elements.Title object at 0x7ab5bb5cc0a0>, <unstructured.documents.elements.Text object at 0x7ab5bb5cf160>, <unstructured.documents.elements.Text object at 0x7ab5bb5cd210>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0ece80>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0ed2d0>, <unstructured.documents.elements.Title object at 0x7ab5bdfa5a80>, <unstructured.documents.elements.ListItem object at 0x7ab5bdf3a6b0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bdf3a650>, <unstructured.documents.elements.Text object at 0x7ab5bdf3a740>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bdf3b670>, <unstructured.documents.elements.Title object at 0x7ab5bdf3b580>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bdf3bbe0>, <unstructured.documents.elements.Text object at 0x7ab5bb00e7a0>, <unstructured.documents.elements.Text object at 0x7ab5bb00e320>, <unstructured.documents.elements.Text object at 0x7ab5bdf387f0>, <unstructured.documents.elements.Title object at 0x7ab5bb39c790>, <unstructured.documents.elements.Text object at 0x7ab5be0f9210>, <unstructured.documents.elements.Text object at 0x7ab5bb39ffd0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb15b460>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb15b160>, <unstructured.documents.elements.Title object at 0x7ab5bb15b1c0>, <unstructured.documents.elements.Title object at 0x7ab5bb15be80>, <unstructured.documents.elements.Title object at 0x7ab5bb15be50>, <unstructured.documents.elements.Text object at 0x7ab5bb15bdf0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb158070>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb1935e0>, <unstructured.documents.elements.Text object at 0x7ab5bb158280>, <unstructured.documents.elements.Title object at 0x7ab5bb158a30>, <unstructured.documents.elements.Title object at 0x7ab5bb1584f0>, <unstructured.documents.elements.Title object at 0x7ab5bb158460>, <unstructured.documents.elements.Title object at 0x7ab5bb158400>, <unstructured.documents.elements.Text object at 0x7ab5bb158d90>, <unstructured.documents.elements.Title object at 0x7ab5bb15bc10>, <unstructured.documents.elements.Title object at 0x7ab5bb158c10>, <unstructured.documents.elements.Title object at 0x7ab5bb158880>, <unstructured.documents.elements.Title object at 0x7ab5bb1589d0>, <unstructured.documents.elements.Title object at 0x7ab5bb158e20>, <unstructured.documents.elements.Title object at 0x7ab5bb158700>, <unstructured.documents.elements.Text object at 0x7ab5bb158d30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb158190>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb158160>, <unstructured.documents.elements.Text object at 0x7ab5be10efe0>, <unstructured.documents.elements.ListItem object at 0x7ab5be10efb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be10f100>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be10f4c0>, <unstructured.documents.elements.Title object at 0x7ab5be10f5e0>, <unstructured.documents.elements.Text object at 0x7ab5be10f880>, <unstructured.documents.elements.Text object at 0x7ab5be10f5b0>, <unstructured.documents.elements.Text object at 0x7ab5be10c700>, <unstructured.documents.elements.Title object at 0x7ab5be10f160>, <unstructured.documents.elements.Text object at 0x7ab5be10c190>, <unstructured.documents.elements.Title object at 0x7ab5be10f7f0>, <unstructured.documents.elements.Text object at 0x7ab5be10c310>, <unstructured.documents.elements.Text object at 0x7ab5be10f940>, <unstructured.documents.elements.Text object at 0x7ab5be10c790>, <unstructured.documents.elements.Title object at 0x7ab5be10fbe0>, <unstructured.documents.elements.Title object at 0x7ab5be10fd60>, <unstructured.documents.elements.Text object at 0x7ab5be10fa30>, <unstructured.documents.elements.Text object at 0x7ab5be10f2e0>, <unstructured.documents.elements.Title object at 0x7ab5be10fa90>, <unstructured.documents.elements.Text object at 0x7ab5be10fd90>, <unstructured.documents.elements.Text object at 0x7ab5be10c250>, <unstructured.documents.elements.Title object at 0x7ab5be10f8e0>, <unstructured.documents.elements.Title object at 0x7ab5be10cd00>, <unstructured.documents.elements.Text object at 0x7ab5be10e560>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be10cfa0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be10d330>, <unstructured.documents.elements.Text object at 0x7ab5be10cf40>, <unstructured.documents.elements.Title object at 0x7ab5be10c3d0>, <unstructured.documents.elements.Title object at 0x7ab5be10d240>, <unstructured.documents.elements.Title object at 0x7ab5be10d6f0>, <unstructured.documents.elements.Text object at 0x7ab5be10d8a0>, <unstructured.documents.elements.Title object at 0x7ab5be10dea0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be10dab0>, <unstructured.documents.elements.Text object at 0x7ab5be10e1d0>, <unstructured.documents.elements.Text object at 0x7ab5be10e230>, <unstructured.documents.elements.Text object at 0x7ab5be10e710>, <unstructured.documents.elements.Text object at 0x7ab5be10e380>, <unstructured.documents.elements.Title object at 0x7ab5be10e5c0>, <unstructured.documents.elements.Text object at 0x7ab5be10e8f0>, <unstructured.documents.elements.Title object at 0x7ab5be10e620>, <unstructured.documents.elements.Text object at 0x7ab5be10e8c0>, <unstructured.documents.elements.Text object at 0x7ab5be10ead0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be10e7d0>, <unstructured.documents.elements.Text object at 0x7ab5bb682320>, <unstructured.documents.elements.Text object at 0x7ab5bb682290>, <unstructured.documents.elements.Title object at 0x7ab5be10e7a0>, <unstructured.documents.elements.Text object at 0x7ab5be10ec50>, <unstructured.documents.elements.Title object at 0x7ab5be10ed70>, <unstructured.documents.elements.Title object at 0x7ab5be10ed40>, <unstructured.documents.elements.Text object at 0x7ab5bb681300>, <unstructured.documents.elements.Text object at 0x7ab5bb6835e0>, <unstructured.documents.elements.Title object at 0x7ab5bb6832e0>, <unstructured.documents.elements.Text object at 0x7ab5bb680fd0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb6814b0>, <unstructured.documents.elements.Text object at 0x7ab5be10f040>, <unstructured.documents.elements.Title object at 0x7ab5bde24070>, <unstructured.documents.elements.ListItem object at 0x7ab5be10cc70>, <unstructured.documents.elements.Title object at 0x7ab5bb50f8e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde248b0>, <unstructured.documents.elements.Text object at 0x7ab5bb50e260>, <unstructured.documents.elements.Text object at 0x7ab5bb50e2c0>, <unstructured.documents.elements.Title object at 0x7ab5bb50fb80>, <unstructured.documents.elements.Text object at 0x7ab5bb50c610>, <unstructured.documents.elements.Text object at 0x7ab5bb50cb50>, <unstructured.documents.elements.Text object at 0x7ab5bb50cd60>, <unstructured.documents.elements.Text object at 0x7ab5bb50c430>, <unstructured.documents.elements.Title object at 0x7ab5bb50d060>, <unstructured.documents.elements.Text object at 0x7ab5bb50cfa0>, <unstructured.documents.elements.Text object at 0x7ab5bb50d1e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50c8b0>, <unstructured.documents.elements.Title object at 0x7ab5bb50c940>, <unstructured.documents.elements.Title object at 0x7ab5bb50c0d0>, <unstructured.documents.elements.Title object at 0x7ab5bb50d660>, <unstructured.documents.elements.Text object at 0x7ab5bb50d9f0>, <unstructured.documents.elements.Text object at 0x7ab5bb50d6f0>, <unstructured.documents.elements.Text object at 0x7ab5bb50cf70>, <unstructured.documents.elements.Text object at 0x7ab5bb50de40>, <unstructured.documents.elements.Title object at 0x7ab5bb50e380>, <unstructured.documents.elements.Text object at 0x7ab5bb50e440>, <unstructured.documents.elements.Title object at 0x7ab5bb50fb20>, <unstructured.documents.elements.Title object at 0x7ab5bb1595d0>, <unstructured.documents.elements.Title object at 0x7ab5bb50fa30>, <unstructured.documents.elements.Title object at 0x7ab5bb39cc10>, <unstructured.documents.elements.Title object at 0x7ab5bb159210>, <unstructured.documents.elements.Text object at 0x7ab5bb50d870>, <unstructured.documents.elements.Title object at 0x7ab5bb50e9e0>, <unstructured.documents.elements.Text object at 0x7ab5bb50e0b0>, <unstructured.documents.elements.Text object at 0x7ab5bb50c250>, <unstructured.documents.elements.Title object at 0x7ab5bb50ce50>, <unstructured.documents.elements.Title object at 0x7ab5bb1596c0>, <unstructured.documents.elements.Title object at 0x7ab5bb50f0a0>, <unstructured.documents.elements.Title object at 0x7ab5bb50c6a0>, <unstructured.documents.elements.Text object at 0x7ab5bb50ebc0>, <unstructured.documents.elements.Title object at 0x7ab5bb50f5b0>, <unstructured.documents.elements.Title object at 0x7ab5bb50e830>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb159540>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50e6e0>, <unstructured.documents.elements.Title object at 0x7ab5bb50e650>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50e5f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50e350>, <unstructured.documents.elements.Text object at 0x7ab5bb50e920>, <unstructured.documents.elements.Title object at 0x7ab5bb50ce20>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50ec50>, <unstructured.documents.elements.Title object at 0x7ab5bb50eda0>, <unstructured.documents.elements.Title object at 0x7ab5bb50e8f0>, <unstructured.documents.elements.Text object at 0x7ab5bdddd3c0>, <unstructured.documents.elements.ListItem object at 0x7ab5bddddcc0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bddde0e0>, <unstructured.documents.elements.Title object at 0x7ab5bddde2c0>, <unstructured.documents.elements.Text object at 0x7ab5bdddcf40>, <unstructured.documents.elements.Title object at 0x7ab5bdddcee0>, <unstructured.documents.elements.Text object at 0x7ab5bddde290>, <unstructured.documents.elements.Text object at 0x7ab5bdddcdc0>, <unstructured.documents.elements.Title object at 0x7ab5bddde560>, <unstructured.documents.elements.Title object at 0x7ab5bddddc90>, <unstructured.documents.elements.Text object at 0x7ab5bdddd120>, <unstructured.documents.elements.Text object at 0x7ab5bdddd1b0>, <unstructured.documents.elements.Text object at 0x7ab5bddde3e0>, <unstructured.documents.elements.Text object at 0x7ab5bdddd210>, <unstructured.documents.elements.Text object at 0x7ab5bb681930>, <unstructured.documents.elements.Text object at 0x7ab5bdddd390>, <unstructured.documents.elements.Text object at 0x7ab5bdddd090>, <unstructured.documents.elements.Text object at 0x7ab5bb681ab0>, <unstructured.documents.elements.Text object at 0x7ab5bb681ae0>, <unstructured.documents.elements.Text object at 0x7ab5bb681600>, <unstructured.documents.elements.Title object at 0x7ab5bb681d20>, <unstructured.documents.elements.Title object at 0x7ab5bb682350>, <unstructured.documents.elements.Title object at 0x7ab5bb2c15a0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c1ff0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c1ea0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c1ba0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c1cc0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c0fa0>, <unstructured.documents.elements.Text object at 0x7ab5bb2c1630>, <unstructured.documents.elements.Title object at 0x7ab5bb2c2050>, <unstructured.documents.elements.Title object at 0x7ab5bb2c1f00>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb682380>, <unstructured.documents.elements.Title object at 0x7ab5bb681ea0>, <unstructured.documents.elements.Title object at 0x7ab5bb6810f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb6811e0>, <unstructured.documents.elements.Title object at 0x7ab5bb681060>, <unstructured.documents.elements.Text object at 0x7ab5bb50c550>, <unstructured.documents.elements.Title object at 0x7ab5bb681210>, <unstructured.documents.elements.Title object at 0x7ab5bb2c0910>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c3040>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c30a0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c34c0>, <unstructured.documents.elements.Text object at 0x7ab5bb2c0b50>, <unstructured.documents.elements.Title object at 0x7ab5bb2c2c80>, <unstructured.documents.elements.Title object at 0x7ab5bb2c1030>, <unstructured.documents.elements.Title object at 0x7ab5bb2c2c50>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c18a0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c2140>, <unstructured.documents.elements.Title object at 0x7ab5bb2c0dc0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c33d0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c2770>, <unstructured.documents.elements.Title object at 0x7ab5bb2c1840>, <unstructured.documents.elements.Title object at 0x7ab5bb2c25c0>, <unstructured.documents.elements.Title object at 0x7ab5bb2c08e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c2440>, <unstructured.documents.elements.Title object at 0x7ab5bb2c2a10>, <unstructured.documents.elements.Text object at 0x7ab5bb2c29e0>, <unstructured.documents.elements.Text object at 0x7ab5bb2c2470>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50f4c0>, <unstructured.documents.elements.Title object at 0x7ab5bde223e0>, <unstructured.documents.elements.ListItem object at 0x7ab5bb548a30>, <unstructured.documents.elements.Title object at 0x7ab5bb523760>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7914e0>, <unstructured.documents.elements.Title object at 0x7ab5bb791120>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb793f70>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bdddd510>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb792d70>, <unstructured.documents.elements.Title object at 0x7ab5bb791b70>, <unstructured.documents.elements.Title object at 0x7ab5bb790130>, <unstructured.documents.elements.Title object at 0x7ab5bb791450>, <unstructured.documents.elements.Title object at 0x7ab5bb791420>, <unstructured.documents.elements.Text object at 0x7ab5bb790a00>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb790670>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb790a60>, <unstructured.documents.elements.Text object at 0x7ab5bb5e3850>, <unstructured.documents.elements.Text object at 0x7ab5bb790c70>, <unstructured.documents.elements.Text object at 0x7ab5bb790f70>, <unstructured.documents.elements.Text object at 0x7ab5bb790b80>, <unstructured.documents.elements.Text object at 0x7ab5bb791300>, <unstructured.documents.elements.Title object at 0x7ab5bb790790>, <unstructured.documents.elements.Title object at 0x7ab5bb791210>, <unstructured.documents.elements.Text object at 0x7ab5bb5e1ea0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb790820>, <unstructured.documents.elements.Text object at 0x7ab5bb5e3d60>, <unstructured.documents.elements.Text object at 0x7ab5bb5e1600>, <unstructured.documents.elements.Text object at 0x7ab5bb790730>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5e34c0>, <unstructured.documents.elements.Title object at 0x7ab5bb5e0af0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5e1240>, <unstructured.documents.elements.Title object at 0x7ab5bb5e08e0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e3340>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5e01c0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e3670>, <unstructured.documents.elements.ListItem object at 0x7ab5bb5e2470>, <unstructured.documents.elements.ListItem object at 0x7ab5bb5e2470>, <unstructured.documents.elements.ListItem object at 0x7ab5bb5e2470>, <unstructured.documents.elements.ListItem object at 0x7ab5bb5e2470>, <unstructured.documents.elements.Title object at 0x7ab5bb5e2bc0>, <unstructured.documents.elements.Title object at 0x7ab5bb5e0190>, <unstructured.documents.elements.Title object at 0x7ab5bb5e3d00>, <unstructured.documents.elements.Text object at 0x7ab5bb5e2e00>, <unstructured.documents.elements.Text object at 0x7ab5bb5e12a0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e13f0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e14b0>, <unstructured.documents.elements.Text object at 0x7ab5bb534940>, <unstructured.documents.elements.Title object at 0x7ab5bb5349d0>, <unstructured.documents.elements.Title object at 0x7ab5bb534880>, <unstructured.documents.elements.Text object at 0x7ab5bb5347f0>, <unstructured.documents.elements.Text object at 0x7ab5bdfbf670>, <unstructured.documents.elements.ListItem object at 0x7ab5bdfbe500>, <unstructured.documents.elements.Text object at 0x7ab5bb04e500>, <unstructured.documents.elements.Title object at 0x7ab5bdfbfeb0>, <unstructured.documents.elements.Text object at 0x7ab5bb04e8c0>, <unstructured.documents.elements.Title object at 0x7ab5bb04eb30>, <unstructured.documents.elements.Text object at 0x7ab5bb04e770>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04ef80>, <unstructured.documents.elements.Title object at 0x7ab5bb04f730>, <unstructured.documents.elements.Text object at 0x7ab5bb04fd00>, <unstructured.documents.elements.Title object at 0x7ab5bb04ef20>, <unstructured.documents.elements.Text object at 0x7ab5bb04dfc0>, <unstructured.documents.elements.Text object at 0x7ab5bb04f940>, <unstructured.documents.elements.Text object at 0x7ab5bb04d780>, <unstructured.documents.elements.Text object at 0x7ab5bb04c550>, <unstructured.documents.elements.Text object at 0x7ab5bb04d9c0>, <unstructured.documents.elements.Text object at 0x7ab5bb04d5d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04f820>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04d720>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04eb00>, <unstructured.documents.elements.Text object at 0x7ab5bb04faf0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04fb80>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04f1f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04d7e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04e6e0>, <unstructured.documents.elements.Title object at 0x7ab5bb50e320>, <unstructured.documents.elements.ListItem object at 0x7ab5be05de10>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50d7e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be151c00>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be152a40>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05dba0>, <unstructured.documents.elements.Title object at 0x7ab5bb192080>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05ea10>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05dfc0>, <unstructured.documents.elements.Title object at 0x7ab5be05e980>, <unstructured.documents.elements.Title object at 0x7ab5be05dcc0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05dab0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05efb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05f9a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05fa30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05f400>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bba50cd0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05fb80>, <unstructured.documents.elements.Text object at 0x7ab5bb8ccf40>, <unstructured.documents.elements.ListItem object at 0x7ab5bb8cfaf0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50ed70>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50ea40>, <unstructured.documents.elements.Title object at 0x7ab5bb50ef50>, <unstructured.documents.elements.Text object at 0x7ab5bb50f6a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50e7a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde242e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50e4d0>, <unstructured.documents.elements.Text object at 0x7ab5bb50dd80>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50d780>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50dc30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50c280>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50cfd0>, <unstructured.documents.elements.Title object at 0x7ab5bb50fc10>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50d5d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb50dcc0>, <unstructured.documents.elements.Title object at 0x7ab5bba177c0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bba16380>, <unstructured.documents.elements.Text object at 0x7ab5bb50c3d0>, <unstructured.documents.elements.Title object at 0x7ab5bb7307f0>, <unstructured.documents.elements.ListItem object at 0x7ab5bb730f10>, <unstructured.documents.elements.Title object at 0x7ab5bb733c70>, <unstructured.documents.elements.Title object at 0x7ab5bb731ea0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0e03a0>, <unstructured.documents.elements.Title object at 0x7ab5bb733550>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb731690>, <unstructured.documents.elements.Text object at 0x7ab5bb733190>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb730d30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb783cd0>, <unstructured.documents.elements.Text object at 0x7ab5bb783040>, <unstructured.documents.elements.Text object at 0x7ab5bb7824a0>, <unstructured.documents.elements.Text object at 0x7ab5bb782ad0>, <unstructured.documents.elements.Text object at 0x7ab5bb781ff0>, <unstructured.documents.elements.Text object at 0x7ab5bb782230>, <unstructured.documents.elements.Text object at 0x7ab5bb7833d0>, <unstructured.documents.elements.Text object at 0x7ab5bb782950>, <unstructured.documents.elements.Text object at 0x7ab5bb782110>, <unstructured.documents.elements.Text object at 0x7ab5bb783b50>, <unstructured.documents.elements.Text object at 0x7ab5bb733430>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb730700>, <unstructured.documents.elements.Text object at 0x7ab5bb782770>, <unstructured.documents.elements.Text object at 0x7ab5bb780160>, <unstructured.documents.elements.Text object at 0x7ab5bb7823b0>, <unstructured.documents.elements.Text object at 0x7ab5bb782260>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb730c40>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb732890>, <unstructured.documents.elements.Title object at 0x7ab5bb730ee0>, <unstructured.documents.elements.Title object at 0x7ab5bb731e40>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb733b80>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7328f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04df00>, <unstructured.documents.elements.Text object at 0x7ab5bb04f670>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5cd6c0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb04e4a0>, <unstructured.documents.elements.Text object at 0x7ab5be0e1660>, <unstructured.documents.elements.ListItem object at 0x7ab5be0e16f0>, <unstructured.documents.elements.Text object at 0x7ab5be0e3fd0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb783d60>, <unstructured.documents.elements.Text object at 0x7ab5bb769900>, <unstructured.documents.elements.Text object at 0x7ab5be0e0850>, <unstructured.documents.elements.Title object at 0x7ab5bb76b760>, <unstructured.documents.elements.Text object at 0x7ab5bb76aa40>, <unstructured.documents.elements.Text object at 0x7ab5bb768d60>, <unstructured.documents.elements.Text object at 0x7ab5bb6821d0>, <unstructured.documents.elements.Text object at 0x7ab5bb76a170>, <unstructured.documents.elements.Text object at 0x7ab5bb76b610>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb76bbb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7903d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb821060>, <unstructured.documents.elements.Text object at 0x7ab5bb823d90>, <unstructured.documents.elements.Text object at 0x7ab5bb8234f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb8222c0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be10ce50>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb790940>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb193d00>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0ef3d0>, <unstructured.documents.elements.Title object at 0x7ab5be0e17e0>, <unstructured.documents.elements.ListItem object at 0x7ab5be10d720>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb74baf0>, <unstructured.documents.elements.Title object at 0x7ab5bb74b100>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb74ae00>, <unstructured.documents.elements.Title object at 0x7ab5bb74a7d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb74b790>, <unstructured.documents.elements.Title object at 0x7ab5bb74b130>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb74a860>, <unstructured.documents.elements.Title object at 0x7ab5bb749ea0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb749630>, <unstructured.documents.elements.Title object at 0x7ab5bb7493f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb74b040>, <unstructured.documents.elements.Title object at 0x7ab5bb7497b0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb74ab60>, <unstructured.documents.elements.Text object at 0x7ab5b93ecee0>, <unstructured.documents.elements.ListItem object at 0x7ab5b93ecfa0>, <unstructured.documents.elements.Title object at 0x7ab5b93ed090>, <unstructured.documents.elements.Title object at 0x7ab5b93ed240>, <unstructured.documents.elements.NarrativeText object at 0x7ab5b93ed420>, <unstructured.documents.elements.Title object at 0x7ab5b93ed390>, <unstructured.documents.elements.NarrativeText object at 0x7ab5b93ed5d0>, <unstructured.documents.elements.ListItem object at 0x7ab5b93ed660>, <unstructured.documents.elements.ListItem object at 0x7ab5b93ed720>, <unstructured.documents.elements.ListItem object at 0x7ab5b93ed6f0>, <unstructured.documents.elements.ListItem object at 0x7ab5b93ed930>, <unstructured.documents.elements.ListItem object at 0x7ab5b93edab0>, <unstructured.documents.elements.ListItem object at 0x7ab5b93edae0>, <unstructured.documents.elements.ListItem object at 0x7ab5b93edf00>, <unstructured.documents.elements.Title object at 0x7ab5b93ee0e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5b93eceb0>, <unstructured.documents.elements.Text object at 0x7ab5b93ecf70>, <unstructured.documents.elements.Title object at 0x7ab5be05dbd0>, <unstructured.documents.elements.ListItem object at 0x7ab5be05c9d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05ddb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05e890>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb99fc40>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05d120>, <unstructured.documents.elements.Title object at 0x7ab5bb783340>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be05f5e0>, <unstructured.documents.elements.Title object at 0x7ab5bb502380>, <unstructured.documents.elements.Title object at 0x7ab5bb502800>, <unstructured.documents.elements.Title object at 0x7ab5bb5027d0>, <unstructured.documents.elements.Title object at 0x7ab5bb500160>, <unstructured.documents.elements.Title object at 0x7ab5bb501f00>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb782e90>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb501150>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb502200>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5022f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5023e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb502470>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb502590>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb5026b0>, <unstructured.documents.elements.Text object at 0x7ab5bb501d50>, <unstructured.documents.elements.Title object at 0x7ab5bb2c17b0>, <unstructured.documents.elements.ListItem object at 0x7ab5bb2c3940>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c2d40>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c1870>, <unstructured.documents.elements.Title object at 0x7ab5bb2c3dc0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c3880>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c3460>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c3eb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c0370>, <unstructured.documents.elements.Title object at 0x7ab5bb2c0400>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c3b50>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c0700>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c0880>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c1360>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c2ce0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c38b0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c36a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c3790>, <unstructured.documents.elements.Title object at 0x7ab5bb2c3550>, <unstructured.documents.elements.Title object at 0x7ab5bb2c3640>, <unstructured.documents.elements.Title object at 0x7ab5bb7832e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb780100>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb782b60>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde230d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde23370>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde22d40>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde207c0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde20670>, <unstructured.documents.elements.Title object at 0x7ab5bde21ba0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde20340>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde23ac0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde23f70>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f9cf0>, <unstructured.documents.elements.Title object at 0x7ab5bde23b80>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde20070>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde23fa0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde220e0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde23250>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde23850>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde23220>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde23550>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde23730>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde236d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb527af0>, <unstructured.documents.elements.Title object at 0x7ab5bb524250>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bde233a0>, <unstructured.documents.elements.Title object at 0x7ab5be0f94b0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f9a80>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f9de0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f9300>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f9330>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0fbd30>, <unstructured.documents.elements.Title object at 0x7ab5be0f95d0>, <unstructured.documents.elements.Title object at 0x7ab5be0f85b0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f9ba0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f87c0>, <unstructured.documents.elements.Title object at 0x7ab5be0fb2e0>, <unstructured.documents.elements.Text object at 0x7ab5bde201c0>, <unstructured.documents.elements.Title object at 0x7ab5bde23190>, <unstructured.documents.elements.Title object at 0x7ab5bb04ec50>, <unstructured.documents.elements.ListItem object at 0x7ab5bb733250>, <unstructured.documents.elements.Text object at 0x7ab5bb501cc0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb769f30>, <unstructured.documents.elements.Text object at 0x7ab5bb768280>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb768d90>, <unstructured.documents.elements.Text object at 0x7ab5bb76b580>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb76ac80>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb8cd540>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb76ab90>, <unstructured.documents.elements.Text object at 0x7ab5bb76a7d0>, <unstructured.documents.elements.Title object at 0x7ab5bb769570>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb769450>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7695d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb76ace0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb76b1c0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb823ee0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb76acb0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb821db0>, <unstructured.documents.elements.Title object at 0x7ab5bb76abc0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb769360>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb768430>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb822ad0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb820a00>, <unstructured.documents.elements.Title object at 0x7ab5bb8204f0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb76a4d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb768790>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb821270>, <unstructured.documents.elements.Text object at 0x7ab5bb822d70>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb823580>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb820af0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb76a530>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb822e30>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb821e10>, <unstructured.documents.elements.Title object at 0x7ab5bb823af0>, <unstructured.documents.elements.Title object at 0x7ab5bb769150>, <unstructured.documents.elements.Text object at 0x7ab5bb76ab60>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb76a560>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb822200>, <unstructured.documents.elements.Title object at 0x7ab5bb7699c0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb7692a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb823b20>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb821f90>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb769270>, <unstructured.documents.elements.Text object at 0x7ab5bb76a5c0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb768220>, <unstructured.documents.elements.Title object at 0x7ab5bb822740>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb822710>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb822ec0>, <unstructured.documents.elements.Title object at 0x7ab5bb823700>, <unstructured.documents.elements.Title object at 0x7ab5be0e0790>, <unstructured.documents.elements.Title object at 0x7ab5bdfcca00>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bdfcc9d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb39e1a0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bdfccdf0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bb2c3ac0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0fbbb0>, <unstructured.documents.elements.Title object at 0x7ab5be0fbfd0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0fa4d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f92d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f8160>, <unstructured.documents.elements.Title object at 0x7ab5be0f8a90>, <unstructured.documents.elements.Title object at 0x7ab5bb15b280>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0fb790>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f8c40>, <unstructured.documents.elements.Title object at 0x7ab5bb15b550>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f8fd0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f9780>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f89d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f86d0>, <unstructured.documents.elements.NarrativeText object at 0x7ab5be0f9ea0>, <unstructured.documents.elements.Title object at 0x7ab5bb5e2440>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bddddc60>, <unstructured.documents.elements.NarrativeText object at 0x7ab5bddddff0>, <unstructured.documents.elements.Text object at 0x7ab5bb5e08b0>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_elements = [str(element) for element in elements]\n",
        "\n",
        "# Combine the text elements into a single string\n",
        "text = ' '.join(text_elements)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "LfgV8mIJg8q4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f0bfab-44d5-4a96-e740-b59c9472f40a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech and Language Processing. Daniel Jurafsky & James H. Martin. rights reserved. Draft of January 7, 2023. Copyright © 2023. CHAPTER 16 Automatic Speech Recognition and Text-to-Speech I KNOW not whether I see your meaning: if I do, it lies Upon the wordy wavelets of your voice, Dim as an evening shadow in a brook, Thomas Lovell Beddoes, 1851 Understanding spoken language, or at least transcribing the words into writing, is one of the earliest goals of computer language processing. In fact, speech processing predates the computer by many decades! The ﬁrst machine that recognized speech was a toy from the 1920s. “Radio Rex”, shown to the right, was a celluloid dog that moved (by means of a spring) when the spring was released by 500 Hz acous- tic energy. Since 500 Hz is roughly the ﬁrst formant of the vowel [eh] in “Rex”, Rex seemed to come when he was called (David, Jr. and Selfridge, 1962). In modern times, we expect more of our automatic systems. The task of auto- ASR matic speech recognition (ASR) is to map any waveform like this: to the appropriate string of words: It’s time for lunch! Automatic transcription of speech by any speaker in any environment is still far from solved, but ASR technology has matured to the point where it is now viable for many practical tasks. Speech is a natural interface for communicating with smart home ap- pliances, personal assistants, or cellphones, where keyboards are less convenient, in telephony applications like call-routing (“Accounting, please”) or in sophisticated dialogue applications (“I’d like to change the return date of my ﬂight”). ASR is also useful for general transcription, for example for automatically generating captions for audio or video text (transcribing movies or videos or live discussions). Transcrip- tion is important in ﬁelds like law where dictation plays an important role. Finally, ASR is important as part of augmentative communication (interaction between com- puters and humans with some disability resulting in difﬁculties or inabilities in typ- ing or audition). The blind Milton famously dictated Paradise Lost to his daughters, and Henry James dictated his later novels after a repetitive stress injury. What about the opposite problem, going from text to speech? This is a problem with an even longer history. In Vienna in 1769, Wolfgang von Kempelen built for All 2 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH the Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton consisting of a wooden box ﬁlled with gears, behind which sat a robot mannequin who played chess by moving pieces with his mechanical arm. The Turk toured Eu- rope and the Americas for decades, defeating Napoleon Bonaparte and even playing Charles Babbage. The Mechanical Turk might have been one of the early successes of artiﬁcial intelligence were it not for the fact that it was, alas, a hoax, powered by a human chess player hidden inside the box. less well known is proliﬁc inventor, also built between 1769 and 1790 what was deﬁnitely the ﬁrst full-sentence not a hoax: speech synthesizer, shown partially to the right. His device consisted of a bellows to simulate the lungs, a rub- ber mouthpiece and a nose aperture, a reed to simulate the vocal folds, var- ious whistles for the fricatives, and a small auxiliary bellows to provide the puff of air for plosives. By moving levers with both hands to open and close apertures, and adjusting the ﬂexible leather “vo- cal tract”, an operator could produce different consonants and vowels. What is that von Kempelen, an extraordinarily speech synthesis text-to-speech More than two centuries later, we no longer build our synthesizers out of wood and leather, nor do we need human operators. The modern task of speech synthesis, also called text-to-speech or TTS, is exactly the reverse of ASR; to map text: TTS It’s time for lunch! to an acoustic waveform: Modern speech synthesis has a wide variety of applications. TTS is used in conversational agents that conduct dialogues with people, plays a role in devices that read out loud for the blind or in games, and can be used to speak for sufferers of neurological disorders, such as the late astrophysicist Steven Hawking who, after he lost the use of his voice because of ALS, spoke by manipulating a TTS system. In the next sections we’ll show how to do ASR with encoder-decoders, intro- duce the CTC loss functions, the standard word error rate evaluation metric, and describe how acoustic features are extracted. We’ll then see how TTS can be mod- eled with almost the same algorithm in reverse, and conclude with a brief mention of other speech tasks. 16.1 The Automatic Speech Recognition Task digit recognition Before describing algorithms for ASR, let’s talk about how the task itself varies. One dimension of variation is vocabulary size. Some ASR tasks can be solved with extremely high accuracy, like those with a 2-word vocabulary (yes versus no) or an 11 word vocabulary like digit recognition (recognizing sequences of digits in- cluding zero to nine plus oh). Open-ended tasks like transcribing videos or human conversations, with large vocabularies of up to 60,000 words, are much harder. read speech conversational speech LibriSpeech Switchboard CALLHOME CORAAL CHiME 16.1 THE AUTOMATIC SPEECH RECOGNITION TASK 3 A second dimension of variation is who the speaker is talking to. Humans speak- ing to machines (either dictating or talking to a dialogue system) are easier to recog- nize than humans speaking to humans. Read speech, in which humans are reading out loud, for example in audio books, is also relatively easy to recognize. Recog- nizing the speech of two humans talking to each other in conversational speech, for example, for transcribing a business meeting, is the hardest. It seems that when humans talk to machines, or read without an audience present, they simplify their speech quite a bit, talking more slowly and more clearly. A third dimension of variation is channel and noise. Speech is easier to recognize if it’s recorded in a quiet room with head-mounted microphones than if it’s recorded by a distant microphone on a noisy city street, or in a car with the window open. A ﬁnal dimension of variation is accent or speaker-class characteristics. Speech is easier to recognize if the speaker is speaking the same dialect or variety that the system was trained on. Speech by speakers of regional or ethnic dialects, or speech by children can be quite difﬁcult to recognize if the system is only trained on speak- ers of standard dialects, or only adult speakers. A number of publicly available corpora with human-created transcripts are used to create ASR test and training sets to explore this variation; we mention a few of them here since you will encounter them in the literature. LibriSpeech is a large open-source read-speech 16 kHz dataset with over 1000 hours of audio books from the LibriVox project, with transcripts aligned at the sentence level (Panayotov et al., 2015). It is divided into an easier (“clean”) and a more difﬁcult portion (“other”) with the clean portion of higher recording quality and with accents closer to US English. This was done by running a speech recognizer (trained on read speech from the Wall Street Journal) on all the audio, computing the WER for each speaker based on the gold transcripts, and dividing the speakers roughly in half, with recordings from lower-WER speakers called “clean” and recordings from higher-WER speakers “other”. The Switchboard corpus of prompted telephone conversations between strangers was collected in the early 1990s; it contains 2430 conversations averaging 6 min- utes each, totaling 240 hours of 8 kHz speech and about 3 million words (Godfrey et al., 1992). Switchboard has the singular advantage of an enormous amount of auxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic and prosodic labeling, and discourse and information structure. The CALLHOME corpus was collected in the late 1990s and consists of 120 unscripted 30-minute telephone conversations between native speakers of English who were usually close friends or family (Canavan et al., 1997). The Santa Barbara Corpus of Spoken American English (Du Bois et al., 2005) is a large corpus of naturally occurring everyday spoken interactions from all over the United States, mostly face-to-face conversation, but also town-hall meetings, food preparation, on-the-job talk, and classroom lectures. The corpus was anonymized by removing personal names and other identifying information (replaced by pseudonyms in the transcripts, and masked in the audio). CORAAL is a collection of over 150 sociolinguistic interviews with African American speakers, with the goal of studying African American Language (AAL), the many variations of language used in African American communities (Kendall and Farrington, 2020). The interviews are anonymized with transcripts aligned at the utterance level. The CHiME Challenge is a series of difﬁcult shared tasks with corpora that deal with robustness in ASR. The CHiME 5 task, for example, is ASR of conversational speech in real home environments (speciﬁcally dinner parties). The 4 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH HKUST AISHELL-1 corpus contains recordings of twenty different dinner parties in real homes, each with four participants, and in three locations (kitchen, dining area, living room), recorded both with distant room microphones and with body-worn mikes. The HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone con- versations between speakers of Mandarin across China, including transcripts of the conversations, which are between either friends or strangers (Liu et al., 2006). The AISHELL-1 corpus contains 170 hours of Mandarin read speech of sentences taken from various domains, read by different speakers mainly from northern China (Bu et al., 2017). Figure 16.1 shows the rough percentage of incorrect words (the word error rate, or WER, deﬁned on page 15) from state-of-the-art systems on some of these tasks. Note that the error rate on read speech (like the LibriSpeech audiobook corpus) is around 2%; this is a solved task, although these numbers come from systems that re- quire enormous computational resources. By contrast, the error rate for transcribing conversations between humans is much higher; 5.8 to 11% for the Switchboard and CALLHOME corpora. The error rate is higher yet again for speakers of varieties like African American Vernacular English, and yet again for difﬁcult conversational tasks like transcription of 4-speaker dinner party speech, which can have error rates as high as 81.3%. Character error rates (CER) are also much lower for read Man- darin speech than for natural conversation. WER% English Tasks 1.4 LibriSpeech audiobooks 960hour clean 2.6 LibriSpeech audiobooks 960hour other 5.8 Switchboard telephone conversations between strangers 11.0 CALLHOME telephone conversations between family 27.0 Sociolinguistic interviews, CORAAL (AAL) 47.9 CHiMe5 dinner parties with body-worn microphones 81.3 CHiMe5 dinner parties with distant microphones CER% Chinese (Mandarin) Tasks 6.7 AISHELL-1 Mandarin read speech corpus HKUST Mandarin Chinese telephone conversations 23.5 Figure 16.1 Rough Word Error Rates (WER = % of words misrecognized) reported around 2020 for ASR on various American English recognition tasks, and character error rates (CER) for two Chinese recognition tasks. 16.2 Feature Extraction for ASR: Log Mel Spectrum feature vector The ﬁrst step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, each vector representing the information in a small time window of the signal. Let’s see how to convert a raw waveﬁle to the most commonly used features, sequences of log mel spectrum vectors. A speech signal processing course is recommended for more details. 16.2.1 Sampling and Quantization Recall from Section ?? that the ﬁrst step is to convert the analog representations (ﬁrst air pressure and then analog electric signals in a microphone) into a digital signal. sampling sampling rate Nyquist frequency telephone- bandwidth quantization stationary non-stationary frame stride rectangular Hamming 16.2 FEATURE EXTRACTION FOR ASR: LOG MEL SPECTRUM 5 This analog-to-digital conversion has two steps: sampling and quantization. A signal is sampled by measuring its amplitude at a particular time; the sampling rate is the number of samples taken per second. To accurately measure a wave, we must have at least two samples in each cycle: one measuring the positive part of the wave and one measuring the negative part. More than two samples per cycle increases the amplitude accuracy, but less than two samples will cause the frequency of the wave to be completely missed. Thus, the maximum frequency wave that can be measured is one whose frequency is half the sample rate (since every cycle needs two samples). This maximum frequency for a given sampling rate is called the Nyquist frequency. Most information in human speech is in frequencies below 10,000 Hz, so a 20,000 Hz sampling rate would be necessary for complete accuracy. But telephone speech is ﬁltered by the switching network, and only frequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz sampling rate is sufﬁcient for telephone-bandwidth speech, and 16,000 Hz for microphone speech. Although using higher sampling rates produces higher ASR accuracy, we can’t combine different sampling rates for training and testing ASR systems. Thus if we are testing on a telephone corpus like Switchboard (8 KHz sampling), we must downsample our training corpus to 8 KHz. Similarly, if we are training on mul- tiple corpora and one of them includes telephone speech, we downsample all the wideband corpora to 8Khz. Amplitude measurements are stored as integers, either 8 bit (values from -128– 127) or 16 bit (values from -32768–32767). This process of representing real-valued numbers as integers is called quantization; all values that are closer together than the minimum granularity (the quantum size) are represented identically. We refer to each sample at time index n in the digitized, quantized waveform as x[n]. 16.2.2 Windowing From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a par- ticular phoneme. Inside this small window, we can roughly think of the signal as stationary (that is, its statistical properties are constant within this region). (By contrast, in general, speech is a non-stationary signal, meaning that its statistical properties are not constant over time). We extract this roughly stationary portion of speech by using a window which is non-zero inside a region and zero elsewhere, run- ning this window across the speech signal and multiplying it by the input waveform to produce a windowed waveform. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. To extract the signal we multiply the value of the signal at time n, s[n] by the value of the window at time n, w[n]: y[n] = w[n]s[n] The window shape sketched in Fig. 16.2 is rectangular; you can see the ex- tracted windowed signal looks just like the original signal. The rectangular window, however, abruptly cuts off the signal at its boundaries, which creates problems when we do Fourier analysis. For this reason, for acoustic feature creation we more com- monly use the Hamming window, which shrinks the values of the signal toward (16.1) 6 CHAPTER 16 Discrete Fourier transform DFT AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH Shift10 ms Window25 ms Window25 ms Window25 ms Shift10 ms Figure 16.2 Windowing, showing a 25 ms rectangular window with a 10ms stride. zero at the window boundaries, avoiding discontinuities. Figure 16.3 shows both; the equations are as follows (assuming a window that is L frames long): (cid:26) 1 0 1 L n 0 otherwise 0.46 cos( 2πn L ) w[n] = rectangular ≤ ≤ − (cid:26) 0.54 0 L n 0 otherwise 1 w[n] = Hamming − − ≤ ≤ 0.0475896 –0.4826 0.4999 Time (s)0.00455938 0 0 Time (s)0.00455938 Rectangular windowHamming window –0.5 –0.5 0.0256563 Time (s)0 0.4999 0 0.4999 0.0256563 Figure 16.3 Windowing a sine wave with the rectangular or Hamming windows. 16.2.3 Discrete Fourier Transform The next step is to extract spectral information for our windowed signal; we need to know how much energy the signal contains at different frequency bands. The tool for extracting spectral information for discrete frequency bands for a discrete-time (sampled) signal is the discrete Fourier transform or DFT. (16.2) (16.3) Euler’s formula fast Fourier transform FFT mel 16.2 FEATURE EXTRACTION FOR ASR: LOG MEL SPECTRUM 7 The input to the DFT is a windowed signal x[n]...x[m], and the output, for each of N discrete frequency bands, is a complex number X[k] representing the magnitude and phase of that frequency component in the original signal. If we plot the mag- nitude against the frequency, we can visualize the spectrum that we introduced in Chapter 28. For example, Fig. 16.4 shows a 25 ms Hamming-windowed portion of a signal and its spectrum as computed by a DFT (with some additional smoothing). 0.04414 Time (s)0.0141752 –0.04121 0.039295 0 Sound pressure level (dB/Hz)–20 20 8000 Frequency (Hz)0 0 (a) (b) (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy] Figure 16.4 and (b) its spectrum computed by a DFT. We do not introduce the mathematical details of the DFT here, except to note that Fourier analysis relies on Euler’s formula, with j as the imaginary unit: e jθ = cos θ + j sin θ (16.4) As a brief reminder for those students who have already studied signal processing, the DFT is deﬁned as follows: X[k] = N 1 (cid:88) − x[n]e− j 2π N kn (16.5) n=0 A commonly used algorithm for computing the DFT is the fast Fourier transform or FFT. This implementation of the DFT is very efﬁcient but only works for values of N that are powers of 2. 16.2.4 Mel Filter Bank and Log The results of the FFT tell us the energy at each frequency band. Human hearing, however, is not equally sensitive at all frequency bands; it is less sensitive at higher frequencies. This bias toward low frequencies helps human recognition, since infor- mation in low frequencies like formants is crucial for distinguishing values or nasals, while information in high frequencies like stop bursts or fricative noise is less cru- cial for successful recognition. Modeling this human perceptual property improves speech recognition performance in the same way. We implement this intuition by collecting energies, not equally at each frequency band, but according to the mel scale, an auditory frequency scale (Chapter 28). A mel (Stevens et al. 1937, Stevens and Volkmann 1940) is a unit of pitch. Pairs of sounds that are perceptually equidistant in pitch are separated by an equal number of mels. The mel frequency m can be computed from the raw acoustic frequency by a log transformation: mel( f ) = 1127 ln(1 + f 700 ) (16.6) 8 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH We implement this intuition by creating a bank of ﬁlters that collect energy from each frequency band, spread logarithmically so that we have very ﬁne resolution at low frequencies, and less resolution at high frequencies. Figure 16.5 shows a sample bank of triangular ﬁlters that implement this idea, that can be multiplied by the spectrum to get a mel spectrum. m1 0 0.5 mM 8K 0 AmplitudeFrequency (Hz) m2 1 ...mel spectrum 7700 Figure 16.5 The mel ﬁlter bank (Davis and Mermelstein, 1980). Each triangular ﬁlter, spaced logarithmically along the mel scale, collects energy from a given frequency range. Finally, we take the log of each of the mel spectrum values. The human response to signal level is logarithmic (like the human response to frequency). Humans are less sensitive to slight differences in amplitude at high amplitudes than at low ampli- tudes. In addition, using a log makes the feature estimates less sensitive to variations in input such as power variations due to the speaker’s mouth moving closer or further from the microphone. 16.3 Speech Recognition Architecture The basic architecture for ASR is the encoder-decoder (implemented with either RNNs or Transformers), exactly the same architecture introduced for MT in Chap- ter 13. Generally we start from the log mel spectral features described in the previous section, and map to letters, although it’s also possible to map to induced morpheme- like chunks like wordpieces or BPE. AED listen attend and spell Fig. 16.6 sketches the standard encoder-decoder architecture, which is com- monly referred to as the attention-based encoder decoder or AED, or listen attend and spell (LAS) after the two papers which ﬁrst applied it to speech (Chorowski et al. 2014, Chan et al. 2016). The input is a sequence of t acoustic feature vectors F = f1, f2, ..., ft , one vector per 10 ms frame. The output can be letters or word- pieces; we’ll assume letters here. Thus the output sequence Y = ( , y1, ..., ym(cid:104) SOS (cid:105) (cid:104) eos and sos assuming special start of sequence and end of sequence tokens (cid:105) (cid:104) (cid:105) (cid:104) each yi is a character; for English we might choose the set: yi ∈ { Of course the encoder-decoder architecture is particularly appropriate when in- put and output sequences have stark length differences, as they do for speech, with very long acoustic feature sequences mapping to much shorter sequences of letters or words. A single word might be 5 letters long but, supposing it lasts about 2 seconds, would take 200 acoustic frames (of 10ms each). a, b, c, ..., z, 0, ..., 9, , space (cid:105) (cid:104) comma (cid:104) , (cid:105) period (cid:104) , (cid:105) apostrophe (cid:104) , (cid:105) unk (cid:104) (cid:105)} Because this length difference is so extreme for speech, encoder-decoder ar- chitectures for speech need to have a special compression stage that shortens the acoustic feature sequence before the encoder stage. (Alternatively, we can use a loss ), EOS (cid:105) and low frame rate n-best list rescore 16.3 SPEECH RECOGNITION ARCHITECTURE 9 ft f1 y7ti y5s y3t‘ Subsampling…H … y1<s>i y2it Feature Computation DECODER … xn y8im ENCODER… y4‘s 80-dimensional log Mel spectrumper frameShorter sequence X y9me ym x1 y6 t Figure 16.6 Schematic architecture for an encoder-decoder speech recognizer. function that is designed to deal well with compression, like the CTC loss function we’ll introduce in the next section.) The goal of the subsampling is to produce a shorter sequence X = x1, ..., xn that will be the input to the encoder. The simplest algorithm is a method sometimes called low frame rate (Pundak and Sainath, 2016): for time i we stack (concatenate) the acoustic feature vector fi with the prior two vectors fi 2 to make a new vector three times longer. Then we simply delete fi 2. Thus instead of (say) a 40-dimensional acoustic feature vector every 10 ms, we have a longer vector (say 120-dimensional) every 30 ms, with a shorter sequence length n = t 1 and fi − 1 and fi − − − 3 .1 After this compression stage, encoder-decoders for speech use the same archi- tecture as for MT or other text, composed of either RNNs (LSTMs) or Transformers. For inference, the probability of the output string Y is decomposed as: p(y1, . . . , yn) = n (cid:89) i=1 p(yi| y1, . . . , yi − 1, X) (16.7) We can produce each letter of the output via greedy decoding: ˆyi = argmaxchar ∈ AlphabetP(char y1...yi | − 1, X) (16.8) Alternatively we can use beam search as described in the next section. This is par- ticularly relevant when we are adding a language model. Adding a language model Since an encoder-decoder model is essentially a con- ditional language model, encoder-decoders implicitly learn a language model for the output domain of letters from their training data. However, the training data (speech paired with text transcriptions) may not include sufﬁcient text to train a good lan- guage model. After all, it’s easier to ﬁnd enormous amounts of pure text training data than it is to ﬁnd text paired with speech. Thus we can can usually improve a model at least slightly by incorporating a very large language model. The simplest way to do this is to use beam search to get a ﬁnal beam of hy- pothesized sentences; this beam is sometimes called an n-best list. We then use a language model to rescore each hypothesis on the beam. The scoring is done by in- 1 There are also more complex alternatives for subsampling, like using a convolutional net that down- samples with max pooling, or layers of pyramidal RNNs, RNNs where each successive layer has half the number of RNNs as the previous layer. 10 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH terpolating the score assigned by the language model with the encoder-decoder score used to create the beam, with a weight λ tuned on a held-out set. Also, since most models prefer shorter sentences, ASR systems normally have some way of adding a length factor. One way to do this is to normalize the probability by the number of characters in the hypothesis |c. The following is thus a typical scoring function (Chan et al., 2016): Y | score(Y X) = | 1 Y | |c log P(Y X) + λ log PLM(Y ) | 16.3.1 Learning Encoder-decoders for speech are trained with the normal cross-entropy loss gener- ally used for conditional language models. At timestep i of decoding, the loss is the log probability of the correct token (letter) yi: LCE = − log p(yi| y1, . . . , yi − 1, X) The loss for the entire sentence is the sum of these losses: LCE = − m (cid:88) i=1 log p(yi| y1, . . . , yi − 1, X) This loss is then backpropagated through the entire end-to-end model to train the entire encoder-decoder. As we described in Chapter 13, we normally use teacher forcing, in which the decoder history is forced to be the correct gold yi rather than the predicted ˆyi. It’s also possible to use a mixture of the gold and decoder output, for example using the gold output 90% of the time, but with probability .1 taking the decoder output instead: LCE = − log p(yi| y1, . . . , ˆyi − 1, X) 16.4 CTC We pointed out in the previous section that speech recognition has two particular properties that make it very appropriate for the encoder-decoder architecture, where the encoder produces an encoding of the input that the decoder uses attention to explore. First, in speech we have a very long acoustic input sequence X mapping to a much shorter sequence of letters Y , and second, it’s hard to know exactly which part of X maps to which part of Y . CTC In this section we brieﬂy introduce an alternative to encoder-decoder: an algo- rithm and loss function called CTC, short for Connectionist Temporal Classiﬁca- tion (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that the output is the same length as the input, and then to apply a collapsing function that combines sequences of identical letters, resulting in a shorter sequence. Let’s imagine inference on someone saying the word dinner, and let’s suppose we had a function that chooses the most probable letter for each input spectral frame representation xi. We’ll call the sequence of letters corresponding to each input (16.9) (16.10) (16.11) (16.12) alignment blank 16.4 CTC 11 frame an alignment, because it tells us where in the acoustic signal each letter aligns to. Fig. 16.7 shows one such alignment, and what happens if we use a collapsing function that just removes consecutive duplicate letters. d r r x6 X (input)A (alignment)Y (output) d n e n x9 n wavefile x8 x5 x13 n n r x4 e r x14 r x12 r x2 x11 x7 x10 i i i x1 x3 r Figure 16.7 A naive algorithm for collapsing an alignment between input and letters. Well, that doesn’t work; our naive algorithm has transcribed the speech as diner, not dinner! Collapsing doesn’t handle double letters. There’s also another problem with our naive function; it doesn’t tell us what symbol to align with silence in the input. We don’t want to be transcribing silence as random letters! The CTC algorithm solves both problems by adding to the transcription alphabet a special symbol for a blank, which we’ll represent as . The blank can be used in the alignment whenever we don’t want to transcribe a letter. Blank can also be used between letters; since our collapsing function collapses only consecutive duplicate letters, it won’t collapse across . More formally, let’s deﬁne the mapping B : a y between an alignment a and an output y, which collapses all repeated letters and then removes all blanks. Fig. 16.8 sketches this collapsing function B. → r r x8 d x6 n n n r r n n n x5 x13 r e e e x9 r x11 X (input)A (alignment)remove blanks n r d e d x12 x4 x3 d x2 x14 n␣␣␣␣␣␣␣ Y (output) i i i i nmerge duplicates x7 x10 x1 Figure 16.8 The CTC collapsing function B, showing the space blank character (consecutive) characters in an alignment A are removed to form the output Y . ; repeated The CTC collapsing function is many-to-one; lots of different alignments map to the same output string. For example, the alignment shown in Fig. 16.8 is not the only alignment that results in the string dinner. Fig. 16.9 shows some other alignments that would produce the same output. e n n n n n n n d d r␣ d d d e n d e r r r r e n r ␣␣␣ i i e i r ␣␣ i␣␣␣␣␣ Figure 16.9 Three other legitimate alignments producing the transcript dinner. It’s useful to think of the set of all alignments that might produce the same output 12 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH Y . We’ll use the inverse of our B function, called B− B− 1(Y ). 1, and represent that set as 16.4.1 CTC Inference X) let’s ﬁrst see how CTC assigns a proba- Before we see how to compute PCTC(Y | bility to one particular alignment ˆA = ˆa1, . . . , ˆan} . CTC makes a strong conditional { independence assumption: it assumes that, given the input X, the CTC model output at at time t is independent of the output labels at any other time ai. Thus: X) = PCTC(A | T (cid:89) t=1 p(at | X) (16.13) Thus to ﬁnd the best alignment ˆA = ˆa1, . . . , ˆaT } { ter with the max probability at each time step t: we can greedily choose the charac- ˆat = argmax c ∈ C X) pt (c | (16.14) We then pass the resulting sequence A to the CTC collapsing function B to get the output sequence Y . Let’s talk about how this simple inference algorithm for ﬁnding the best align- ment A would be implemented. Because we are making a decision at each time point, we can treat CTC as a sequence-modeling task, where we output one letter ˆyt at time t corresponding to each input token xt , eliminating the need for a full de- coder. Fig. 16.10 sketches this architecture, where we take an encoder, produce a hidden state ht at each timestep, and decode by taking a softmax over the character vocabulary at each time step. ENCODER… y1iy2iy3iy4t Feature Computation Subsampling… yn … ft xnClassiﬁer+softmax log Mel spectrumShorter inputsequence X x1 ty5……output lettersequence Y f1 Figure 16.10 simple softmaxes over the hidden state ht at each output step. Inference with CTC: using an encoder-only model, with decoding done by Alas, there is a potential ﬂaw with the inference algorithm sketched in (Eq. 16.14) and Fig. 16.9. The problem is that we chose the most likely alignment A, but the most likely alignment may not correspond to the most likely ﬁnal collapsed output string Y . That’s because there are many possible alignments that lead to the same output string, and hence the most likely output string might not correspond to the 16.4 CTC 13 most probable alignment. For example, imagine the most probable alignment A for an input X = [x1x2x3] is the string [a b (cid:15)] but the next two most probable alignments are [b (cid:15) b] and [(cid:15) b b]. The output Y =[b b], summing over those two alignments, might be more probable than Y =[a b]. For this reason, the most probable output sequence Y is the one that has, not the single best CTC alignment, but the highest sum over the probability of all its possible alignments: PCTC(Y (cid:88) X) = | X) | P(A 1(Y ) A B− ∈ (cid:88) T (cid:89) p(at | ht ) = t=1 1(Y ) A ∈ ˆY = argmax B− X) | PCTC(Y Y (16.15) Alas, summing over all alignments is very expensive (there are a lot of alignments), so we approximate this sum by using a version of Viterbi beam search that cleverly keeps in the beam the high-probability alignments that map to the same output string, and sums those as an approximation of (Eq. 16.15). See Hannun (2017) for a clear explanation of this extension of beam search for CTC. Because of the strong conditional independence assumption mentioned earlier (that the output at time t is independent of the output at time t 1, given the input), CTC does not implicitly learn a language model over the data (unlike the attention- based encoder-decoder architectures). It is therefore essential when using CTC to interpolate a language model (and some sort of length factor L(Y )) using interpola- tion weights that are trained on a dev set: − scoreCTC(Y X) = log PCTC(Y | X) + λ1 log PLM(Y )λ2L(Y ) | (16.16) 16.4.2 CTC Training To train a CTC-based ASR system, we use negative log-likelihood loss with a special CTC loss function. Thus the loss for an entire dataset D is the sum of the negative log-likelihoods of the correct output Y for each input X: LCTC = (cid:88) (X,Y ) ∈ D − log PCTC(Y X) | (16.17) To compute CTC loss function for a single input pair (X,Y ), we need the probability of the output Y given the input X. As we saw in Eq. 16.15, to compute the probability of a given output Y we need to sum over all the possible alignments that would collapse to Y . In other words: T (cid:89) (cid:88) PCTC(Y X) = | p(at | ht ) 1(Y ) t=1 A B− ∈ Naively summing over all possible alignments is not feasible (there are too many alignments). However, we can efﬁciently compute the sum by using dynamic pro- gramming to merge alignments, with a version of the forward-backward algo- rithm also used to train HMMs (Appendix A) and CRFs. The original dynamic pro- gramming algorithms for both training and inference are laid out in (Graves et al., 2006); see (Hannun, 2017) for a detailed explanation of both. (16.18) 14 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH 16.4.3 Combining CTC and Encoder-Decoder It’s also possible to combine the two architectures/loss functions we’ve described, the cross-entropy loss from the encoder-decoder architecture, and the CTC loss. Fig. 16.11 shows a sketch. For training, we can simply weight the two losses with a λ tuned on a dev set: L = − λ log Pencdec(Y X) | − (1 − λ ) log Pctc(Y X) | (16.19) For inference, we can combine the two with the language model (or the length penalty), again with learned weights: ˆY = argmax Y [λ log Pencdec(Y X) | − (1 − λ ) log PCTC(Y X) + γ log PLM(Y )] (16.20) | H CTC Loss i m xn ENCODER… s … ‘ i Encoder-Decoder Loss t t <s> x1 DECODER ……i t ’ s t i m e … Figure 16.11 Combining the CTC and encoder-decoder loss functions. 16.4.4 Streaming Models: RNN-T for improving CTC streaming Because of the strong independence assumption in CTC (assuming that the output at time t is independent of the output at time t 1), recognizers based on CTC don’t achieve as high an accuracy as the attention-based encoder-decoder recog- nizers. CTC recognizers have the advantage, however, that they can be used for streaming. Streaming means recognizing words on-line rather than waiting until the end of the sentence to recognize them. Streaming is crucial for many applica- tions, from commands to dictation, where we want to start recognition while the user is still talking. Algorithms that use attention need to compute the hidden state sequence over the entire input ﬁrst in order to provide the attention distribution con- text, before the decoder can start decoding. By contrast, a CTC algorithm can input letters from left to right immediately. − RNN-T If we want to do streaming, we need a way to improve CTC recognition to re- move the conditional independent assumption, enabling it to know about output his- tory. The RNN-Transducer (RNN-T), shown in Fig. 16.12, is just such a model (Graves 2012, Graves et al. 2013). The RNN-T has two main components: a CTC acoustic model, and a separate language model component called the predictor that conditions on the output token history. At each time step t, the CTC encoder outputs a hidden state henc given the input x1...xt . The language model predictor takes as in- put the previous output token (not counting blanks), outputting a hidden state hpred . The two are passed through another network whose output is then passed through a t u 16.5 ASR EVALUATION: WORD ERROR RATE 15 softmax to predict the next character. PRNN − T (Y X) = | = A A (cid:88) 1(Y ) B− ∈ (cid:88) ∈ B− 1(Y ) X) P(A | T (cid:89) t=1 p(at | ht , y<ut ) SOFTMAX JOINT NETWORK xt hpredu yu-1 zt,uDECODER ENCODERP ( yt,u | x[1..t] , y[1..u-1] ) henct PREDICTIONNETWORK Figure 16.12 The RNN-T model computing the output token distribution at time t by inte- grating the output of a CTC acoustic encoder and a separate ‘predictor’ language model. 16.5 ASR Evaluation: Word Error Rate word error The standard evaluation metric for speech recognition systems is the word error rate. The word error rate is based on how much the word string returned by the recognizer (the hypothesized word string) differs from a reference transcription. The ﬁrst step in computing word error is to compute the minimum edit distance in words between the hypothesized and correct strings, giving us the minimum num- ber of word substitutions, word insertions, and word deletions necessary to map between the correct and hypothesized strings. The word error rate (WER) is then deﬁned as follows (note that because the equation includes insertions, the error rate can be greater than 100%): Word Error Rate = 100 × Insertions + Substitutions + Deletions Total Words in Correct Transcript alignment Here is a sample alignment between a reference and a hypothesis utterance from the CallHome corpus, showing the counts used to compute the error rate: REF: HYP: Eval: i *** ** UM the PHONE IS i GOT IT TO the ***** I I S D i LEFT THE portable **** FULLEST i LOVE TO portable FORM OF S S S I PHONE UPSTAIRS last night last night STORES S S This utterance has six substitutions, three insertions, and one deletion: Word Error Rate = 100 6 + 3 + 1 13 = 76.9% The standard method for computing word error rates is a free script called sclite, available from the National Institute of Standards and Technologies (NIST) (NIST, 16 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH Sentence error rate 2005). Sclite is given a series of reference (hand-transcribed, gold-standard) sen- tences and a matching set of hypothesis sentences. Besides performing alignments, and computing word error rate, sclite performs a number of other useful tasks. For example, for error analysis it gives useful information such as confusion matrices showing which words are often misrecognized for others, and summarizes statistics of words that are often inserted or deleted. sclite also gives error rates by speaker (if sentences are labeled for speaker ID), as well as useful statistics like the sentence error rate, the percentage of sentences with at least one word error. Statistical signiﬁcance for ASR: MAPSSWE or MacNemar As with other language processing algorithms, we need to know whether a particular improvement in word error rate is signiﬁcant or not. The standard statistical tests for determining if two word error rates are different is the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in Gillick and Cox (1989). The MAPSSWE test is a parametric test that looks at the difference between the number of word errors the two systems produce, averaged across a number of segments. The segments may be quite short or as long as an entire utterance; in general, we want to have the largest number of (short) segments in order to justify the normality assumption and to maximize power. The test requires that the errors in one segment be statistically independent of the errors in another segment. Since ASR systems tend to use trigram LMs, we can approximate this requirement by deﬁning a segment as a region bounded on both sides by words that both recognizers get correct (or by turn/utterance boundaries). Here’s an example from NIST (2007) with four regions: I II III IV REF: |it was|the best|of|times it|was the worst|of times| |it was | | |the best|of|times it|IS the worst |of times|OR|it was | | | | | | | SYS A:|ITS | SYS B:|it was|the best| | | |times it|WON the TEST |of times| |it was | | | | In region I, system A has two errors (a deletion and an insertion) and system B has zero; in region III, system A has one error (a substitution) and system B has two. Let’s deﬁne a sequence of variables Z representing the difference between the errors in the two systems as follows: Ni A Ni B Z the number of errors made on segment i by system A the number of errors made on segment i by system B Ni Ni B, i = 1, 2, , n where n is the number of segments A − · · In the example above, the sequence of Z values is − · · In the example above, the sequence of Z values is − · · In the example above, the sequence of Z values is − · · In the example above, the sequence of Z values is − σ 2 z = n 1 − 1 n (cid:88) i=1 (Zi − µz)2 (16.21) 16.6 TTS 17 Let W = ˆµz σz/√n (16.22) For a large enough n (> 50), W will approximately have a normal distribution with unit variance. The null hypothesis is H0 : µz = 0, and it can thus be rejected if 0.05 (one-tailed), where Z is 2 standard normal and w is the realized value W ; these probabilities can be looked up in the standard tables of the normal distribution. P(Z ) w | 0.05 (two-tailed) or P(Z ) w | ∗ ≥ | ≤ ≥ | ≤ McNemar’s test Earlier work sometimes used McNemar’s test for signiﬁcance, but McNemar’s is only applicable when the errors made by the system are independent, which is not true in continuous speech recognition, where errors made on a word are extremely dependent on errors made on neighboring words. Could we improve on word error rate as a metric? It would be nice, for exam- ple, to have something that didn’t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difﬁcult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 15 on page ??. 16.6 TTS The goal of text-to-speech (TTS) systems is to map from strings of letters to wave- forms, a technology that’s important for a variety of applications from dialogue sys- tems to games to education. Like ASR systems, TTS systems are generally based on the encoder-decoder architecture, either using LSTMs or Transformers. There is a general difference in training. The default condition for ASR systems is to be speaker-independent: they are trained on large corpora with thousands of hours of speech from many speakers because they must generalize well to an unseen test speaker. By contrast, in TTS, it’s less crucial to use multiple voices, and so basic TTS systems are speaker-dependent: trained to have a consistent voice, on much less data, but all from one speaker. For example, one commonly used public domain dataset, the LJ speech corpus, consists of 24 hours of one speaker, Linda Johnson, reading audio books in the LibriVox project (Ito and Johnson, 2017), much smaller than standard ASR corpora which are hundreds or thousands of hours.2 We generally break up the TTS task into two components. The ﬁrst component is an encoder-decoder model for spectrogram prediction: it maps from strings of letters to mel spectrographs: sequences of mel spectral values over time. Thus we 2 There is also recent TTS research on the task of multi-speaker TTS, in which a system is trained on speech from many speakers, and can switch between different voices. 18 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH might map from this string: It’s time for lunch! to the following mel spectrogram: vocoding vocoder The second component maps from mel spectrograms to waveforms. Generating waveforms from intermediate representations like spectrograms is called vocoding and this second component is called a vocoder: These standard encoder-decoder algorithms for TTS are still quite computation- ally intensive, so a signiﬁcant focus of modern research is on ways to speed them up. 16.6.1 TTS Preprocessing: Text normalization non-standard words Before either of these two steps, however, TTS systems require text normaliza- tion preprocessing for handling non-standard words: numbers, monetary amounts, dates, and other concepts that are verbalized differently than they are spelled. A TTS system seeing a number like 151 needs to know to verbalize it as one hundred ﬁfty one if it occurs as $151 but as one ﬁfty one if it occurs in the context 151 Chapulte- pec Ave.. The number 1750 can be spoken in at least four different ways, depending on the context: seventeen fifty: one seven five zero: seventeen hundred and fifty: one thousand, seven hundred, and fifty: (in “The European economy in 1750”) (in “The password is 1750”) (in “1750 dollars”) (in “1750 dollars”) Often the verbalization of a non-standard word depends on its meaning (what Taylor (2009) calls its semiotic class). Fig. 16.13 lays out some English non- standard word types. Many classes have preferred realizations. A year is generally read as paired digits (e.g., seventeen fifty for 1750). $3.2 billion must be read out with the word dollars at the end, as three point two billion dollars. Some ab- breviations like N.Y. are expanded (to New York), while other acronyms like GPU are pronounced as letter sequences. In languages with grammatical gender, normal- ization may depend on morphological properties. In French, the phrase 1 mangue (‘one mangue’) is normalized to une mangue, but 1 ananas (‘one pineapple’) is normalized to un ananas. In German, Heinrich IV (‘Henry IV’) can be normalized to Heinrich der Vierte, Heinrich des Vierten, Heinrich dem Vierten, or Heinrich den Vierten depending on the grammatical case of the noun (Demberg, 2006). 16.6 TTS semiotic class abbreviations acronyms read as letters GPU, D.C., PC, UN, IBM G P U twelve cardinal numbers seventh ordinal numbers one oh one numbers read as digits eleven forty ﬁve times February twenty eighth dates nineteen ninety nine years three dollars forty ﬁve money three point four ﬁve billion dollars money in tr/m/billions seventy ﬁve percent percentage examples gov’t, N.Y., mph verbalization government 12, 45, 1/2, 0.6 May 7, 3rd, Bill Gates III Room 101 3.20, 11:45 28/02 (or in US, 2/28) 1999, 80s, 1900s, 2045 $3.45, e250, $200K $3.45 billion 75% 3.4% Figure 16.13 Some types of non-standard words in text normalization; see Sproat et al. (2001) and (van Esch and Sproat, 2018) for many more. Modern end-to-end TTS systems can learn to do some normalization themselves, but TTS systems are only trained on a limited amount of data (like the 220,000 words we mentioned above for the LJ corpus (Ito and Johnson, 2017)), and so a separate normalization step is important. Normalization can be done by rule or by an encoder-decoder model. Rule-based normalization is done in two stages: tokenization and verbalization. In the tokeniza- tion stage we hand-write write rules to detect non-standard words. These can be regular expressions, like the following for detecting years: (20[0-9][0-9]/ /(1[89][0-9][0-9]) | A second pass of rules express how to verbalize each semiotic class. Larger TTS systems instead use more complex rule-systems, like the Kestral system of (Ebden and Sproat, 2015), which ﬁrst classiﬁes and parses each input into a normal form and then produces text using a verbalization grammar. Rules have the advantage that they don’t require training data, and they can be designed for high precision, but can be brittle, and require expert rule-writers so are hard to maintain. The alternative model is to use encoder-decoder models, which have been shown to work better than rules for such transduction tasks, but do require expert-labeled training sets in which non-standard words have been replaced with the appropriate verbalization; such training sets for some languages are available (Sproat and Gor- man 2018, Zhang et al. 2019). In the simplest encoder-decoder setting, we simply treat the problem like ma- chine translation, training a system to map from: They live at 224 Mission St. to They live at two twenty four Mission Street While encoder-decoder algorithms are highly accurate, they occasionally pro- duce errors that are egregious; for example normalizing 45 minutes as forty ﬁve mil- limeters. To address this, more complex systems use mechanisms like lightweight covering grammars, which enumerate a large set of possible verbalizations but don’t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang et al., 2019). 16.6.2 TTS: Spectrogram prediction The exact same architecture we described for ASR—the encoder-decoder with attention– can be used for the ﬁrst component of TTS. Here we’ll give a simpliﬁed overview 19 20 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH Tacotron2 Wavenet of the Tacotron2 architecture (Shen et al., 2018), which extends the earlier Tacotron (Wang et al., 2017) architecture and the Wavenet vocoder (van den Oord et al., 2016). Fig. 16.14 sketches out the entire architecture. location-based attention The encoder’s job is to take a sequence of letters and produce a hidden repre- sentation representing the letter sequence, which is then used by the attention mech- anism in the decoder. The Tacotron2 encoder ﬁrst maps every input grapheme to a 512-dimensional character embedding. These are then passed through a stack of 3 convolutional layers, each containing 512 ﬁlters with shape 5 1, i.e. each ﬁlter spanning 5 characters, to model the larger letter context. The output of the ﬁnal convolutional layer is passed through a biLSTM to produce the ﬁnal encod- ing. It’s common to use a slightly higher quality (but slower) version of attention called location-based attention, in which the computation of the α values (Eq. ?? in Chapter 13) makes use of the α values from the prior time-state. × In the decoder, the predicted mel spectrum from the prior time slot is passed through a small pre-net as a bottleneck. This prior output is then concatenated with the encoder’s attention vector context and passed through 2 LSTM layers. The out- put of this LSTM is used in two ways. First, it is passed through a linear layer, and some output processing, to autoregressively predict one 80-dimensional log-mel ﬁl- terbank vector frame (50 ms, with a 12.5 ms stride) at each step. Second, it is passed through another linear layer to a sigmoid to make a “stop token prediction” decision about whether to stop producing output. Fig.1.BlockdiagramoftheTacotron2systemarchitecture.post-netlayeriscomprisedof512ﬁlterswithshape5⇥1withbatchnormalization,followedbytanhactivationsonallbuttheﬁnallayer.Weminimizethesummedmeansquarederror(MSE)frombeforeandafterthepost-nettoaidconvergence.Wealsoexperimentedwithalog-likelihoodlossbymodelingtheoutputdistributionwithaMixtureDensityNetwork[23,24]toavoidassumingaconstantvarianceovertime,butfoundthattheseweremoredifﬁculttotrainandtheydidnotleadtobettersoundingsamples.Inparalleltospectrogramframeprediction,theconcatenationofdecoderLSTMoutputandtheattentioncontextisprojecteddowntoascalarandpassedthroughasigmoidactivationtopredicttheprobabilitythattheoutputsequencehascompleted.This“stoptoken”predictionisusedduringinferencetoallowthemodeltodynamicallydeterminewhentoterminategenerationinsteadofalwaysgeneratingforaﬁxedduration.Speciﬁcally,generationcompletesattheﬁrstframeforwhichthisprobabilityexceedsathresholdof0.5.Theconvolutionallayersinthenetworkareregularizedusingdropout[25]withprobability0.5,andLSTMlayersareregularizedusingzoneout[26]withprobability0.1.Inordertointroduceoutputvariationatinferencetime,dropoutwithprobability0.5isappliedonlytolayersinthepre-netoftheautoregressivedecoder.IncontrasttotheoriginalTacotron,ourmodelusessimplerbuild-ingblocks,usingvanillaLSTMandconvolutionallayersintheen-coderanddecoderinsteadof“CBHG”stacksandGRUrecurrentlayers.Wedonotusea“reductionfactor”,i.e.,eachdecoderstepcorrespondstoasinglespectrogramframe.2.3.WaveNetVocoderWeuseamodiﬁedversionoftheWaveNetarchitecturefrom[8]toinvertthemelspectrogramfeaturerepresentationintotime-domainwaveformsamples.Asintheoriginalarchitecture,thereare30dilatedconvolutionlayers,groupedinto3dilationcycles,i.e.,thedilationrateoflayerk(k=0...29)is2k(mod10).Toworkwiththe12.5msframehopofthespectrogramframes,only2upsamplinglayersareusedintheconditioningstackinsteadof3layers.Insteadofpredictingdiscretizedbucketswithasoftmaxlayer,wefollowPixelCNN++[27]andParallelWaveNet[28]andusea10-componentmixtureoflogisticdistributions(MoL)togenerate16-bitsamplesat24kHz.Tocomputethelogisticmixturedistribution,theWaveNetstackoutputispassedthroughaReLUactivationfollowedEncoderDecoderVocoder (cid:22)(cid:4)(cid:48)(cid:55)(cid:56)(cid:49)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:87) (cid:48)(cid:77)(cid:82)(cid:73)(cid:69)(cid:86)(cid:4)(cid:52)(cid:86)(cid:83)(cid:78)(cid:73)(cid:71)(cid:88)(cid:77)(cid:83)(cid:82) (cid:59)(cid:69)(cid:90)(cid:73)(cid:50)(cid:73)(cid:88)(cid:4)(cid:49)(cid:83)(cid:48) (cid:59)(cid:69)(cid:90)(cid:73)(cid:74)(cid:83)(cid:86)(cid:81)(cid:4)(cid:55)(cid:69)(cid:81)(cid:84)(cid:80)(cid:73)(cid:87) (cid:22)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:4)(cid:52)(cid:86)(cid:73)(cid:17)(cid:50)(cid:73)(cid:88) (cid:38)(cid:77)(cid:72)(cid:77)(cid:86)(cid:73)(cid:71)(cid:88)(cid:77)(cid:83)(cid:82)(cid:69)(cid:80)(cid:4)(cid:48)(cid:55)(cid:56)(cid:49) (cid:23)(cid:4)(cid:39)(cid:83)(cid:82)(cid:90)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:87) (cid:55)(cid:88)(cid:83)(cid:84)(cid:4)(cid:56)(cid:83)(cid:79)(cid:73)(cid:82) (cid:48)(cid:77)(cid:82)(cid:73)(cid:69)(cid:86)(cid:4)(cid:52)(cid:86)(cid:83)(cid:78)(cid:73)(cid:71)(cid:88)(cid:77)(cid:83)(cid:82) (cid:39)(cid:76)(cid:69)(cid:86)(cid:69)(cid:71)(cid:88)(cid:73)(cid:86)(cid:4)(cid:41)(cid:81)(cid:70)(cid:73)(cid:72)(cid:72)(cid:77)(cid:82)(cid:75) Whilelinearspectrogramsdiscardphaseinformation(andarethereforelossy),algorithmssuchasGrifﬁn-Lim[14]arecapableofestimatingthisdiscardedinformation,whichenablestime-domainconversionviatheinverseshort-timeFouriertransform.Melspectro-gramsdiscardevenmoreinformation,presentingachallengingin-verseproblem.However,incomparisontothelinguisticandacousticfeaturesusedinWaveNet,themelspectrogramisasimpler,lower-levelacousticrepresentationofaudiosignals.ItshouldthereforebestraightforwardforasimilarWaveNetmodelconditionedonmelspectrogramstogenerateaudio,essentiallyasaneuralvocoder.In-deed,wewillshowthatitispossibletogeneratehighqualityaudiofrommelspectrogramsusingamodiﬁedWaveNetarchitecture.2.2.SpectrogramPredictionNetworkAsinTacotron,melspectrogramsarecomputedthroughashort-timeFouriertransform(STFT)usinga50msframesize,12.5msframehop,andaHannwindowfunction.Weexperimentedwitha5msframehoptomatchthefrequencyoftheconditioninginputsintheoriginalWaveNet,butthecorrespondingincreaseintemporalresolutionresultedinsigniﬁcantlymorepronunciationissues.WetransformtheSTFTmagnitudetothemelscaleusingan80channelmelﬁlterbankspanning125Hzto7.6kHz,followedbylogdynamicrangecompression.Priortologcompression,theﬁlterbankoutputmagnitudesareclippedtoaminimumvalueof0.01inordertolimitdynamicrangeinthelogarithmicdomain.Thenetworkiscomposedofanencoderandadecoderwithatten-tion.Theencoderconvertsacharactersequenceintoahiddenfeaturerepresentationwhichthedecoderconsumestopredictaspectrogram.Inputcharactersarerepresentedusingalearned512-dimensionalcharacterembedding,whicharepassedthroughastackof3convolu-tionallayerseachcontaining512ﬁlterswithshape5⇥1,i.e.,whereeachﬁlterspans5characters,followedbybatchnormalization[18]andReLUactivations.AsinTacotron,theseconvolutionallayersmodellonger-termcontext(e.g.,N-grams)intheinputcharactersequence.Theoutputoftheﬁnalconvolutionallayerispassedintoasinglebi-directional[19]LSTM[20]layercontaining512units(256ineachdirection)togeneratetheencodedfeatures.Theencoderoutputisconsumedbyanattentionnetworkwhichsummarizesthefullencodedsequenceasaﬁxed-lengthcontextvectorforeachdecoderoutputstep.Weusethelocation-sensitiveattentionfrom[21],whichextendstheadditiveattentionmechanism[22]tousecumulativeattentionweightsfrompreviousdecodertimestepsasanadditionalfeature.Thisencouragesthemodeltomoveforwardconsistentlythroughtheinput,mitigatingpotentialfailuremodeswheresomesubsequencesarerepeatedorignoredbythedecoder.Attentionprobabilitiesarecomputedafterprojectinginputsandlo-cationfeaturesto128-dimensionalhiddenrepresentations.Locationfeaturesarecomputedusing321-Dconvolutionﬁltersoflength31.Thedecoderisanautoregressiverecurrentneuralnetworkwhichpredictsamelspectrogramfromtheencodedinputsequenceoneframeatatime.Thepredictionfromtheprevioustimestepisﬁrstpassedthroughasmallpre-netcontaining2fullyconnectedlayersof256hiddenReLUunits.Wefoundthatthepre-netactingasaninformationbottleneckwasessentialforlearningattention.Thepre-netoutputandattentioncontextvectorareconcatenatedandpassedthroughastackof2uni-directionalLSTMlayerswith1024units.TheconcatenationoftheLSTMoutputandtheattentioncontextvectorisprojectedthroughalineartransformtopredictthetargetspectrogramframe.Finally,thepredictedmelspectrogramispassedthrougha5-layerconvolutionalpost-netwhichpredictsaresidualtoaddtothepredictiontoimprovetheoverallreconstruction.Each (cid:48)(cid:72)(cid:79)(cid:3)(cid:54)(cid:83)(cid:72)(cid:70)(cid:87)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80) (cid:45)(cid:82)(cid:84)(cid:89)(cid:88)(cid:4)(cid:56)(cid:73)(cid:92)(cid:88) (cid:48)(cid:83)(cid:71)(cid:69)(cid:88)(cid:77)(cid:83)(cid:82)(cid:4)(cid:55)(cid:73)(cid:82)(cid:87)(cid:77)(cid:88)(cid:77)(cid:90)(cid:73)(cid:4)(cid:37)(cid:88)(cid:88)(cid:73)(cid:82)(cid:88)(cid:77)(cid:83)(cid:82) (cid:25)(cid:4)(cid:39)(cid:83)(cid:82)(cid:90)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:4)(cid:52)(cid:83)(cid:87)(cid:88)(cid:17)(cid:50)(cid:73)(cid:88) Figure 16.14 The Tacotron2 architecture: An encoder-decoder maps from graphemes to mel spectrograms, followed by a vocoder that maps to waveﬁles. Figure modiﬁed from Shen et al. (2018). The system is trained on gold log-mel ﬁlterbank features, using teacher forcing, that is the decoder is fed the correct log-model spectral feature at each decoder step instead of the predicted decoder output from the prior step. 16.6.3 TTS: Vocoding WaveNet The vocoder for Tacotron 2 is an adaptation of the WaveNet vocoder (van den Oord et al., 2016). Here we’ll give a somewhat simpliﬁed description of vocoding using WaveNet. Recall that the goal of the vocoding process here will be to invert a log mel spec- trum representations back into a time-domain waveform representation. WaveNet is an autoregressive network, like the language models we introduced in Chapter 9. It Figure3:Visualizationofastackofdilatedcausalconvolutionallayers.Stackeddilatedconvolutionsenablenetworkstohaveverylargereceptiveﬁeldswithjustafewlay-ers,whilepreservingtheinputresolutionthroughoutthenetworkaswellascomputationalefﬁciency.Inthispaper,thedilationisdoubledforeverylayeruptoalimitandthenrepeated:e.g.1,2,4,...,512,1,2,4,...,512,1,2,4,...,512.Theintuitionbehindthisconﬁgurationistwo-fold.First,exponentiallyincreasingthedilationfactorresultsinexponentialreceptiveﬁeldgrowthwithdepth(Yu&Koltun,2016).Forexampleeach1,2,4,...,512blockhasreceptiveﬁeldofsize1024,andcanbeseenasamoreefﬁcientanddis-criminative(non-linear)counterpartofa1⇥1024convolution.Second,stackingtheseblocksfurtherincreasesthemodelcapacityandthereceptiveﬁeldsize.2.2SOFTMAXDISTRIBUTIONSOneapproachtomodelingtheconditionaldistributionsp(xt|x1,...,xt\u00001)overtheindividualaudiosampleswouldbetouseamixturemodelsuchasamixturedensitynetwork(Bishop,1994)ormixtureofconditionalGaussianscalemixtures(MCGSM)(Theis&Bethge,2015).However,vandenOordetal.(2016a)showedthatasoftmaxdistributiontendstoworkbetter,evenwhenthedataisimplicitlycontinuous(asisthecaseforimagepixelintensitiesoraudiosamplevalues).Oneofthereasonsisthatacategoricaldistributionismoreﬂexibleandcanmoreeasilymodelarbitrarydistributionsbecauseitmakesnoassumptionsabouttheirshape.Becauserawaudioistypicallystoredasasequenceof16-bitintegervalues(onepertimestep),asoftmaxlayerwouldneedtooutput65,536probabilitiespertimesteptomodelallpossiblevalues.Tomakethismoretractable,weﬁrstapplyaµ-lawcompandingtransformation(ITU-T,1988)tothedata,andthenquantizeitto256possiblevalues:f(xt)=sign(xt)ln(1+µ|xt|) ln(1+µ),3 Becausemodelswithcausalconvolutionsdonothaverecurrentconnections,theyaretypicallyfastertotrainthanRNNs,especiallywhenappliedtoverylongsequences.Oneoftheproblemsofcausalconvolutionsisthattheyrequiremanylayers,orlargeﬁlterstoincreasethereceptiveﬁeld.Forexample,inFig.2thereceptiveﬁeldisonly5(=#layers+ﬁlterlength-1).Inthispaperweusedilatedconvolutionstoincreasethereceptiveﬁeldbyordersofmagnitude,withoutgreatlyincreasingcomputationalcost.Adilatedconvolution(alsocalled`atrous,orconvolutionwithholes)isaconvolutionwheretheﬁlterisappliedoveranarealargerthanitslengthbyskippinginputvalueswithacertainstep.Itisequivalenttoaconvolutionwithalargerﬁlterderivedfromtheoriginalﬁlterbydilatingitwithzeros,butissigniﬁcantlymoreefﬁcient.Adilatedconvolutioneffectivelyallowsthenetworktooperateonacoarserscalethanwithanormalconvolution.Thisissimilartopoolingorstridedconvolutions,butheretheoutputhasthesamesizeastheinput.Asaspecialcase,dilatedconvolutionwithdilation1yieldsthestandardconvolution.Fig.3depictsdilatedcausalconvolutionsfordilations1,2,4,and8.Dilatedconvolutionshavepreviouslybeenusedinvariouscontexts,e.g.signalprocessing(Holschneideretal.,1989;Dutilleux,1989),andimagesegmentation(Chenetal.,2015;Yu&Koltun,2016). InputHidden LayerDilation = 1Hidden LayerDilation = 2Hidden LayerDilation = 4OutputDilation = 8 16.6 TTS 21 takes spectrograms as input and produces audio output represented as sequences of 8-bit mu-law (page ??). The probability of a waveform , a sequence of 8-bit mu-law values Y = y1, ..., yt , given an intermediate input mel spectrogram h is computed as: t (cid:89) p(Y ) = P(yt | y1, ..., yt 1, h1, ..., ht ) (16.23) − t=1 This probability distribution is modeled by a stack of special convolution layers, which include a speciﬁc convolutional structure called dilated convolutions, and a speciﬁc non-linearity function. A dilated convolution is a subtype of causal convolutional layer. Causal or masked convolutions look only at the past input, rather than the future; the pre- diction of yt+1 can only depend on y1, ..., yt , useful for autoregressive left-to-right processing. In dilated convolutions, at each successive layer we apply the convolu- tional ﬁlter over a span longer than its length by skipping input values. Thus at time t with a dilation value of 1, a convolutional ﬁlter of length 2 would see input values xt and xt 1. But a ﬁlter with a distillation value of 2 would skip an input, so would see input values xt and xt 1. Fig. 16.15 shows the computation of the output at time t with 4 dilated convolution layers with dilation values, 1, 2, 4, and 8. dilated convolutions − − Figure 16.15 Dilated convolutions, showing one dilation cycle size of 4, i.e., dilation values of 1, 2, 4, 8. Figure from van den Oord et al. (2016). The Tacotron 2 synthesizer uses 12 convolutional layers in two cycles with a dilation cycle size of 6, meaning that the ﬁrst 6 layers have dilations of 1, 2, 4, 8, 16, and 32. and the next 6 layers again have dilations of 1, 2, 4, 8, 16, and 32. Dilated convolutions allow the vocoder to grow the receptive ﬁeld exponentially with depth. WaveNet predicts mu-law audio samples. Recall from page ?? that this is a standard compression for audio in which the values at each sampling timestep are compressed into 8-bits. This means that we can predict the value of each sample with a simple 256-way categorical classiﬁer. The output of the dilated convolutions is thus passed through a softmax which makes this 256-way decision. The spectrogram prediction encoder-decoder and the WaveNet vocoder are trained separately. After the spectrogram predictor is trained, the spectrogram prediction network is run in teacher-forcing mode, with each predicted spectral frame condi- tioned on the encoded text input and the previous frame from the ground truth spec- trogram. This sequence of ground truth-aligned spectral features and gold audio output is then used to train the vocoder. This has been only a high-level sketch of the TTS process. There are numer- ous important details that the reader interested in going further with TTS may want 22 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH to look into. For example WaveNet uses a special kind of a gated activation func- In practice, tion as its non-linearity, and contains residual and skip connections. predicting 8-bit audio values doesn’t as work as well as 16-bit, for which a simple softmax is insufﬁcient, so decoders use fancier ways as the last step of predicting audio sample values, like mixtures of distributions. Finally, the WaveNet vocoder as we have described it would be so slow as to be useless; many different kinds of efﬁciency improvements are necessary in practice, for example by ﬁnding ways to do non-autoregressive generation, avoiding the latency of having to wait to generate each frame until the prior frame has been generated, and instead making predictions in parallel. We encourage the interested reader to consult the original papers and various version of the code. 16.6.4 TTS Evaluation Speech synthesis systems are evaluated by human listeners. (The development of a good automatic metric for synthesis evaluation, one that would eliminate the need for expensive and time-consuming human listening experiments, remains an open and exciting research topic.) MOS We evaluate the quality of synthesized utterances by playing a sentence to lis- teners and ask them to give a mean opinion score (MOS), a rating of how good the synthesized utterances are, usually on a scale from 1–5. We can then compare systems by comparing their MOS scores on the same sentences (using, e.g., paired t-tests to test for signiﬁcant differences). AB tests If we are comparing exactly two systems (perhaps to see if a particular change actually improved the system), we can use AB tests. In AB tests, we play the same sentence synthesized by two different systems (an A and a B system). The human listeners choose which of the two utterances they like better. We do this for say 50 sentences (presented in random order) and compare the number of sentences preferred for each system. 16.7 Other Speech Tasks While we have focused on speech recognition and TTS in this chapter, there are a wide variety of speech-related tasks. wake word The task of wake word detection is to detect a word or short phrase, usually in order to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant. The goal with wake words is build the detection into small devices at the computing edge, to maintain privacy by transmitting the least amount of user speech to a cloud- based server. Thus wake word detectors need to be fast, small footprint software that can ﬁt into embedded devices. Wake word detectors usually use the same frontend feature extraction we saw for ASR, often followed by a whole-word classiﬁer. speaker diarization Speaker diarization is the task of determining ‘who spoke when’ in a long multi-speaker audio recording, marking the start and end of each speaker’s turns in the interaction. This can be useful for transcribing meetings, classroom speech, or medical interactions. Often diarization systems use voice activity detection (VAD) to ﬁnd segments of continuous speech, extract speaker embedding vectors, and cluster the vectors to group together segments likely from the same speaker. More recent work is investigating end-to-end algorithms to map directly from input speech to a sequence of speaker labels for each frame. 16.8 SUMMARY speaker recognition language identiﬁcation Speaker recognition, is the task of identifying a speaker. We generally distin- guish the subtasks of speaker veriﬁcation, where we make a binary decision (is this speaker X or not?), such as for security when accessing personal information over the telephone, and speaker identiﬁcation, where we make a one of N decision trying to match a speaker’s voice against a database of many speakers . These tasks are related to language identiﬁcation, in which we are given a waveﬁle and must identify which language is being spoken; this is useful for example for automatically directing callers to human operators that speak appropriate languages. 16.8 Summary This chapter introduced the fundamental algorithms of automatic speech recognition (ASR) and text-to-speech (TTS). The task of speech recognition (or speech-to-text) is to map acoustic wave- forms to sequences of graphemes. The input to a speech recognizer is a series of acoustic waves. that are sam- pled, quantized, and converted to a spectral representation like the log mel spectrum. Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attention- based models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete. ASR is evaluated using the Word Error Rate; the edit distance between the hypothesis and the gold transcription. TTS systems are also based on the encoder-decoder architecture. The en- coder maps letters to an encoding, which is consumed by the decoder which generates mel spectrogram output. A neural vocoder then reads the spectro- gram and generates waveforms. TTS systems require a ﬁrst pass of text normalization to deal with numbers and abbreviations and other non-standard words. TTS is evaluated by playing a sentence to human listeners and having them give a mean opinion score (MOS) or by doing AB tests. Bibliographical and Historical Notes ASR A number of speech recognition systems were developed by the late 1940s and early 1950s. An early Bell Labs system could recognize any of the 10 digits from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent stored patterns, one for each digit, each of which roughly represented the ﬁrst two vowel formants in the digit. They achieved 97%–99% accuracy by choosing the pat- tern that had the highest relative correlation coefﬁcient with the input. Fry (1959) and Denes (1959) built a phoneme recognizer at University College, London, that recognized four vowels and nine consonants based on a similar pattern-recognition principle. Fry and Denes’s system was the ﬁrst to use phoneme transition probabili- ties to constrain the recognizer. 23 24 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH warping The late 1960s and early 1970s produced a number of important paradigm shifts. First were a number of feature-extraction algorithms, including the efﬁcient fast Fourier transform (FFT) (Cooley and Tukey, 1965), the application of cepstral pro- cessing to speech (Oppenheim et al., 1968), and the development of LPC for speech coding (Atal and Hanauer, 1971). Second were a number of ways of handling warp- ing; stretching or shrinking the input signal to handle differences in speaking rate and segment length when matching against stored patterns. The natural algorithm for solving this problem was dynamic programming, and, as we saw in Appendix A, the algorithm was reinvented multiple times to address this problem. The ﬁrst applica- tion to speech processing was by Vintsyuk (1968), although his result was not picked up by other researchers, and was reinvented by Velichko and Zagoruyko (1970) and Sakoe and Chiba (1971) (and 1984). Soon afterward, Itakura (1975) combined this dynamic programming idea with the LPC coefﬁcients that had previously been used only for speech coding. The resulting system extracted LPC features from incoming words and used dynamic programming to match them against stored LPC templates. The non-probabilistic use of dynamic programming to match a template against in- coming speech is called dynamic time warping. dynamic time warping The third innovation of this period was the rise of the HMM. Hidden Markov models seem to have been applied to speech independently at two laboratories around 1972. One application arose from the work of statisticians, in particular Baum and colleagues at the Institute for Defense Analyses in Princeton who applied HMMs to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967). James Baker learned of this work and applied the algorithm to speech processing (Baker, 1975) during his graduate work at CMU. Independently, Frederick Jelinek and collaborators (drawing from their research in information-theoretical models inﬂuenced by the work of Shannon (1948)) applied HMMs to speech at the IBM Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was the decoding algorithm; Baker’s DRAGON system used Viterbi (dynamic program- ming) decoding, while the IBM system applied Jelinek’s stack decoding algorithm (Jelinek, 1969). Baker then joined the IBM group for a brief time before founding the speech-recognition company Dragon Systems. bakeoff The use of the HMM, with Gaussian Mixture Models (GMMs) as the phonetic component, slowly spread through the speech community, becoming the dominant paradigm by the 1990s. One cause was encouragement by ARPA, the Advanced Research Projects Agency of the U.S. Department of Defense. ARPA started a ﬁve-year program in 1971 to build 1000-word, constrained grammar, few speaker speech understanding (Klatt, 1977), and funded four competing systems of which Carnegie-Mellon University’s Harpy system (Lowerre, 1968), which used a simpli- ﬁed version of Baker’s HMM-based DRAGON system was the best of the tested sys- tems. ARPA (and then DARPA) funded a number of new speech research programs, beginning with 1000-word speaker-independent read-speech tasks like “Resource Management” (Price et al., 1988), recognition of sentences read from the Wall Street Journal (WSJ), Broadcast News domain (LDC 1998, Graff 1997) (transcription of actual news broadcasts, including quite difﬁcult passages such as on-the-street inter- views) and the Switchboard, CallHome, CallFriend, and Fisher domains (Godfrey et al. 1992, Cieri et al. 2004) (natural telephone conversations between friends or strangers). Each of the ARPA tasks involved an approximately annual bakeoff at which systems were evaluated against each other. The ARPA competitions resulted in wide-scale borrowing of techniques among labs since it was easy to see which ideas reduced errors the previous year, and the competitions were probably an im- hybrid Kaldi ESPnet BIBLIOGRAPHICAL AND HISTORICAL NOTES portant factor in the eventual spread of the HMM paradigm. By around 1990 neural alternatives to the HMM/GMM architecture for ASR arose, based on a number of earlier experiments with neural networks for phoneme recognition and other speech tasks. Architectures included the time-delay neural network (TDNN)—the ﬁrst use of convolutional networks for speech— (Waibel et al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid HMM/MLP architecture in which a feedforward neural network is trained as a pho- netic classiﬁer whose outputs are used as probability estimates for an HMM-based architecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and Bourlard 1995). While the hybrid systems showed performance close to the standard HMM/GMM models, the problem was speed: large hybrid models were too slow to train on the CPUs of that era. For example, the largest hybrid system, a feedforward network, was limited to a hidden layer of 4000 units, producing probabilities over only a few dozen monophones. Yet training this model still required the research group to de- sign special hardware boards to do vector processing (Morgan and Bourlard, 1995). A later analytic study showed the performance of such simple feedforward MLPs for ASR increases sharply with more than 1 hidden layer, even controlling for the total number of parameters (Maas et al., 2017). But the computational resources of the time were insufﬁcient for more layers. Over the next two decades a combination of Moore’s law and the rise of GPUs allowed deep neural networks with many layers. Performance was getting close to traditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mo- hamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed traditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia). Originally it seemed that unsupervised pretraining of the networks using a tech- nique like deep belief networks was important, but by 2013, it was clear that for hybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data and enough layers, although a few other components did improve performance: us- ing log mel features instead of MFCCs, using dropout, and using rectiﬁed linear units (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013). Meanwhile early work had proposed the CTC loss function by 2006 (Graves et al., 2006), and by 2012 the RNN-Transducer was deﬁned and applied to phone recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recog- nition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015), (Our de- with advances such as specialized beam search (Hannun et al., 2014). scription of CTC in the chapter draws on Hannun (2017), which we encourage the interested reader to follow). The encoder-decoder architecture was applied to speech at about the same time by two different groups, in the Listen Attend and Spell system of Chan et al. (2016) and the attention-based encoder decoder architecture of Chorowski et al. (2014) and Bahdanau et al. (2016). By 2018 Transformers were included in this encoder- decoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Trans- formers in encoder-architectures for ASR, TTS, and speech-to-speech translation. Popular toolkits for speech processing include Kaldi (Povey et al., 2011) and ESPnet (Watanabe et al. 2018, Hayashi et al. 2020). TTS As we noted at the beginning of the chapter, speech synthesis is one of the earliest ﬁelds of speech and language processing. The 18th century saw a number of physical models of the articulation process, including the von Kempelen model mentioned above, as well as the 1773 vowel model of Kratzenstein in Copenhagen 25 26 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH using organ pipes. The early 1950s saw the development of three early paradigms of waveform synthesis: formant synthesis, articulatory synthesis, and concatenative synthesis. Modern encoder-decoder systems are distant descendants of formant synthesiz- ers. Formant synthesizers originally were inspired by attempts to mimic human speech by generating artiﬁcial spectrograms. The Haskins Laboratories Pattern Playback Machine generated a sound wave by painting spectrogram patterns on a moving transparent belt and using reﬂectance to ﬁlter the harmonics of a wave- form (Cooper et al., 1951); other very early formant synthesizers include those of Lawrence (1953) and Fant (1951). Perhaps the most well-known of the formant synthesizers were the Klatt formant synthesizer and its successor systems, includ- ing the MITalk system (Allen et al., 1987) and the Klattalk software used in Digital Equipment Corporation’s DECtalk (Klatt, 1982). See Klatt (1975) for details. A second early paradigm, concatenative synthesis, seems to have been ﬁrst pro- posed by Harris (1953) at Bell Laboratories; he literally spliced together pieces of magnetic tape corresponding to phones. Soon afterwards, Peterson et al. (1958) pro- posed a theoretical model based on diphones, including a database with multiple copies of each diphone with differing prosody, each labeled with prosodic features including F0, stress, and duration, and the use of join costs based on F0 and formant distance between neighboring units. But such diphone synthesis models were not actually implemented until decades later (Dixon and Maxey 1968, Olive 1977). The 1980s and 1990s saw the invention of unit selection synthesis, based on larger units of non-uniform length and the use of a target cost, (Sagisaka 1988, Sagisaka et al. 1992, Hunt and Black 1996, Black and Taylor 1994, Syrdal et al. 2000). A third paradigm, articulatory synthesizers attempt to synthesize speech by modeling the physics of the vocal tract as an open tube. Representative models include Stevens et al. (1953), Flanagan et al. (1975), and Fant (1986). See Klatt (1975) and Flanagan (1972) for more details. Most early TTS systems used phonemes as input; development of the text anal- ysis components of TTS came somewhat later, drawing on NLP. Indeed the ﬁrst true text-to-speech system seems to have been the system of Umeda and Teranishi (Umeda et al. 1968, Teranishi and Umeda 1968, Umeda 1976), which included a parser that assigned prosodic boundaries, as well as accent and stress. Exercises 16.1 Analyze each of the errors in the incorrectly recognized transcription of “um the phone is I left the. . . ” on page 15. For each one, give your best guess as to whether you think it is caused by a problem in signal processing, pronun- ciation modeling, lexicon size, language model, or pruning in the decoding search. Allen, J., M. S. Hunnicut, and D. H. Klatt. 1987. From Text to Speech: The MITalk system. Cambridge University Press. Atal, B. S. and S. Hanauer. 1971. Speech analysis and syn- thesis by prediction of the speech wave. JASA, 50:637– 655. Bahdanau, D., J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio. 2016. End-to-end attention-based large vo- cabulary speech recognition. ICASSP. Baker, J. K. 1975. The DRAGON system – An overview. IEEE Transactions on Acoustics, Speech, and Signal Pro- cessing, ASSP-23(1):24–29. Baum, L. E. and J. A. Eagon. 1967. An inequality with appli- cations to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. Bulletin of the American Mathematical Society, 73(3):360–363. Baum, L. E. and T. Petrie. 1966. Statistical inference for probabilistic functions of ﬁnite-state Markov chains. An- nals of Mathematical Statistics, 37(6):1554–1563. Black, A. W. and P. Taylor. 1994. CHATR: A generic speech synthesis system. COLING. Bourlard, H. and N. Morgan. 1994. Connectionist Speech Recognition: A Hybrid Approach. Kluwer. Bu, H., J. Du, X. Na, B. Wu, and H. Zheng. 2017. AISHELL- 1: An open-source Mandarin speech corpus and a speech recognition baseline. O-COCOSDA Proceedings. Canavan, A., D. Graff, and G. Zipperlen. 1997. CALL- HOME American English speech LDC97S42. Linguistic Data Consortium. Chan, W., N. Jaitly, Q. Le, and O. Vinyals. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. ICASSP. Chorowski, J., D. Bahdanau, K. Cho, and Y. Bengio. 2014. End-to-end continuous speech recognition us- ing attention-based recurrent NN: First results. NeurIPS Deep Learning and Representation Learning Workshop. Cieri, C., D. Miller, and K. Walker. 2004. The Fisher cor- pus: A resource for the next generations of speech-to-text. LREC. Cooley, J. W. and J. W. Tukey. 1965. An algorithm for the machine calculation of complex Fourier series. Mathe- matics of Computation, 19(90):297–301. Cooper, F. S., A. M. Liberman, and J. M. Borst. 1951. The interconversion of audible and visible patterns as a basis for research in the perception of speech. Proceedings of the National Academy of Sciences, 37(5):318–325. Dahl, G. E., T. N. Sainath, and G. E. Hinton. 2013. Im- proving deep neural networks for LVCSR using rectiﬁed linear units and dropout. ICASSP. Dahl, G. E., D. Yu, L. Deng, and A. Acero. 2012. Context- dependent pre-trained deep neural networks for large- vocabulary speech recognition. IEEE Transactions on au- dio, speech, and language processing, 20(1):30–42. David, Jr., E. E. and O. G. Selfridge. 1962. Eyes and ears for computers. Proceedings of the IRE (Institute of Radio Engineers), 50:1093–1101. Davis, K. H., R. Biddulph, and S. Balashek. 1952. Automatic recognition of spoken digits. JASA, 24(6):637–642. Exercises Davis, S. and P. Mermelstein. 1980. Comparison of para- metric representations for monosyllabic word recognition IEEE Transactions in continuously spoken sentences. on Acoustics, Speech, and Signal Processing, 28(4):357– 366. Demberg, V. 2006. Letter-to-phoneme conversion for a Ger- man text-to-speech system. Diplomarbeit Nr. 47, Univer- sit¨at Stuttgart. Denes, P. 1959. The design and operation of the mechanical speech recognizer at University College London. Journal of the British Institution of Radio Engineers, 19(4):219– 234. Appears together with companion paper (Fry 1959). Deng, L., G. Hinton, and B. Kingsbury. 2013. New types of deep neural network learning for speech recognition and related applications: An overview. ICASSP. Dixon, N. and H. Maxey. 1968. Terminal analog synthesis of continuous speech using the diphone method of segment assembly. IEEE Transactions on Audio and Electroacous- tics, 16(1):40–50. Du Bois, J. W., W. L. Chafe, C. Meyer, S. A. Thompson, R. Englebretson, and N. Martey. 2005. Santa Barbara cor- pus of spoken American English, Parts 1-4. Philadelphia: Linguistic Data Consortium. Ebden, P. and R. Sproat. 2015. The Kestrel TTS text normalization system. Natural Language Engineering, 21(3):333. van Esch, D. and R. Sproat. 2018. An expanded taxonomy of semiotic classes for text normalization. INTERSPEECH. Fant, G. M. 1951. Speech communication research. Vetenskaps Akad. Stockholm, Sweden, 24:331–337. Fant, G. M. 1986. Glottal ﬂow: Models and interaction. Journal of Phonetics, 14:393–399. Flanagan, J. L. 1972. Speech Analysis, Synthesis, and Per- ception. Springer. Flanagan, J. L., K. Ishizaka, and K. L. Shipley. 1975. Syn- thesis of speech from a dynamic model of the vocal cords and vocal tract. The Bell System Technical Jour- nal, 54(3):485–506. Fry, D. B. 1959. Theoretical aspects of mechanical speech recognition. Journal of the British Institution of Radio Engineers, 19(4):211–218. Appears together with com- panion paper (Denes 1959). Gillick, L. and S. J. Cox. 1989. Some statistical issues in the comparison of speech recognition algorithms. ICASSP. Godfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCH- BOARD: Telephone speech corpus for research and de- velopment. ICASSP. Graff, D. 1997. The 1996 Broadcast News speech and language-model corpus. Proceedings DARPA Speech Recognition Workshop. Graves, A. 2012. Sequence transduction with recurrent neu- ral networks. ICASSP. Graves, A., S. Fern´andez, F. Gomez, and J. Schmidhuber. 2006. Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural net- works. ICML. Graves, A. and N. Jaitly. 2014. Towards end-to-end speech recognition with recurrent neural networks. ICML. 27 Ing. 28 Chapter 16 Automatic Speech Recognition and Text-to-Speech Graves, A., A.-r. Mohamed, and G. Hinton. 2013. Speech recognition with deep recurrent neural networks. ICASSP. Hannun, A. 2017. Sequence modeling with CTC. Distill, Liu, Y., P. Fung, Y. Yang, C. Cieri, S. Huang, and D. Graff. 2006. HKUST/MTS: A very large scale Mandarin tele- phone speech corpus. International Conference on Chi- nese Spoken Language Processing. 2(11). Hannun, A. Y., A. L. Maas, D. Jurafsky, and A. Y. Ng. 2014. First-pass large vocabulary continuous speech recogni- tion using bi-directional recurrent DNNs. ArXiv preprint arXiv:1408.2873. Lowerre, B. T. 1968. The Harpy Speech Recognition System. Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA. Maas, A., Z. Xie, D. Jurafsky, and A. Y. Ng. 2015. Lexicon- free conversational speech recognition with neural net- works. NAACL HLT. Harris, C. M. 1953. A study of the building blocks in speech. JASA, 25(5):962–969. Hayashi, T., R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, Y. Zhang, and X. Tan. 2020. ESPnet-TTS: Uniﬁed, reproducible, and inte- gratable open source end-to-end text-to-speech toolkit. ICASSP. Maas, A. L., A. Y. Hannun, and A. Y. Ng. 2013. Rectiﬁer nonlinearities improve neural network acoustic models. ICML. Maas, A. L., P. Qi, Z. Xie, A. Y. Hannun, C. T. Lengerich, D. Jurafsky, and A. Y. Ng. 2017. Building dnn acoustic models for large vocabulary speech recognition. Com- puter Speech & Language, 41:195–213. Hunt, A. J. and A. W. Black. 1996. Unit selection in a con- catenative speech synthesis system using a large speech database. ICASSP. Mohamed, A., G. E. Dahl, and G. E. Hinton. 2009. Deep Belief Networks for phone recognition. NIPS Workshop on Deep Learning for Speech Recognition and Related Applications. Itakura, F. 1975. Minimum prediction residual principle ap- plied to speech recognition. IEEE Transactions on Acous- tics, Speech, and Signal Processing, ASSP-32:67–72. Ito, K. and L. Johnson. 2017. The LJ speech dataset. https: Morgan, N. and H. Bourlard. 1990. Continuous speech recognition using multilayer perceptrons with hidden markov models. ICASSP. //keithito.com/LJ-Speech-Dataset/. Jaitly, N., P. Nguyen, A. Senior, and V. Vanhoucke. 2012. Application of pretrained deep neural networks to large vocabulary speech recognition. INTERSPEECH. Jelinek, F. 1969. A fast sequential decoding algorithm us- ing a stack. IBM Journal of Research and Development, 13:675–685. Morgan, N. and H. A. Bourlard. 1995. Neural networks for statistical recognition of continuous speech. Proceedings of the IEEE, 83(5):742–772. NIST. 2005. Speech recognition scoring toolkit (sctk) ver- sion 2.1. http://www.nist.gov/speech/tools/. NIST. 2007. Matched Pairs Sentence-Segment Word Error (MAPSSWE) Test. Jelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a linguistic statistical decoder for the recognition of contin- uous speech. IEEE Transactions on Information Theory, IT-21(3):250–256. Karita, S., N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe, T. Yoshimura, and W. Zhang. 2019. A comparative study on transformer vs RNN in speech applications. IEEE ASRU-19. Olive, J. P. 1977. Rule synthesis of speech from dyadic units. ICASSP77. van den Oord, A., S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. 2016. WaveNet: A Generative Model ISCA Workshop on Speech Synthesis for Raw Audio. Workshop. Oppenheim, A. V., R. W. Schafer, and T. G. J. Stockham. 1968. Nonlinear ﬁltering of multiplied and convolved sig- nals. Proceedings of the IEEE, 56(8):1264–1291. Kendall, T. and C. Farrington. 2020. The Corpus of Regional African American Language. Version 2020.05. Eugene, OR: The Online Resources for African American Lan- guage Project. http://oraal.uoregon.edu/coraal. Klatt, D. H. 1975. Voice onset time, friction, and aspiration in word-initial consonant clusters. Journal of Speech and Hearing Research, 18:686–706. Panayotov, V., G. Chen, D. Povey, and S. Khudanpur. 2015. Librispeech: an ASR corpus based on public domain au- dio books. ICASSP. Peterson, G. E., W. S.-Y. Wang, and E. Sivertsen. 1958. JASA, Segmentation techniques in speech synthesis. 30(8):739–742. Klatt, D. H. 1977. Review of the ARPA speech understand- ing project. JASA, 62(6):1345–1366. Klatt, D. H. 1982. The Klattalk text-to-speech conversion Povey, D., A. Ghoshal, G. Boulianne, L. Burget, O. Glem- bek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsk´y, G. Stemmer, and K. Vesel´y. 2011. The Kaldi speech recognition toolkit. ASRU. system. ICASSP. Lang, K. J., A. H. Waibel, and G. E. Hinton. 1990. A time-delay neural network architecture for isolated word recognition. Neural networks, 3(1):23–43. Price, P. J., W. Fisher, J. Bernstein, and D. Pallet. 1988. The DARPA 1000-word resource management database for continuous speech recognition. ICASSP. Pundak, G. and T. N. Sainath. 2016. Lower frame rate neural Lawrence, W. 1953. The synthesis of speech from signals which have a low information rate. In W. Jackson, editor, Communication Theory, pages 460–469. Butterworth. LDC. 1998. LDC Catalog: Hub4 project. Univer- sity of Pennsylvania. www.ldc.upenn.edu/Catalog/ LDC98S71.html. network acoustic models. INTERSPEECH. Robinson, T. and F. Fallside. 1991. A recurrent error prop- agation network speech recognition system. Computer Speech & Language, 5(3):259–274. Sagisaka, Y. 1988. Speech synthesis by rule using an optimal selection of non-uniform synthesis units. ICASSP. Sagisaka, Y., N. Kaiki, N. Iwahashi, and K. Mimura. 1992. Atr – ν-talk speech synthesis system. ICSLP. Sakoe, H. and S. Chiba. 1971. A dynamic programming approach to continuous speech recognition. Proceedings of the Seventh International Congress on Acoustics, vol- ume 3. Akad´emiai Kiad´o. Sakoe, H. and S. Chiba. 1984. Dynamic programming al- gorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech, and Signal Process- ing, ASSP-26(1):43–49. Shannon, C. E. 1948. A mathematical theory of commu- nication. Bell System Technical Journal, 27(3):379–423. Continued in the following volume. Shen, J., R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu. 2018. Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. ICASSP. Sproat, R., A. W. Black, S. F. Chen, S. Kumar, M. Ostendorf, and C. Richards. 2001. Normalization of non-standard words. Computer Speech & Language, 15(3):287–333. Sproat, R. and K. Gorman. 2018. A brief summary of the Kaggle text normalization challenge. Stevens, K. N., S. Kasowski, and G. M. Fant. 1953. An elec- trical analog of the vocal tract. JASA, 25(4):734–742. Stevens, S. S. and J. Volkmann. 1940. The relation of pitch to frequency: A revised scale. The American Journal of Psychology, 53(3):329–353. Stevens, S. S., J. Volkmann, and E. B. Newman. 1937. A scale for the measurement of the psychological magni- tude pitch. JASA, 8:185–190. Syrdal, A. K., C. W. Wightman, A. Conkie, Y. Stylianou, M. Beutnagel, J. Schroeter, V. Strom, and K.-S. Lee. 2000. Corpus-based techniques in the AT&T NEXTGEN synthesis system. ICSLP. Taylor, P. 2009. Text-to-Speech Synthesis. Cambridge Uni- versity Press. Teranishi, R. and N. Umeda. 1968. Use of pronouncing dic- tionary in speech synthesis experiments. 6th International Congress on Acoustics. Umeda, N. 1976. Linguistic rules for text-to-speech synthe- sis. Proceedings of the IEEE, 64(4):443–451. Umeda, N., E. Matui, T. Suzuki, and H. Omura. 1968. Syn- thesis of fairy tale using an analog vocal tract. 6th Inter- national Congress on Acoustics. Velichko, V. M. and N. G. Zagoruyko. 1970. Automatic recognition of 200 words. International Journal of Man- Machine Studies, 2:223–234. Vintsyuk, T. K. 1968. Speech discrimination by dynamic programming. Cybernetics, 4(1):52–57. Russian Kiber- netika 4(1):81-88. 1968. Waibel, A., T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang. 1989. Phoneme recognition using time-delay neu- IEEE transactions on Acoustics, Speech, ral networks. and Signal Processing, 37(3):328–339. Wang, Y., R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous. 2017. Tacotron: Towards end-to-end speech synthesis. INTER- SPEECH. Exercises Watanabe, S., T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai. 2018. ESPnet: End-to-end speech processing toolkit. INTERSPEECH. Zhang, H., R. Sproat, A. H. Ng, F. Stahlberg, X. Peng, K. Gorman, and B. Roark. 2019. Neural models of text normalization for speech applications. Computational Linguistics, 45(2):293–337. 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_api_key = \"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\"\n",
        "hf_api_key_2 = \"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\""
      ],
      "metadata": {
        "id": "upC7FuHGA5SK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/distilbert/distilbert-base-cased-distilled-squad\"\n",
        "headers = {\"Authorization\": f\"Bearer {hf_api_key}\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": {\n",
        "\t\t\"question\": \"What is the motivation?\",\n",
        "\t\t\"context\": text\n",
        "\t},\n",
        "})"
      ],
      "metadata": {
        "id": "Igl9xhyCPjtS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b_RXWtXP85a",
        "outputId": "a1472445-6f5d-4bb6-c9f7-af35697c4d25"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.4503627121448517, 'start': 15737, 'end': 15762, 'answer': 'acoustic feature creation'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "# from unstructured import text_to_stream, StreamChunk\n",
        "\n",
        "# Define Hugging Face API details (replace with yours)\n",
        "API_URL = \"https://api-inference.huggingface.co/models/distilbert/distilbert-base-cased-distilled-squad\"\n",
        "headers = {\"Authorization\": f\"Bearer {hf_api_key}\"}\n",
        "\n",
        "def chunk_text(text, chunk_size):\n",
        "  \"\"\"Chunks a string text into a list of substrings with a specified size\"\"\"\n",
        "  chunks = []\n",
        "  start_index = 0\n",
        "  while start_index < len(text):\n",
        "    end_index = min(start_index + chunk_size, len(text))\n",
        "    chunk = text[start_index:end_index]\n",
        "    chunks.append(chunk)\n",
        "    start_index = end_index\n",
        "  return chunks\n",
        "\n",
        "def query(payload):\n",
        "  \"\"\"Sends a POST request with the text chunks and question to Hugging Face\"\"\"\n",
        "\n",
        "  response = requests.post(API_URL, headers=headers, json=payload)\n",
        "  return response.json()\n",
        "\n",
        "# text = \"This is a long text that talks about different chapters. The first chapter covers introduction,  the second chapter dives into Machine Learning.\"\n",
        "text = text\n",
        "question = \"What is the motivation?\"\n",
        "chunk_size = 100  # Adjust chunk size as needed\n",
        "\n",
        "def chunk_and_query(text, question):\n",
        "  \"\"\"Chunks the text, queries Hugging Face for each chunk, and identifies the relevant one\"\"\"\n",
        "  chunks = chunk_text(text, chunk_size)\n",
        "  inference_results = []\n",
        "  for chunk in chunks:\n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"question\": question,\n",
        "            \"context\": chunk\n",
        "        }\n",
        "    }\n",
        "    inference_result = query(payload)\n",
        "    inference_results.append(inference_result)\n",
        "\n",
        "  # Analyze inference results to identify the relevant chunk\n",
        "  for result in inference_results:\n",
        "    # Analyze the answer based on your model's output (modify as needed)\n",
        "    # if question in result[\"answer\"] or any(answer in question for answer in result[\"answer\"]):\n",
        "    #   return chunk, result\n",
        "    if \"answer\" in result and (question in result[\"answer\"] or any(answer in question for answer in result[\"answer\"])):\n",
        "      return chunk, result\n",
        "\n",
        "  return None, None  # Question not answered in any chunk\n",
        "\n",
        "answering_chunk, inference_result = chunk_and_query(text, question)\n",
        "\n",
        "if answering_chunk:\n",
        "  print(f\"Answering Chunk:\\n{answering_chunk}\")\n",
        "  print(f\"Inference Result:\\n{inference_result}\")  # Access the Hugging Face response for the chunk\n",
        "else:\n",
        "  print(\"The answer to the question could not be found in any chunk.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTlG9TTnT20f",
        "outputId": "f868ad0d-a8ea-488b-d74a-ea31f2745fab"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answering Chunk:\n",
            "tational Linguistics, 45(2):293–337. 29\n",
            "Inference Result:\n",
            "{'score': 0.5213509202003479, 'start': 39, 'end': 67, 'answer': 'Automatic Speech Recognition'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/distilbert/distilbert-base-uncased-distilled-squad\"\n",
        "headers = {\"Authorization\": f\"Bearer {hf_api_key_2}\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": {\n",
        "\t\t\"question\": \"What is the motivation in the paper?\",\n",
        "\t\t\"context\": text\n",
        "\t},\n",
        "})"
      ],
      "metadata": {
        "id": "GzUVbPbPGXE1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg--5wLdGc3_",
        "outputId": "16fcc59e-3252-486a-f51f-bab6d348813a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.6138640642166138, 'start': 43274, 'end': 43319, 'answer': 'to map from strings of letters to wave- forms'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "# from unstructured import text_to_stream, StreamChunk\n",
        "\n",
        "# Define Hugging Face API details (replace with yours)\n",
        "API_URL = \"https://api-inference.huggingface.co/models/distilbert/distilbert-base-uncased-distilled-squad\"\n",
        "headers = {\"Authorization\": f\"Bearer {hf_api_key}\"}\n",
        "\n",
        "def chunk_text(text, chunk_size):\n",
        "  \"\"\"Chunks a string text into a list of substrings with a specified size\"\"\"\n",
        "  chunks = []\n",
        "  start_index = 0\n",
        "  while start_index < len(text):\n",
        "    end_index = min(start_index + chunk_size, len(text))\n",
        "    chunk = text[start_index:end_index]\n",
        "    chunks.append(chunk)\n",
        "    start_index = end_index\n",
        "  return chunks\n",
        "\n",
        "def query(payload):\n",
        "  \"\"\"Sends a POST request with the text chunks and question to Hugging Face\"\"\"\n",
        "\n",
        "  response = requests.post(API_URL, headers=headers, json=payload)\n",
        "  return response.json()\n",
        "\n",
        "# text = \"This is a long text that talks about different chapters. The first chapter covers introduction,  the second chapter dives into Machine Learning.\"\n",
        "text = text\n",
        "question = \"What is the motivation?\"\n",
        "chunk_size = 100  # Adjust chunk size as needed\n",
        "\n",
        "def chunk_and_query(text, question):\n",
        "  \"\"\"Chunks the text, queries Hugging Face for each chunk, and identifies the relevant one\"\"\"\n",
        "  chunks = chunk_text(text, chunk_size)\n",
        "  inference_results = []\n",
        "  for chunk in chunks:\n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"question\": question,\n",
        "            \"context\": chunk\n",
        "        }\n",
        "    }\n",
        "    inference_result = query(payload)\n",
        "    inference_results.append(inference_result)\n",
        "\n",
        "  # Analyze inference results to identify the relevant chunk\n",
        "  for result in inference_results:\n",
        "    # Analyze the answer based on your model's output (modify as needed)\n",
        "    # if question in result[\"answer\"] or any(answer in question for answer in result[\"answer\"]):\n",
        "    #   return chunk, result\n",
        "    if \"answer\" in result and (question in result[\"answer\"] or any(answer in question for answer in result[\"answer\"])):\n",
        "      return chunk, result\n",
        "\n",
        "  return None, None  # Question not answered in any chunk\n",
        "\n",
        "answering_chunk, inference_result = chunk_and_query(text, question)\n",
        "\n",
        "if answering_chunk:\n",
        "  print(f\"Answering Chunk:\\n{answering_chunk}\")\n",
        "  print(f\"Inference Result:\\n{inference_result}\")  # Access the Hugging Face response for the chunk\n",
        "else:\n",
        "  print(\"The answer to the question could not be found in any chunk.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzajnnpeGuIN",
        "outputId": "29d3c025-c60d-47a2-b7d7-8fb528aeb166"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answering Chunk:\n",
            "tational Linguistics, 45(2):293–337. 29\n",
            "Inference Result:\n",
            "{'score': 0.007238917984068394, 'start': 15, 'end': 99, 'answer': 'WetransformtheSTFTmagnitudetothemelscaleusingan80channelmelﬁlterbankspanning125Hzto7'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/Intel/dynamic_tinybert\"\n",
        "headers = {\"Authorization\": f\"Bearer {hf_api_key_2}\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": {\n",
        "\t\t\"question\": \"What is the motivation?\",\n",
        "\t\t\"context\": text\n",
        "\t},\n",
        "})"
      ],
      "metadata": {
        "id": "xbl38eFJdcLw"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUQvHLrWdfx-",
        "outputId": "e9087194-e732-4486-cbb1-9fe8c7087e1b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.17011895775794983, 'start': 43274, 'end': 43319, 'answer': 'to map from strings of letters to wave- forms'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "# from unstructured import text_to_stream, StreamChunk\n",
        "\n",
        "# Define Hugging Face API details (replace with yours)\n",
        "API_URL = \"https://api-inference.huggingface.co/models/Intel/dynamic_tinybert\"\n",
        "headers = {\"Authorization\": f\"Bearer {hf_api_key_2}\"}\n",
        "\n",
        "def chunk_text(text, chunk_size):\n",
        "  \"\"\"Chunks a string text into a list of substrings with a specified size\"\"\"\n",
        "  chunks = []\n",
        "  start_index = 0\n",
        "  while start_index < len(text):\n",
        "    end_index = min(start_index + chunk_size, len(text))\n",
        "    chunk = text[start_index:end_index]\n",
        "    chunks.append(chunk)\n",
        "    start_index = end_index\n",
        "  return chunks\n",
        "\n",
        "def query(payload):\n",
        "  \"\"\"Sends a POST request with the text chunks and question to Hugging Face\"\"\"\n",
        "\n",
        "  response = requests.post(API_URL, headers=headers, json=payload)\n",
        "  return response.json()\n",
        "\n",
        "# text = \"This is a long text that talks about different chapters. The first chapter covers introduction,  the second chapter dives into Machine Learning.\"\n",
        "text = text\n",
        "question = \"What is the LibriSpeech?\"\n",
        "chunk_size = 100  # Adjust chunk size as needed\n",
        "\n",
        "def chunk_and_query(text, question):\n",
        "  \"\"\"Chunks the text, queries Hugging Face for each chunk, and identifies the relevant one\"\"\"\n",
        "  chunks = chunk_text(text, chunk_size)\n",
        "  inference_results = []\n",
        "  for chunk in chunks:\n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"question\": question,\n",
        "            \"context\": chunk\n",
        "        }\n",
        "    }\n",
        "    inference_result = query(payload)\n",
        "    inference_results.append(inference_result)\n",
        "\n",
        "  # Analyze inference results to identify the relevant chunk\n",
        "  for result in inference_results:\n",
        "    # Analyze the answer based on your model's output (modify as needed)\n",
        "    # if question in result[\"answer\"] or any(answer in question for answer in result[\"answer\"]):\n",
        "    #   return chunk, result\n",
        "    if \"answer\" in result and (question in result[\"answer\"] or any(answer in question for answer in result[\"answer\"])):\n",
        "      return chunk, result\n",
        "\n",
        "  return None, None  # Question not answered in any chunk\n",
        "\n",
        "answering_chunk, inference_result = chunk_and_query(text, question)\n",
        "\n",
        "if answering_chunk:\n",
        "  print(f\"Answering Chunk:\\n{answering_chunk}\")\n",
        "  print(f\"Inference Result:\\n{inference_result}\")  # Access the Hugging Face response for the chunk\n",
        "else:\n",
        "  print(\"The answer to the question could not be found in any chunk.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cl9k7-FeAou",
        "outputId": "fb110b66-413e-4305-d0b6-b8f6d2fdd83e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answering Chunk:\n",
            "tational Linguistics, 45(2):293–337. 29\n",
            "Inference Result:\n",
            "{'score': 0.23597754538059235, 'start': 67, 'end': 82, 'answer': 'rights reserved'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/dslim/bert-base-NER\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": text\n",
        "})"
      ],
      "metadata": {
        "id": "Qc5MTLJ4vJdt"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoEO5TqFvOx-",
        "outputId": "653ebdf3-eabf-43e8-fe34-4a489168aaad"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity_group': 'PER', 'score': 0.8530672788619995, 'word': 'Daniel Jurafs', 'start': 32, 'end': 45}, {'entity_group': 'PER', 'score': 0.9079881906509399, 'word': 'James', 'start': 50, 'end': 55}, {'entity_group': 'ORG', 'score': 0.8937530517578125, 'word': 'H.', 'start': 56, 'end': 58}, {'entity_group': 'PER', 'score': 0.5877600908279419, 'word': 'Martin', 'start': 59, 'end': 65}, {'entity_group': 'MISC', 'score': 0.622501790523529, 'word': 'Speech', 'start': 149, 'end': 155}, {'entity_group': 'MISC', 'score': 0.4945046007633209, 'word': '##cognition', 'start': 158, 'end': 167}, {'entity_group': 'MISC', 'score': 0.9612181186676025, 'word': 'to', 'start': 177, 'end': 179}, {'entity_group': 'PER', 'score': 0.9885424971580505, 'word': 'Thomas Love', 'start': 319, 'end': 330}, {'entity_group': 'ORG', 'score': 0.3969520032405853, 'word': '##ll', 'start': 330, 'end': 332}, {'entity_group': 'PER', 'score': 0.9596381783485413, 'word': 'Bed', 'start': 333, 'end': 336}, {'entity_group': 'MISC', 'score': 0.4773089289665222, 'word': 'Radio', 'start': 622, 'end': 627}, {'entity_group': 'ORG', 'score': 0.5192830562591553, 'word': 'Rex', 'start': 628, 'end': 631}, {'entity_group': 'MISC', 'score': 0.5079433917999268, 'word': 'Rex', 'start': 829, 'end': 832}, {'entity_group': 'PER', 'score': 0.8908087611198425, 'word': 'Rex', 'start': 835, 'end': 838}, {'entity_group': 'PER', 'score': 0.7735739946365356, 'word': 'David', 'start': 874, 'end': 879}, {'entity_group': 'ORG', 'score': 0.39420223236083984, 'word': 'Jr', 'start': 881, 'end': 883}, {'entity_group': 'ORG', 'score': 0.6248342394828796, 'word': 'Selfridge', 'start': 889, 'end': 898}, {'entity_group': 'MISC', 'score': 0.8563793897628784, 'word': 'auto', 'start': 977, 'end': 981}, {'entity_group': 'MISC', 'score': 0.6106706261634827, 'word': 'AS', 'start': 983, 'end': 985}, {'entity_group': 'ORG', 'score': 0.5182526111602783, 'word': 'ASR', 'start': 1207, 'end': 1210}, {'entity_group': 'ORG', 'score': 0.5131110548973083, 'word': 'AS', 'start': 1608, 'end': 1610}, {'entity_group': 'PER', 'score': 0.7820316553115845, 'word': 'Henry', 'start': 2135, 'end': 2140}, {'entity_group': 'PER', 'score': 0.768654465675354, 'word': 'James', 'start': 2141, 'end': 2146}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/dslim/bert-large-NER\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": text\n",
        "})"
      ],
      "metadata": {
        "id": "fhzJoiiJvhxj"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tQ86upCp-Pn",
        "outputId": "eba9e4f4-c4d9-474c-a6c5-615810f9d929"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity_group': 'MISC', 'score': 0.6973965167999268, 'word': 'Language', 'start': 11, 'end': 19}, {'entity_group': 'PER', 'score': 0.9812703132629395, 'word': 'Daniel Jurafsky', 'start': 32, 'end': 47}, {'entity_group': 'PER', 'score': 0.9967543482780457, 'word': 'James H', 'start': 50, 'end': 57}, {'entity_group': 'PER', 'score': 0.9990001320838928, 'word': 'Martin', 'start': 59, 'end': 65}, {'entity_group': 'MISC', 'score': 0.7057746648788452, 'word': '##cognition', 'start': 158, 'end': 167}, {'entity_group': 'MISC', 'score': 0.7720351219177246, 'word': 'Text', 'start': 172, 'end': 176}, {'entity_group': 'PER', 'score': 0.9866201281547546, 'word': 'Thomas Lovell Beddoes', 'start': 319, 'end': 340}, {'entity_group': 'MISC', 'score': 0.5177059769630432, 'word': 'Radio', 'start': 622, 'end': 627}, {'entity_group': 'PER', 'score': 0.417510062456131, 'word': 'Rex ”', 'start': 628, 'end': 632}, {'entity_group': 'PER', 'score': 0.9609756469726562, 'word': 'Rex', 'start': 829, 'end': 832}, {'entity_group': 'PER', 'score': 0.9806894659996033, 'word': 'Rex', 'start': 835, 'end': 838}, {'entity_group': 'PER', 'score': 0.9992119073867798, 'word': 'David', 'start': 874, 'end': 879}, {'entity_group': 'PER', 'score': 0.9951419830322266, 'word': 'Jr', 'start': 881, 'end': 883}, {'entity_group': 'PER', 'score': 0.9877334833145142, 'word': 'Selfridge', 'start': 889, 'end': 898}, {'entity_group': 'MISC', 'score': 0.9678267240524292, 'word': 'AS', 'start': 983, 'end': 985}, {'entity_group': 'MISC', 'score': 0.874109148979187, 'word': 'AS', 'start': 1013, 'end': 1015}, {'entity_group': 'MISC', 'score': 0.9567983746528625, 'word': 'AS', 'start': 1207, 'end': 1209}, {'entity_group': 'MISC', 'score': 0.9043359756469727, 'word': 'AS', 'start': 1608, 'end': 1610}, {'entity_group': 'MISC', 'score': 0.8771973848342896, 'word': 'AS', 'start': 1878, 'end': 1880}, {'entity_group': 'PER', 'score': 0.9993470311164856, 'word': 'Milton', 'start': 2074, 'end': 2080}, {'entity_group': 'PER', 'score': 0.9954740405082703, 'word': 'Henry', 'start': 2135, 'end': 2140}, {'entity_group': 'PER', 'score': 0.9983318448066711, 'word': 'James', 'start': 2141, 'end': 2146}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/dslim/distilbert-NER\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": text\n",
        "})"
      ],
      "metadata": {
        "id": "e4Dp0e2tyJ7S"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoWiV7Urx7hY",
        "outputId": "4b53de74-25ed-4885-8018-2a84185188e5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity_group': 'LABEL_0', 'score': 0.9723325371742249, 'word': 'Speech and Language Processing.', 'start': 0, 'end': 31}, {'entity_group': 'LABEL_1', 'score': 0.9539551138877869, 'word': 'Daniel', 'start': 32, 'end': 38}, {'entity_group': 'LABEL_2', 'score': 0.9957494139671326, 'word': 'Jurafsky', 'start': 39, 'end': 47}, {'entity_group': 'LABEL_0', 'score': 0.9524639248847961, 'word': '&', 'start': 48, 'end': 49}, {'entity_group': 'LABEL_3', 'score': 0.38568228483200073, 'word': 'James', 'start': 50, 'end': 55}, {'entity_group': 'LABEL_2', 'score': 0.7333908081054688, 'word': 'H. Martin', 'start': 56, 'end': 65}, {'entity_group': 'LABEL_4', 'score': 0.47123289108276367, 'word': '.', 'start': 65, 'end': 66}, {'entity_group': 'LABEL_0', 'score': 0.9964744448661804, 'word': 'rights reserved. Draft of January 7, 2023. Copyright © 2023. CHAPTER 16 Automatic', 'start': 67, 'end': 148}, {'entity_group': 'LABEL_8', 'score': 0.8427508473396301, 'word': 'Speech Recognition', 'start': 149, 'end': 167}, {'entity_group': 'LABEL_0', 'score': 0.7909656763076782, 'word': 'and Text - to -', 'start': 168, 'end': 180}, {'entity_group': 'LABEL_8', 'score': 0.5926604866981506, 'word': 'Speech', 'start': 180, 'end': 186}, {'entity_group': 'LABEL_0', 'score': 0.9877727031707764, 'word': 'I KNOW not whether I see your meaning : if I do, it lies Upon the wordy wavelets of your voice, Dim as an evening shadow in a brook,', 'start': 187, 'end': 318}, {'entity_group': 'LABEL_1', 'score': 0.9972919821739197, 'word': 'Thomas', 'start': 319, 'end': 325}, {'entity_group': 'LABEL_2', 'score': 0.9959989786148071, 'word': 'Lovell Beddoes', 'start': 326, 'end': 340}, {'entity_group': 'LABEL_0', 'score': 0.9969606995582581, 'word': ', 1851 Understanding spoken language, or at least transcribing the words into writing, is one of the earliest goals of computer language processing. In fact, speech processing predates the computer by many decades! The ﬁrst machine that recognized speech was a toy from the 1920s. “', 'start': 340, 'end': 622}, {'entity_group': 'LABEL_7', 'score': 0.5371034741401672, 'word': 'Radio', 'start': 622, 'end': 627}, {'entity_group': 'LABEL_8', 'score': 0.5662664771080017, 'word': 'Rex ”', 'start': 628, 'end': 632}, {'entity_group': 'LABEL_0', 'score': 0.9915516376495361, 'word': ', shown to the right, was a celluloid dog that moved ( by means of a spring ) when the spring was released by 500 Hz acous - tic energy. Since 500 Hz is roughly the ﬁrst formant of the vowel [ eh ] in “', 'start': 632, 'end': 829}, {'entity_group': 'LABEL_1', 'score': 0.9096064567565918, 'word': 'Rex', 'start': 829, 'end': 832}, {'entity_group': 'LABEL_0', 'score': 0.9090474843978882, 'word': '”,', 'start': 832, 'end': 834}, {'entity_group': 'LABEL_1', 'score': 0.9829750657081604, 'word': 'Rex', 'start': 835, 'end': 838}, {'entity_group': 'LABEL_0', 'score': 0.9997958540916443, 'word': 'seemed to come when he was called (', 'start': 839, 'end': 874}, {'entity_group': 'LABEL_1', 'score': 0.9952337145805359, 'word': 'David', 'start': 874, 'end': 879}, {'entity_group': 'LABEL_0', 'score': 0.940733790397644, 'word': ',', 'start': 879, 'end': 880}, {'entity_group': 'LABEL_2', 'score': 0.7581267356872559, 'word': 'Jr.', 'start': 881, 'end': 884}, {'entity_group': 'LABEL_0', 'score': 0.969874918460846, 'word': 'and', 'start': 885, 'end': 888}, {'entity_group': 'LABEL_2', 'score': 0.8160101175308228, 'word': 'Selfridge', 'start': 889, 'end': 898}, {'entity_group': 'LABEL_0', 'score': 0.9979161620140076, 'word': ', 1962 ). In modern times, we expect more of our automatic systems. The task of auto -', 'start': 898, 'end': 982}, {'entity_group': 'LABEL_7', 'score': 0.5538347959518433, 'word': 'ASR', 'start': 983, 'end': 986}, {'entity_group': 'LABEL_0', 'score': 0.9428529739379883, 'word': 'matic speech recognition ( AS', 'start': 987, 'end': 1015}, {'entity_group': 'LABEL_7', 'score': 0.6401770114898682, 'word': '##R', 'start': 1015, 'end': 1016}, {'entity_group': 'LABEL_0', 'score': 0.9997284412384033, 'word': ') is to map any waveform like this : to the appropriate string of words : It ’ s time for lunch! Automatic transcription of speech by any speaker in any environment is still far from solved, but', 'start': 1016, 'end': 1206}, {'entity_group': 'LABEL_7', 'score': 0.5889174938201904, 'word': 'ASR', 'start': 1207, 'end': 1210}, {'entity_group': 'LABEL_0', 'score': 0.9887455105781555, 'word': 'technology has matured to the point where it is now viable for many practical tasks. Speech is a natural interface for communicating with smart home ap - pliances, personal assistants, or cellphones, where keyboards are less convenient, in telephony applications like call - routing ( “ Accounting, please ” ) or in sophisticated dialogue applications ( “ I ’ d like to change the return date of my ﬂight ” ). AS', 'start': 1211, 'end': 1610}, {'entity_group': 'LABEL_7', 'score': 0.36110246181488037, 'word': '##R', 'start': 1610, 'end': 1611}, {'entity_group': 'LABEL_0', 'score': 0.9864385724067688, 'word': 'is also useful for general transcription, for example for automatically generating captions for audio or video text ( transcribing movies or videos or live discussions ). Transcrip - tion is important in ﬁelds like law where dictation plays an important role. Finally, ASR is important as part of augmentative communication ( interaction between com - puters and humans with some disability resulting in difﬁculties or inabilities in typ - ing or audition ). The blind Milton famously dictated Paradise Lost to his daughters, and', 'start': 1612, 'end': 2134}, {'entity_group': 'LABEL_1', 'score': 0.37987664341926575, 'word': 'Henry', 'start': 2135, 'end': 2140}, {'entity_group': 'LABEL_0', 'score': 0.9676520228385925, 'word': 'James dictated his later novels after a repetitive stress injury. What about the opposite', 'start': 2141, 'end': 2230}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "    \"inputs\": text,\n",
        "    \"parameters\": {\"candidate_labels\": [\"Chapter name\"]}\n",
        "\n",
        "})\n",
        "\n",
        "# Understanding spoken language"
      ],
      "metadata": {
        "id": "pBPyRchXwaAf"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFx_NGdN7ZE1",
        "outputId": "1dfbba23-5a81-46db-ba3b-b63e479e67d9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sequence': 'Speech and Language Processing. Daniel Jurafsky & James H. Martin. rights reserved. Draft of January 7, 2023. Copyright © 2023. CHAPTER 16 Automatic Speech Recognition and Text-to-Speech I KNOW not whether I see your meaning: if I do, it lies Upon the wordy wavelets of your voice, Dim as an evening shadow in a brook, Thomas Lovell Beddoes, 1851 Understanding spoken language, or at least transcribing the words into writing, is one of the earliest goals of computer language processing. In fact, speech processing predates the computer by many decades! The ﬁrst machine that recognized speech was a toy from the 1920s. “Radio Rex”, shown to the right, was a celluloid dog that moved (by means of a spring) when the spring was released by 500 Hz acous- tic energy. Since 500 Hz is roughly the ﬁrst formant of the vowel [eh] in “Rex”, Rex seemed to come when he was called (David, Jr. and Selfridge, 1962). In modern times, we expect more of our automatic systems. The task of auto- ASR matic speech recognition (ASR) is to map any waveform like this: to the appropriate string of words: It’s time for lunch! Automatic transcription of speech by any speaker in any environment is still far from solved, but ASR technology has matured to the point where it is now viable for many practical tasks. Speech is a natural interface for communicating with smart home ap- pliances, personal assistants, or cellphones, where keyboards are less convenient, in telephony applications like call-routing (“Accounting, please”) or in sophisticated dialogue applications (“I’d like to change the return date of my ﬂight”). ASR is also useful for general transcription, for example for automatically generating captions for audio or video text (transcribing movies or videos or live discussions). Transcrip- tion is important in ﬁelds like law where dictation plays an important role. Finally, ASR is important as part of augmentative communication (interaction between com- puters and humans with some disability resulting in difﬁculties or inabilities in typ- ing or audition). The blind Milton famously dictated Paradise Lost to his daughters, and Henry James dictated his later novels after a repetitive stress injury. What about the opposite problem, going from text to speech? This is a problem with an even longer history. In Vienna in 1769, Wolfgang von Kempelen built for All 2 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH the Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton consisting of a wooden box ﬁlled with gears, behind which sat a robot mannequin who played chess by moving pieces with his mechanical arm. The Turk toured Eu- rope and the Americas for decades, defeating Napoleon Bonaparte and even playing Charles Babbage. The Mechanical Turk might have been one of the early successes of artiﬁcial intelligence were it not for the fact that it was, alas, a hoax, powered by a human chess player hidden inside the box. less well known is proliﬁc inventor, also built between 1769 and 1790 what was deﬁnitely the ﬁrst full-sentence not a hoax: speech synthesizer, shown partially to the right. His device consisted of a bellows to simulate the lungs, a rub- ber mouthpiece and a nose aperture, a reed to simulate the vocal folds, var- ious whistles for the fricatives, and a small auxiliary bellows to provide the puff of air for plosives. By moving levers with both hands to open and close apertures, and adjusting the ﬂexible leather “vo- cal tract”, an operator could produce different consonants and vowels. What is that von Kempelen, an extraordinarily speech synthesis text-to-speech More than two centuries later, we no longer build our synthesizers out of wood and leather, nor do we need human operators. The modern task of speech synthesis, also called text-to-speech or TTS, is exactly the reverse of ASR; to map text: TTS It’s time for lunch! to an acoustic waveform: Modern speech synthesis has a wide variety of applications. TTS is used in conversational agents that conduct dialogues with people, plays a role in devices that read out loud for the blind or in games, and can be used to speak for sufferers of neurological disorders, such as the late astrophysicist Steven Hawking who, after he lost the use of his voice because of ALS, spoke by manipulating a TTS system. In the next sections we’ll show how to do ASR with encoder-decoders, intro- duce the CTC loss functions, the standard word error rate evaluation metric, and describe how acoustic features are extracted. We’ll then see how TTS can be mod- eled with almost the same algorithm in reverse, and conclude with a brief mention of other speech tasks. 16.1 The Automatic Speech Recognition Task digit recognition Before describing algorithms for ASR, let’s talk about how the task itself varies. One dimension of variation is vocabulary size. Some ASR tasks can be solved with extremely high accuracy, like those with a 2-word vocabulary (yes versus no) or an 11 word vocabulary like digit recognition (recognizing sequences of digits in- cluding zero to nine plus oh). Open-ended tasks like transcribing videos or human conversations, with large vocabularies of up to 60,000 words, are much harder. read speech conversational speech LibriSpeech Switchboard CALLHOME CORAAL CHiME 16.1 THE AUTOMATIC SPEECH RECOGNITION TASK 3 A second dimension of variation is who the speaker is talking to. Humans speak- ing to machines (either dictating or talking to a dialogue system) are easier to recog- nize than humans speaking to humans. Read speech, in which humans are reading out loud, for example in audio books, is also relatively easy to recognize. Recog- nizing the speech of two humans talking to each other in conversational speech, for example, for transcribing a business meeting, is the hardest. It seems that when humans talk to machines, or read without an audience present, they simplify their speech quite a bit, talking more slowly and more clearly. A third dimension of variation is channel and noise. Speech is easier to recognize if it’s recorded in a quiet room with head-mounted microphones than if it’s recorded by a distant microphone on a noisy city street, or in a car with the window open. A ﬁnal dimension of variation is accent or speaker-class characteristics. Speech is easier to recognize if the speaker is speaking the same dialect or variety that the system was trained on. Speech by speakers of regional or ethnic dialects, or speech by children can be quite difﬁcult to recognize if the system is only trained on speak- ers of standard dialects, or only adult speakers. A number of publicly available corpora with human-created transcripts are used to create ASR test and training sets to explore this variation; we mention a few of them here since you will encounter them in the literature. LibriSpeech is a large open-source read-speech 16 kHz dataset with over 1000 hours of audio books from the LibriVox project, with transcripts aligned at the sentence level (Panayotov et al., 2015). It is divided into an easier (“clean”) and a more difﬁcult portion (“other”) with the clean portion of higher recording quality and with accents closer to US English. This was done by running a speech recognizer (trained on read speech from the Wall Street Journal) on all the audio, computing the WER for each speaker based on the gold transcripts, and dividing the speakers roughly in half, with recordings from lower-WER speakers called “clean” and recordings from higher-WER speakers “other”. The Switchboard corpus of prompted telephone conversations between strangers was collected in the early 1990s; it contains 2430 conversations averaging 6 min- utes each, totaling 240 hours of 8 kHz speech and about 3 million words (Godfrey et al., 1992). Switchboard has the singular advantage of an enormous amount of auxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic and prosodic labeling, and discourse and information structure. The CALLHOME corpus was collected in the late 1990s and consists of 120 unscripted 30-minute telephone conversations between native speakers of English who were usually close friends or family (Canavan et al., 1997). The Santa Barbara Corpus of Spoken American English (Du Bois et al., 2005) is a large corpus of naturally occurring everyday spoken interactions from all over the United States, mostly face-to-face conversation, but also town-hall meetings, food preparation, on-the-job talk, and classroom lectures. The corpus was anonymized by removing personal names and other identifying information (replaced by pseudonyms in the transcripts, and masked in the audio). CORAAL is a collection of over 150 sociolinguistic interviews with African American speakers, with the goal of studying African American Language (AAL), the many variations of language used in African American communities (Kendall and Farrington, 2020). The interviews are anonymized with transcripts aligned at the utterance level. The CHiME Challenge is a series of difﬁcult shared tasks with corpora that deal with robustness in ASR. The CHiME 5 task, for example, is ASR of conversational speech in real home environments (speciﬁcally dinner parties). The 4 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH HKUST AISHELL-1 corpus contains recordings of twenty different dinner parties in real homes, each with four participants, and in three locations (kitchen, dining area, living room), recorded both with distant room microphones and with body-worn mikes. The HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone con- versations between speakers of Mandarin across China, including transcripts of the conversations, which are between either friends or strangers (Liu et al., 2006). The AISHELL-1 corpus contains 170 hours of Mandarin read speech of sentences taken from various domains, read by different speakers mainly from northern China (Bu et al., 2017). Figure 16.1 shows the rough percentage of incorrect words (the word error rate, or WER, deﬁned on page 15) from state-of-the-art systems on some of these tasks. Note that the error rate on read speech (like the LibriSpeech audiobook corpus) is around 2%; this is a solved task, although these numbers come from systems that re- quire enormous computational resources. By contrast, the error rate for transcribing conversations between humans is much higher; 5.8 to 11% for the Switchboard and CALLHOME corpora. The error rate is higher yet again for speakers of varieties like African American Vernacular English, and yet again for difﬁcult conversational tasks like transcription of 4-speaker dinner party speech, which can have error rates as high as 81.3%. Character error rates (CER) are also much lower for read Man- darin speech than for natural conversation. WER% English Tasks 1.4 LibriSpeech audiobooks 960hour clean 2.6 LibriSpeech audiobooks 960hour other 5.8 Switchboard telephone conversations between strangers 11.0 CALLHOME telephone conversations between family 27.0 Sociolinguistic interviews, CORAAL (AAL) 47.9 CHiMe5 dinner parties with body-worn microphones 81.3 CHiMe5 dinner parties with distant microphones CER% Chinese (Mandarin) Tasks 6.7 AISHELL-1 Mandarin read speech corpus HKUST Mandarin Chinese telephone conversations 23.5 Figure 16.1 Rough Word Error Rates (WER = % of words misrecognized) reported around 2020 for ASR on various American English recognition tasks, and character error rates (CER) for two Chinese recognition tasks. 16.2 Feature Extraction for ASR: Log Mel Spectrum feature vector The ﬁrst step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, each vector representing the information in a small time window of the signal. Let’s see how to convert a raw waveﬁle to the most commonly used features, sequences of log mel spectrum vectors. A speech signal processing course is recommended for more details. 16.2.1 Sampling and Quantization Recall from Section ?? that the ﬁrst step is to convert the analog representations (ﬁrst air pressure and then analog electric signals in a microphone) into a digital signal. sampling sampling rate Nyquist frequency telephone- bandwidth quantization stationary non-stationary frame stride rectangular Hamming 16.2 FEATURE EXTRACTION FOR ASR: LOG MEL SPECTRUM 5 This analog-to-digital conversion has two steps: sampling and quantization. A signal is sampled by measuring its amplitude at a particular time; the sampling rate is the number of samples taken per second. To accurately measure a wave, we must have at least two samples in each cycle: one measuring the positive part of the wave and one measuring the negative part. More than two samples per cycle increases the amplitude accuracy, but less than two samples will cause the frequency of the wave to be completely missed. Thus, the maximum frequency wave that can be measured is one whose frequency is half the sample rate (since every cycle needs two samples). This maximum frequency for a given sampling rate is called the Nyquist frequency. Most information in human speech is in frequencies below 10,000 Hz, so a 20,000 Hz sampling rate would be necessary for complete accuracy. But telephone speech is ﬁltered by the switching network, and only frequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz sampling rate is sufﬁcient for telephone-bandwidth speech, and 16,000 Hz for microphone speech. Although using higher sampling rates produces higher ASR accuracy, we can’t combine different sampling rates for training and testing ASR systems. Thus if we are testing on a telephone corpus like Switchboard (8 KHz sampling), we must downsample our training corpus to 8 KHz. Similarly, if we are training on mul- tiple corpora and one of them includes telephone speech, we downsample all the wideband corpora to 8Khz. Amplitude measurements are stored as integers, either 8 bit (values from -128– 127) or 16 bit (values from -32768–32767). This process of representing real-valued numbers as integers is called quantization; all values that are closer together than the minimum granularity (the quantum size) are represented identically. We refer to each sample at time index n in the digitized, quantized waveform as x[n]. 16.2.2 Windowing From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a par- ticular phoneme. Inside this small window, we can roughly think of the signal as stationary (that is, its statistical properties are constant within this region). (By contrast, in general, speech is a non-stationary signal, meaning that its statistical properties are not constant over time). We extract this roughly stationary portion of speech by using a window which is non-zero inside a region and zero elsewhere, run- ning this window across the speech signal and multiplying it by the input waveform to produce a windowed waveform. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. To extract the signal we multiply the value of the signal at time n, s[n] by the value of the window at time n, w[n]: y[n] = w[n]s[n] The window shape sketched in Fig. 16.2 is rectangular; you can see the ex- tracted windowed signal looks just like the original signal. The rectangular window, however, abruptly cuts off the signal at its boundaries, which creates problems when we do Fourier analysis. For this reason, for acoustic feature creation we more com- monly use the Hamming window, which shrinks the values of the signal toward (16.1) 6 CHAPTER 16 Discrete Fourier transform DFT AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH Shift10 ms Window25 ms Window25 ms Window25 ms Shift10 ms Figure 16.2 Windowing, showing a 25 ms rectangular window with a 10ms stride. zero at the window boundaries, avoiding discontinuities. Figure 16.3 shows both; the equations are as follows (assuming a window that is L frames long): (cid:26) 1 0 1 L n 0 otherwise 0.46 cos( 2πn L ) w[n] = rectangular ≤ ≤ − (cid:26) 0.54 0 L n 0 otherwise 1 w[n] = Hamming − − ≤ ≤ 0.0475896 –0.4826 0.4999 Time (s)0.00455938 0 0 Time (s)0.00455938 Rectangular windowHamming window –0.5 –0.5 0.0256563 Time (s)0 0.4999 0 0.4999 0.0256563 Figure 16.3 Windowing a sine wave with the rectangular or Hamming windows. 16.2.3 Discrete Fourier Transform The next step is to extract spectral information for our windowed signal; we need to know how much energy the signal contains at different frequency bands. The tool for extracting spectral information for discrete frequency bands for a discrete-time (sampled) signal is the discrete Fourier transform or DFT. (16.2) (16.3) Euler’s formula fast Fourier transform FFT mel 16.2 FEATURE EXTRACTION FOR ASR: LOG MEL SPECTRUM 7 The input to the DFT is a windowed signal x[n]...x[m], and the output, for each of N discrete frequency bands, is a complex number X[k] representing the magnitude and phase of that frequency component in the original signal. If we plot the mag- nitude against the frequency, we can visualize the spectrum that we introduced in Chapter 28. For example, Fig. 16.4 shows a 25 ms Hamming-windowed portion of a signal and its spectrum as computed by a DFT (with some additional smoothing). 0.04414 Time (s)0.0141752 –0.04121 0.039295 0 Sound pressure level (dB/Hz)–20 20 8000 Frequency (Hz)0 0 (a) (b) (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy] Figure 16.4 and (b) its spectrum computed by a DFT. We do not introduce the mathematical details of the DFT here, except to note that Fourier analysis relies on Euler’s formula, with j as the imaginary unit: e jθ = cos θ + j sin θ (16.4) As a brief reminder for those students who have already studied signal processing, the DFT is deﬁned as follows: X[k] = N 1 (cid:88) − x[n]e− j 2π N kn (16.5) n=0 A commonly used algorithm for computing the DFT is the fast Fourier transform or FFT. This implementation of the DFT is very efﬁcient but only works for values of N that are powers of 2. 16.2.4 Mel Filter Bank and Log The results of the FFT tell us the energy at each frequency band. Human hearing, however, is not equally sensitive at all frequency bands; it is less sensitive at higher frequencies. This bias toward low frequencies helps human recognition, since infor- mation in low frequencies like formants is crucial for distinguishing values or nasals, while information in high frequencies like stop bursts or fricative noise is less cru- cial for successful recognition. Modeling this human perceptual property improves speech recognition performance in the same way. We implement this intuition by collecting energies, not equally at each frequency band, but according to the mel scale, an auditory frequency scale (Chapter 28). A mel (Stevens et al. 1937, Stevens and Volkmann 1940) is a unit of pitch. Pairs of sounds that are perceptually equidistant in pitch are separated by an equal number of mels. The mel frequency m can be computed from the raw acoustic frequency by a log transformation: mel( f ) = 1127 ln(1 + f 700 ) (16.6) 8 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH We implement this intuition by creating a bank of ﬁlters that collect energy from each frequency band, spread logarithmically so that we have very ﬁne resolution at low frequencies, and less resolution at high frequencies. Figure 16.5 shows a sample bank of triangular ﬁlters that implement this idea, that can be multiplied by the spectrum to get a mel spectrum. m1 0 0.5 mM 8K 0 AmplitudeFrequency (Hz) m2 1 ...mel spectrum 7700 Figure 16.5 The mel ﬁlter bank (Davis and Mermelstein, 1980). Each triangular ﬁlter, spaced logarithmically along the mel scale, collects energy from a given frequency range. Finally, we take the log of each of the mel spectrum values. The human response to signal level is logarithmic (like the human response to frequency). Humans are less sensitive to slight differences in amplitude at high amplitudes than at low ampli- tudes. In addition, using a log makes the feature estimates less sensitive to variations in input such as power variations due to the speaker’s mouth moving closer or further from the microphone. 16.3 Speech Recognition Architecture The basic architecture for ASR is the encoder-decoder (implemented with either RNNs or Transformers), exactly the same architecture introduced for MT in Chap- ter 13. Generally we start from the log mel spectral features described in the previous section, and map to letters, although it’s also possible to map to induced morpheme- like chunks like wordpieces or BPE. AED listen attend and spell Fig. 16.6 sketches the standard encoder-decoder architecture, which is com- monly referred to as the attention-based encoder decoder or AED, or listen attend and spell (LAS) after the two papers which ﬁrst applied it to speech (Chorowski et al. 2014, Chan et al. 2016). The input is a sequence of t acoustic feature vectors F = f1, f2, ..., ft , one vector per 10 ms frame. The output can be letters or word- pieces; we’ll assume letters here. Thus the output sequence Y = ( , y1, ..., ym(cid:104) SOS (cid:105) (cid:104) eos and sos assuming special start of sequence and end of sequence tokens (cid:105) (cid:104) (cid:105) (cid:104) each yi is a character; for English we might choose the set: yi ∈ { Of course the encoder-decoder architecture is particularly appropriate when in- put and output sequences have stark length differences, as they do for speech, with very long acoustic feature sequences mapping to much shorter sequences of letters or words. A single word might be 5 letters long but, supposing it lasts about 2 seconds, would take 200 acoustic frames (of 10ms each). a, b, c, ..., z, 0, ..., 9, , space (cid:105) (cid:104) comma (cid:104) , (cid:105) period (cid:104) , (cid:105) apostrophe (cid:104) , (cid:105) unk (cid:104) (cid:105)} Because this length difference is so extreme for speech, encoder-decoder ar- chitectures for speech need to have a special compression stage that shortens the acoustic feature sequence before the encoder stage. (Alternatively, we can use a loss ), EOS (cid:105) and low frame rate n-best list rescore 16.3 SPEECH RECOGNITION ARCHITECTURE 9 ft f1 y7ti y5s y3t‘ Subsampling…H … y1<s>i y2it Feature Computation DECODER … xn y8im ENCODER… y4‘s 80-dimensional log Mel spectrumper frameShorter sequence X y9me ym x1 y6 t Figure 16.6 Schematic architecture for an encoder-decoder speech recognizer. function that is designed to deal well with compression, like the CTC loss function we’ll introduce in the next section.) The goal of the subsampling is to produce a shorter sequence X = x1, ..., xn that will be the input to the encoder. The simplest algorithm is a method sometimes called low frame rate (Pundak and Sainath, 2016): for time i we stack (concatenate) the acoustic feature vector fi with the prior two vectors fi 2 to make a new vector three times longer. Then we simply delete fi 2. Thus instead of (say) a 40-dimensional acoustic feature vector every 10 ms, we have a longer vector (say 120-dimensional) every 30 ms, with a shorter sequence length n = t 1 and fi − 1 and fi − − − 3 .1 After this compression stage, encoder-decoders for speech use the same archi- tecture as for MT or other text, composed of either RNNs (LSTMs) or Transformers. For inference, the probability of the output string Y is decomposed as: p(y1, . . . , yn) = n (cid:89) i=1 p(yi| y1, . . . , yi − 1, X) (16.7) We can produce each letter of the output via greedy decoding: ˆyi = argmaxchar ∈ AlphabetP(char y1...yi | − 1, X) (16.8) Alternatively we can use beam search as described in the next section. This is par- ticularly relevant when we are adding a language model. Adding a language model Since an encoder-decoder model is essentially a con- ditional language model, encoder-decoders implicitly learn a language model for the output domain of letters from their training data. However, the training data (speech paired with text transcriptions) may not include sufﬁcient text to train a good lan- guage model. After all, it’s easier to ﬁnd enormous amounts of pure text training data than it is to ﬁnd text paired with speech. Thus we can can usually improve a model at least slightly by incorporating a very large language model. The simplest way to do this is to use beam search to get a ﬁnal beam of hy- pothesized sentences; this beam is sometimes called an n-best list. We then use a language model to rescore each hypothesis on the beam. The scoring is done by in- 1 There are also more complex alternatives for subsampling, like using a convolutional net that down- samples with max pooling, or layers of pyramidal RNNs, RNNs where each successive layer has half the number of RNNs as the previous layer. 10 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH terpolating the score assigned by the language model with the encoder-decoder score used to create the beam, with a weight λ tuned on a held-out set. Also, since most models prefer shorter sentences, ASR systems normally have some way of adding a length factor. One way to do this is to normalize the probability by the number of characters in the hypothesis |c. The following is thus a typical scoring function (Chan et al., 2016): Y | score(Y X) = | 1 Y | |c log P(Y X) + λ log PLM(Y ) | 16.3.1 Learning Encoder-decoders for speech are trained with the normal cross-entropy loss gener- ally used for conditional language models. At timestep i of decoding, the loss is the log probability of the correct token (letter) yi: LCE = − log p(yi| y1, . . . , yi − 1, X) The loss for the entire sentence is the sum of these losses: LCE = − m (cid:88) i=1 log p(yi| y1, . . . , yi − 1, X) This loss is then backpropagated through the entire end-to-end model to train the entire encoder-decoder. As we described in Chapter 13, we normally use teacher forcing, in which the decoder history is forced to be the correct gold yi rather than the predicted ˆyi. It’s also possible to use a mixture of the gold and decoder output, for example using the gold output 90% of the time, but with probability .1 taking the decoder output instead: LCE = − log p(yi| y1, . . . , ˆyi − 1, X) 16.4 CTC We pointed out in the previous section that speech recognition has two particular properties that make it very appropriate for the encoder-decoder architecture, where the encoder produces an encoding of the input that the decoder uses attention to explore. First, in speech we have a very long acoustic input sequence X mapping to a much shorter sequence of letters Y , and second, it’s hard to know exactly which part of X maps to which part of Y . CTC In this section we brieﬂy introduce an alternative to encoder-decoder: an algo- rithm and loss function called CTC, short for Connectionist Temporal Classiﬁca- tion (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that the output is the same length as the input, and then to apply a collapsing function that combines sequences of identical letters, resulting in a shorter sequence. Let’s imagine inference on someone saying the word dinner, and let’s suppose we had a function that chooses the most probable letter for each input spectral frame representation xi. We’ll call the sequence of letters corresponding to each input (16.9) (16.10) (16.11) (16.12) alignment blank 16.4 CTC 11 frame an alignment, because it tells us where in the acoustic signal each letter aligns to. Fig. 16.7 shows one such alignment, and what happens if we use a collapsing function that just removes consecutive duplicate letters. d r r x6 X (input)A (alignment)Y (output) d n e n x9 n wavefile x8 x5 x13 n n r x4 e r x14 r x12 r x2 x11 x7 x10 i i i x1 x3 r Figure 16.7 A naive algorithm for collapsing an alignment between input and letters. Well, that doesn’t work; our naive algorithm has transcribed the speech as diner, not dinner! Collapsing doesn’t handle double letters. There’s also another problem with our naive function; it doesn’t tell us what symbol to align with silence in the input. We don’t want to be transcribing silence as random letters! The CTC algorithm solves both problems by adding to the transcription alphabet a special symbol for a blank, which we’ll represent as . The blank can be used in the alignment whenever we don’t want to transcribe a letter. Blank can also be used between letters; since our collapsing function collapses only consecutive duplicate letters, it won’t collapse across . More formally, let’s deﬁne the mapping B : a y between an alignment a and an output y, which collapses all repeated letters and then removes all blanks. Fig. 16.8 sketches this collapsing function B. → r r x8 d x6 n n n r r n n n x5 x13 r e e e x9 r x11 X (input)A (alignment)remove blanks n r d e d x12 x4 x3 d x2 x14 n␣␣␣␣␣␣␣ Y (output) i i i i nmerge duplicates x7 x10 x1 Figure 16.8 The CTC collapsing function B, showing the space blank character (consecutive) characters in an alignment A are removed to form the output Y . ; repeated The CTC collapsing function is many-to-one; lots of different alignments map to the same output string. For example, the alignment shown in Fig. 16.8 is not the only alignment that results in the string dinner. Fig. 16.9 shows some other alignments that would produce the same output. e n n n n n n n d d r␣ d d d e n d e r r r r e n r ␣␣␣ i i e i r ␣␣ i␣␣␣␣␣ Figure 16.9 Three other legitimate alignments producing the transcript dinner. It’s useful to think of the set of all alignments that might produce the same output 12 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH Y . We’ll use the inverse of our B function, called B− B− 1(Y ). 1, and represent that set as 16.4.1 CTC Inference X) let’s ﬁrst see how CTC assigns a proba- Before we see how to compute PCTC(Y | bility to one particular alignment ˆA = ˆa1, . . . , ˆan} . CTC makes a strong conditional { independence assumption: it assumes that, given the input X, the CTC model output at at time t is independent of the output labels at any other time ai. Thus: X) = PCTC(A | T (cid:89) t=1 p(at | X) (16.13) Thus to ﬁnd the best alignment ˆA = ˆa1, . . . , ˆaT } { ter with the max probability at each time step t: we can greedily choose the charac- ˆat = argmax c ∈ C X) pt (c | (16.14) We then pass the resulting sequence A to the CTC collapsing function B to get the output sequence Y . Let’s talk about how this simple inference algorithm for ﬁnding the best align- ment A would be implemented. Because we are making a decision at each time point, we can treat CTC as a sequence-modeling task, where we output one letter ˆyt at time t corresponding to each input token xt , eliminating the need for a full de- coder. Fig. 16.10 sketches this architecture, where we take an encoder, produce a hidden state ht at each timestep, and decode by taking a softmax over the character vocabulary at each time step. ENCODER… y1iy2iy3iy4t Feature Computation Subsampling… yn … ft xnClassiﬁer+softmax log Mel spectrumShorter inputsequence X x1 ty5……output lettersequence Y f1 Figure 16.10 simple softmaxes over the hidden state ht at each output step. Inference with CTC: using an encoder-only model, with decoding done by Alas, there is a potential ﬂaw with the inference algorithm sketched in (Eq. 16.14) and Fig. 16.9. The problem is that we chose the most likely alignment A, but the most likely alignment may not correspond to the most likely ﬁnal collapsed output string Y . That’s because there are many possible alignments that lead to the same output string, and hence the most likely output string might not correspond to the 16.4 CTC 13 most probable alignment. For example, imagine the most probable alignment A for an input X = [x1x2x3] is the string [a b (cid:15)] but the next two most probable alignments are [b (cid:15) b] and [(cid:15) b b]. The output Y =[b b], summing over those two alignments, might be more probable than Y =[a b]. For this reason, the most probable output sequence Y is the one that has, not the single best CTC alignment, but the highest sum over the probability of all its possible alignments: PCTC(Y (cid:88) X) = | X) | P(A 1(Y ) A B− ∈ (cid:88) T (cid:89) p(at | ht ) = t=1 1(Y ) A ∈ ˆY = argmax B− X) | PCTC(Y Y (16.15) Alas, summing over all alignments is very expensive (there are a lot of alignments), so we approximate this sum by using a version of Viterbi beam search that cleverly keeps in the beam the high-probability alignments that map to the same output string, and sums those as an approximation of (Eq. 16.15). See Hannun (2017) for a clear explanation of this extension of beam search for CTC. Because of the strong conditional independence assumption mentioned earlier (that the output at time t is independent of the output at time t 1, given the input), CTC does not implicitly learn a language model over the data (unlike the attention- based encoder-decoder architectures). It is therefore essential when using CTC to interpolate a language model (and some sort of length factor L(Y )) using interpola- tion weights that are trained on a dev set: − scoreCTC(Y X) = log PCTC(Y | X) + λ1 log PLM(Y )λ2L(Y ) | (16.16) 16.4.2 CTC Training To train a CTC-based ASR system, we use negative log-likelihood loss with a special CTC loss function. Thus the loss for an entire dataset D is the sum of the negative log-likelihoods of the correct output Y for each input X: LCTC = (cid:88) (X,Y ) ∈ D − log PCTC(Y X) | (16.17) To compute CTC loss function for a single input pair (X,Y ), we need the probability of the output Y given the input X. As we saw in Eq. 16.15, to compute the probability of a given output Y we need to sum over all the possible alignments that would collapse to Y . In other words: T (cid:89) (cid:88) PCTC(Y X) = | p(at | ht ) 1(Y ) t=1 A B− ∈ Naively summing over all possible alignments is not feasible (there are too many alignments). However, we can efﬁciently compute the sum by using dynamic pro- gramming to merge alignments, with a version of the forward-backward algo- rithm also used to train HMMs (Appendix A) and CRFs. The original dynamic pro- gramming algorithms for both training and inference are laid out in (Graves et al., 2006); see (Hannun, 2017) for a detailed explanation of both. (16.18) 14 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH 16.4.3 Combining CTC and Encoder-Decoder It’s also possible to combine the two architectures/loss functions we’ve described, the cross-entropy loss from the encoder-decoder architecture, and the CTC loss. Fig. 16.11 shows a sketch. For training, we can simply weight the two losses with a λ tuned on a dev set: L = − λ log Pencdec(Y X) | − (1 − λ ) log Pctc(Y X) | (16.19) For inference, we can combine the two with the language model (or the length penalty), again with learned weights: ˆY = argmax Y [λ log Pencdec(Y X) | − (1 − λ ) log PCTC(Y X) + γ log PLM(Y )] (16.20) | H CTC Loss i m xn ENCODER… s … ‘ i Encoder-Decoder Loss t t <s> x1 DECODER ……i t ’ s t i m e … Figure 16.11 Combining the CTC and encoder-decoder loss functions. 16.4.4 Streaming Models: RNN-T for improving CTC streaming Because of the strong independence assumption in CTC (assuming that the output at time t is independent of the output at time t 1), recognizers based on CTC don’t achieve as high an accuracy as the attention-based encoder-decoder recog- nizers. CTC recognizers have the advantage, however, that they can be used for streaming. Streaming means recognizing words on-line rather than waiting until the end of the sentence to recognize them. Streaming is crucial for many applica- tions, from commands to dictation, where we want to start recognition while the user is still talking. Algorithms that use attention need to compute the hidden state sequence over the entire input ﬁrst in order to provide the attention distribution con- text, before the decoder can start decoding. By contrast, a CTC algorithm can input letters from left to right immediately. − RNN-T If we want to do streaming, we need a way to improve CTC recognition to re- move the conditional independent assumption, enabling it to know about output his- tory. The RNN-Transducer (RNN-T), shown in Fig. 16.12, is just such a model (Graves 2012, Graves et al. 2013). The RNN-T has two main components: a CTC acoustic model, and a separate language model component called the predictor that conditions on the output token history. At each time step t, the CTC encoder outputs a hidden state henc given the input x1...xt . The language model predictor takes as in- put the previous output token (not counting blanks), outputting a hidden state hpred . The two are passed through another network whose output is then passed through a t u 16.5 ASR EVALUATION: WORD ERROR RATE 15 softmax to predict the next character. PRNN − T (Y X) = | = A A (cid:88) 1(Y ) B− ∈ (cid:88) ∈ B− 1(Y ) X) P(A | T (cid:89) t=1 p(at | ht , y<ut ) SOFTMAX JOINT NETWORK xt hpredu yu-1 zt,uDECODER ENCODERP ( yt,u | x[1..t] , y[1..u-1] ) henct PREDICTIONNETWORK Figure 16.12 The RNN-T model computing the output token distribution at time t by inte- grating the output of a CTC acoustic encoder and a separate ‘predictor’ language model. 16.5 ASR Evaluation: Word Error Rate word error The standard evaluation metric for speech recognition systems is the word error rate. The word error rate is based on how much the word string returned by the recognizer (the hypothesized word string) differs from a reference transcription. The ﬁrst step in computing word error is to compute the minimum edit distance in words between the hypothesized and correct strings, giving us the minimum num- ber of word substitutions, word insertions, and word deletions necessary to map between the correct and hypothesized strings. The word error rate (WER) is then deﬁned as follows (note that because the equation includes insertions, the error rate can be greater than 100%): Word Error Rate = 100 × Insertions + Substitutions + Deletions Total Words in Correct Transcript alignment Here is a sample alignment between a reference and a hypothesis utterance from the CallHome corpus, showing the counts used to compute the error rate: REF: HYP: Eval: i *** ** UM the PHONE IS i GOT IT TO the ***** I I S D i LEFT THE portable **** FULLEST i LOVE TO portable FORM OF S S S I PHONE UPSTAIRS last night last night STORES S S This utterance has six substitutions, three insertions, and one deletion: Word Error Rate = 100 6 + 3 + 1 13 = 76.9% The standard method for computing word error rates is a free script called sclite, available from the National Institute of Standards and Technologies (NIST) (NIST, 16 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH Sentence error rate 2005). Sclite is given a series of reference (hand-transcribed, gold-standard) sen- tences and a matching set of hypothesis sentences. Besides performing alignments, and computing word error rate, sclite performs a number of other useful tasks. For example, for error analysis it gives useful information such as confusion matrices showing which words are often misrecognized for others, and summarizes statistics of words that are often inserted or deleted. sclite also gives error rates by speaker (if sentences are labeled for speaker ID), as well as useful statistics like the sentence error rate, the percentage of sentences with at least one word error. Statistical signiﬁcance for ASR: MAPSSWE or MacNemar As with other language processing algorithms, we need to know whether a particular improvement in word error rate is signiﬁcant or not. The standard statistical tests for determining if two word error rates are different is the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in Gillick and Cox (1989). The MAPSSWE test is a parametric test that looks at the difference between the number of word errors the two systems produce, averaged across a number of segments. The segments may be quite short or as long as an entire utterance; in general, we want to have the largest number of (short) segments in order to justify the normality assumption and to maximize power. The test requires that the errors in one segment be statistically independent of the errors in another segment. Since ASR systems tend to use trigram LMs, we can approximate this requirement by deﬁning a segment as a region bounded on both sides by words that both recognizers get correct (or by turn/utterance boundaries). Here’s an example from NIST (2007) with four regions: I II III IV REF: |it was|the best|of|times it|was the worst|of times| |it was | | |the best|of|times it|IS the worst |of times|OR|it was | | | | | | | SYS A:|ITS | SYS B:|it was|the best| | | |times it|WON the TEST |of times| |it was | | | | In region I, system A has two errors (a deletion and an insertion) and system B has zero; in region III, system A has one error (a substitution) and system B has two. Let’s deﬁne a sequence of variables Z representing the difference between the errors in the two systems as follows: Ni A Ni B Z the number of errors made on segment i by system A the number of errors made on segment i by system B Ni Ni B, i = 1, 2, , n where n is the number of segments A − · · In the example above, the sequence of Z values is − · · In the example above, the sequence of Z values is − · · In the example above, the sequence of Z values is − · · In the example above, the sequence of Z values is − σ 2 z = n 1 − 1 n (cid:88) i=1 (Zi − µz)2 (16.21) 16.6 TTS 17 Let W = ˆµz σz/√n (16.22) For a large enough n (> 50), W will approximately have a normal distribution with unit variance. The null hypothesis is H0 : µz = 0, and it can thus be rejected if 0.05 (one-tailed), where Z is 2 standard normal and w is the realized value W ; these probabilities can be looked up in the standard tables of the normal distribution. P(Z ) w | 0.05 (two-tailed) or P(Z ) w | ∗ ≥ | ≤ ≥ | ≤ McNemar’s test Earlier work sometimes used McNemar’s test for signiﬁcance, but McNemar’s is only applicable when the errors made by the system are independent, which is not true in continuous speech recognition, where errors made on a word are extremely dependent on errors made on neighboring words. Could we improve on word error rate as a metric? It would be nice, for exam- ple, to have something that didn’t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difﬁcult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 15 on page ??. 16.6 TTS The goal of text-to-speech (TTS) systems is to map from strings of letters to wave- forms, a technology that’s important for a variety of applications from dialogue sys- tems to games to education. Like ASR systems, TTS systems are generally based on the encoder-decoder architecture, either using LSTMs or Transformers. There is a general difference in training. The default condition for ASR systems is to be speaker-independent: they are trained on large corpora with thousands of hours of speech from many speakers because they must generalize well to an unseen test speaker. By contrast, in TTS, it’s less crucial to use multiple voices, and so basic TTS systems are speaker-dependent: trained to have a consistent voice, on much less data, but all from one speaker. For example, one commonly used public domain dataset, the LJ speech corpus, consists of 24 hours of one speaker, Linda Johnson, reading audio books in the LibriVox project (Ito and Johnson, 2017), much smaller than standard ASR corpora which are hundreds or thousands of hours.2 We generally break up the TTS task into two components. The ﬁrst component is an encoder-decoder model for spectrogram prediction: it maps from strings of letters to mel spectrographs: sequences of mel spectral values over time. Thus we 2 There is also recent TTS research on the task of multi-speaker TTS, in which a system is trained on speech from many speakers, and can switch between different voices. 18 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH might map from this string: It’s time for lunch! to the following mel spectrogram: vocoding vocoder The second component maps from mel spectrograms to waveforms. Generating waveforms from intermediate representations like spectrograms is called vocoding and this second component is called a vocoder: These standard encoder-decoder algorithms for TTS are still quite computation- ally intensive, so a signiﬁcant focus of modern research is on ways to speed them up. 16.6.1 TTS Preprocessing: Text normalization non-standard words Before either of these two steps, however, TTS systems require text normaliza- tion preprocessing for handling non-standard words: numbers, monetary amounts, dates, and other concepts that are verbalized differently than they are spelled. A TTS system seeing a number like 151 needs to know to verbalize it as one hundred ﬁfty one if it occurs as $151 but as one ﬁfty one if it occurs in the context 151 Chapulte- pec Ave.. The number 1750 can be spoken in at least four different ways, depending on the context: seventeen fifty: one seven five zero: seventeen hundred and fifty: one thousand, seven hundred, and fifty: (in “The European economy in 1750”) (in “The password is 1750”) (in “1750 dollars”) (in “1750 dollars”) Often the verbalization of a non-standard word depends on its meaning (what Taylor (2009) calls its semiotic class). Fig. 16.13 lays out some English non- standard word types. Many classes have preferred realizations. A year is generally read as paired digits (e.g., seventeen fifty for 1750). $3.2 billion must be read out with the word dollars at the end, as three point two billion dollars. Some ab- breviations like N.Y. are expanded (to New York), while other acronyms like GPU are pronounced as letter sequences. In languages with grammatical gender, normal- ization may depend on morphological properties. In French, the phrase 1 mangue (‘one mangue’) is normalized to une mangue, but 1 ananas (‘one pineapple’) is normalized to un ananas. In German, Heinrich IV (‘Henry IV’) can be normalized to Heinrich der Vierte, Heinrich des Vierten, Heinrich dem Vierten, or Heinrich den Vierten depending on the grammatical case of the noun (Demberg, 2006). 16.6 TTS semiotic class abbreviations acronyms read as letters GPU, D.C., PC, UN, IBM G P U twelve cardinal numbers seventh ordinal numbers one oh one numbers read as digits eleven forty ﬁve times February twenty eighth dates nineteen ninety nine years three dollars forty ﬁve money three point four ﬁve billion dollars money in tr/m/billions seventy ﬁve percent percentage examples gov’t, N.Y., mph verbalization government 12, 45, 1/2, 0.6 May 7, 3rd, Bill Gates III Room 101 3.20, 11:45 28/02 (or in US, 2/28) 1999, 80s, 1900s, 2045 $3.45, e250, $200K $3.45 billion 75% 3.4% Figure 16.13 Some types of non-standard words in text normalization; see Sproat et al. (2001) and (van Esch and Sproat, 2018) for many more. Modern end-to-end TTS systems can learn to do some normalization themselves, but TTS systems are only trained on a limited amount of data (like the 220,000 words we mentioned above for the LJ corpus (Ito and Johnson, 2017)), and so a separate normalization step is important. Normalization can be done by rule or by an encoder-decoder model. Rule-based normalization is done in two stages: tokenization and verbalization. In the tokeniza- tion stage we hand-write write rules to detect non-standard words. These can be regular expressions, like the following for detecting years: (20[0-9][0-9]/ /(1[89][0-9][0-9]) | A second pass of rules express how to verbalize each semiotic class. Larger TTS systems instead use more complex rule-systems, like the Kestral system of (Ebden and Sproat, 2015), which ﬁrst classiﬁes and parses each input into a normal form and then produces text using a verbalization grammar. Rules have the advantage that they don’t require training data, and they can be designed for high precision, but can be brittle, and require expert rule-writers so are hard to maintain. The alternative model is to use encoder-decoder models, which have been shown to work better than rules for such transduction tasks, but do require expert-labeled training sets in which non-standard words have been replaced with the appropriate verbalization; such training sets for some languages are available (Sproat and Gor- man 2018, Zhang et al. 2019). In the simplest encoder-decoder setting, we simply treat the problem like ma- chine translation, training a system to map from: They live at 224 Mission St. to They live at two twenty four Mission Street While encoder-decoder algorithms are highly accurate, they occasionally pro- duce errors that are egregious; for example normalizing 45 minutes as forty ﬁve mil- limeters. To address this, more complex systems use mechanisms like lightweight covering grammars, which enumerate a large set of possible verbalizations but don’t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang et al., 2019). 16.6.2 TTS: Spectrogram prediction The exact same architecture we described for ASR—the encoder-decoder with attention– can be used for the ﬁrst component of TTS. Here we’ll give a simpliﬁed overview 19 20 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH Tacotron2 Wavenet of the Tacotron2 architecture (Shen et al., 2018), which extends the earlier Tacotron (Wang et al., 2017) architecture and the Wavenet vocoder (van den Oord et al., 2016). Fig. 16.14 sketches out the entire architecture. location-based attention The encoder’s job is to take a sequence of letters and produce a hidden repre- sentation representing the letter sequence, which is then used by the attention mech- anism in the decoder. The Tacotron2 encoder ﬁrst maps every input grapheme to a 512-dimensional character embedding. These are then passed through a stack of 3 convolutional layers, each containing 512 ﬁlters with shape 5 1, i.e. each ﬁlter spanning 5 characters, to model the larger letter context. The output of the ﬁnal convolutional layer is passed through a biLSTM to produce the ﬁnal encod- ing. It’s common to use a slightly higher quality (but slower) version of attention called location-based attention, in which the computation of the α values (Eq. ?? in Chapter 13) makes use of the α values from the prior time-state. × In the decoder, the predicted mel spectrum from the prior time slot is passed through a small pre-net as a bottleneck. This prior output is then concatenated with the encoder’s attention vector context and passed through 2 LSTM layers. The out- put of this LSTM is used in two ways. First, it is passed through a linear layer, and some output processing, to autoregressively predict one 80-dimensional log-mel ﬁl- terbank vector frame (50 ms, with a 12.5 ms stride) at each step. Second, it is passed through another linear layer to a sigmoid to make a “stop token prediction” decision about whether to stop producing output. Fig.1.BlockdiagramoftheTacotron2systemarchitecture.post-netlayeriscomprisedof512ﬁlterswithshape5⇥1withbatchnormalization,followedbytanhactivationsonallbuttheﬁnallayer.Weminimizethesummedmeansquarederror(MSE)frombeforeandafterthepost-nettoaidconvergence.Wealsoexperimentedwithalog-likelihoodlossbymodelingtheoutputdistributionwithaMixtureDensityNetwork[23,24]toavoidassumingaconstantvarianceovertime,butfoundthattheseweremoredifﬁculttotrainandtheydidnotleadtobettersoundingsamples.Inparalleltospectrogramframeprediction,theconcatenationofdecoderLSTMoutputandtheattentioncontextisprojecteddowntoascalarandpassedthroughasigmoidactivationtopredicttheprobabilitythattheoutputsequencehascompleted.This“stoptoken”predictionisusedduringinferencetoallowthemodeltodynamicallydeterminewhentoterminategenerationinsteadofalwaysgeneratingforaﬁxedduration.Speciﬁcally,generationcompletesattheﬁrstframeforwhichthisprobabilityexceedsathresholdof0.5.Theconvolutionallayersinthenetworkareregularizedusingdropout[25]withprobability0.5,andLSTMlayersareregularizedusingzoneout[26]withprobability0.1.Inordertointroduceoutputvariationatinferencetime,dropoutwithprobability0.5isappliedonlytolayersinthepre-netoftheautoregressivedecoder.IncontrasttotheoriginalTacotron,ourmodelusessimplerbuild-ingblocks,usingvanillaLSTMandconvolutionallayersintheen-coderanddecoderinsteadof“CBHG”stacksandGRUrecurrentlayers.Wedonotusea“reductionfactor”,i.e.,eachdecoderstepcorrespondstoasinglespectrogramframe.2.3.WaveNetVocoderWeuseamodiﬁedversionoftheWaveNetarchitecturefrom[8]toinvertthemelspectrogramfeaturerepresentationintotime-domainwaveformsamples.Asintheoriginalarchitecture,thereare30dilatedconvolutionlayers,groupedinto3dilationcycles,i.e.,thedilationrateoflayerk(k=0...29)is2k(mod10).Toworkwiththe12.5msframehopofthespectrogramframes,only2upsamplinglayersareusedintheconditioningstackinsteadof3layers.Insteadofpredictingdiscretizedbucketswithasoftmaxlayer,wefollowPixelCNN++[27]andParallelWaveNet[28]andusea10-componentmixtureoflogisticdistributions(MoL)togenerate16-bitsamplesat24kHz.Tocomputethelogisticmixturedistribution,theWaveNetstackoutputispassedthroughaReLUactivationfollowedEncoderDecoderVocoder (cid:22)(cid:4)(cid:48)(cid:55)(cid:56)(cid:49)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:87) (cid:48)(cid:77)(cid:82)(cid:73)(cid:69)(cid:86)(cid:4)(cid:52)(cid:86)(cid:83)(cid:78)(cid:73)(cid:71)(cid:88)(cid:77)(cid:83)(cid:82) (cid:59)(cid:69)(cid:90)(cid:73)(cid:50)(cid:73)(cid:88)(cid:4)(cid:49)(cid:83)(cid:48) (cid:59)(cid:69)(cid:90)(cid:73)(cid:74)(cid:83)(cid:86)(cid:81)(cid:4)(cid:55)(cid:69)(cid:81)(cid:84)(cid:80)(cid:73)(cid:87) (cid:22)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:4)(cid:52)(cid:86)(cid:73)(cid:17)(cid:50)(cid:73)(cid:88) (cid:38)(cid:77)(cid:72)(cid:77)(cid:86)(cid:73)(cid:71)(cid:88)(cid:77)(cid:83)(cid:82)(cid:69)(cid:80)(cid:4)(cid:48)(cid:55)(cid:56)(cid:49) (cid:23)(cid:4)(cid:39)(cid:83)(cid:82)(cid:90)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:87) (cid:55)(cid:88)(cid:83)(cid:84)(cid:4)(cid:56)(cid:83)(cid:79)(cid:73)(cid:82) (cid:48)(cid:77)(cid:82)(cid:73)(cid:69)(cid:86)(cid:4)(cid:52)(cid:86)(cid:83)(cid:78)(cid:73)(cid:71)(cid:88)(cid:77)(cid:83)(cid:82) (cid:39)(cid:76)(cid:69)(cid:86)(cid:69)(cid:71)(cid:88)(cid:73)(cid:86)(cid:4)(cid:41)(cid:81)(cid:70)(cid:73)(cid:72)(cid:72)(cid:77)(cid:82)(cid:75) Whilelinearspectrogramsdiscardphaseinformation(andarethereforelossy),algorithmssuchasGrifﬁn-Lim[14]arecapableofestimatingthisdiscardedinformation,whichenablestime-domainconversionviatheinverseshort-timeFouriertransform.Melspectro-gramsdiscardevenmoreinformation,presentingachallengingin-verseproblem.However,incomparisontothelinguisticandacousticfeaturesusedinWaveNet,themelspectrogramisasimpler,lower-levelacousticrepresentationofaudiosignals.ItshouldthereforebestraightforwardforasimilarWaveNetmodelconditionedonmelspectrogramstogenerateaudio,essentiallyasaneuralvocoder.In-deed,wewillshowthatitispossibletogeneratehighqualityaudiofrommelspectrogramsusingamodiﬁedWaveNetarchitecture.2.2.SpectrogramPredictionNetworkAsinTacotron,melspectrogramsarecomputedthroughashort-timeFouriertransform(STFT)usinga50msframesize,12.5msframehop,andaHannwindowfunction.Weexperimentedwitha5msframehoptomatchthefrequencyoftheconditioninginputsintheoriginalWaveNet,butthecorrespondingincreaseintemporalresolutionresultedinsigniﬁcantlymorepronunciationissues.WetransformtheSTFTmagnitudetothemelscaleusingan80channelmelﬁlterbankspanning125Hzto7.6kHz,followedbylogdynamicrangecompression.Priortologcompression,theﬁlterbankoutputmagnitudesareclippedtoaminimumvalueof0.01inordertolimitdynamicrangeinthelogarithmicdomain.Thenetworkiscomposedofanencoderandadecoderwithatten-tion.Theencoderconvertsacharactersequenceintoahiddenfeaturerepresentationwhichthedecoderconsumestopredictaspectrogram.Inputcharactersarerepresentedusingalearned512-dimensionalcharacterembedding,whicharepassedthroughastackof3convolu-tionallayerseachcontaining512ﬁlterswithshape5⇥1,i.e.,whereeachﬁlterspans5characters,followedbybatchnormalization[18]andReLUactivations.AsinTacotron,theseconvolutionallayersmodellonger-termcontext(e.g.,N-grams)intheinputcharactersequence.Theoutputoftheﬁnalconvolutionallayerispassedintoasinglebi-directional[19]LSTM[20]layercontaining512units(256ineachdirection)togeneratetheencodedfeatures.Theencoderoutputisconsumedbyanattentionnetworkwhichsummarizesthefullencodedsequenceasaﬁxed-lengthcontextvectorforeachdecoderoutputstep.Weusethelocation-sensitiveattentionfrom[21],whichextendstheadditiveattentionmechanism[22]tousecumulativeattentionweightsfrompreviousdecodertimestepsasanadditionalfeature.Thisencouragesthemodeltomoveforwardconsistentlythroughtheinput,mitigatingpotentialfailuremodeswheresomesubsequencesarerepeatedorignoredbythedecoder.Attentionprobabilitiesarecomputedafterprojectinginputsandlo-cationfeaturesto128-dimensionalhiddenrepresentations.Locationfeaturesarecomputedusing321-Dconvolutionﬁltersoflength31.Thedecoderisanautoregressiverecurrentneuralnetworkwhichpredictsamelspectrogramfromtheencodedinputsequenceoneframeatatime.Thepredictionfromtheprevioustimestepisﬁrstpassedthroughasmallpre-netcontaining2fullyconnectedlayersof256hiddenReLUunits.Wefoundthatthepre-netactingasaninformationbottleneckwasessentialforlearningattention.Thepre-netoutputandattentioncontextvectorareconcatenatedandpassedthroughastackof2uni-directionalLSTMlayerswith1024units.TheconcatenationoftheLSTMoutputandtheattentioncontextvectorisprojectedthroughalineartransformtopredictthetargetspectrogramframe.Finally,thepredictedmelspectrogramispassedthrougha5-layerconvolutionalpost-netwhichpredictsaresidualtoaddtothepredictiontoimprovetheoverallreconstruction.Each (cid:48)(cid:72)(cid:79)(cid:3)(cid:54)(cid:83)(cid:72)(cid:70)(cid:87)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80) (cid:45)(cid:82)(cid:84)(cid:89)(cid:88)(cid:4)(cid:56)(cid:73)(cid:92)(cid:88) (cid:48)(cid:83)(cid:71)(cid:69)(cid:88)(cid:77)(cid:83)(cid:82)(cid:4)(cid:55)(cid:73)(cid:82)(cid:87)(cid:77)(cid:88)(cid:77)(cid:90)(cid:73)(cid:4)(cid:37)(cid:88)(cid:88)(cid:73)(cid:82)(cid:88)(cid:77)(cid:83)(cid:82) (cid:25)(cid:4)(cid:39)(cid:83)(cid:82)(cid:90)(cid:4)(cid:48)(cid:69)(cid:93)(cid:73)(cid:86)(cid:4)(cid:52)(cid:83)(cid:87)(cid:88)(cid:17)(cid:50)(cid:73)(cid:88) Figure 16.14 The Tacotron2 architecture: An encoder-decoder maps from graphemes to mel spectrograms, followed by a vocoder that maps to waveﬁles. Figure modiﬁed from Shen et al. (2018). The system is trained on gold log-mel ﬁlterbank features, using teacher forcing, that is the decoder is fed the correct log-model spectral feature at each decoder step instead of the predicted decoder output from the prior step. 16.6.3 TTS: Vocoding WaveNet The vocoder for Tacotron 2 is an adaptation of the WaveNet vocoder (van den Oord et al., 2016). Here we’ll give a somewhat simpliﬁed description of vocoding using WaveNet. Recall that the goal of the vocoding process here will be to invert a log mel spec- trum representations back into a time-domain waveform representation. WaveNet is an autoregressive network, like the language models we introduced in Chapter 9. It Figure3:Visualizationofastackofdilatedcausalconvolutionallayers.Stackeddilatedconvolutionsenablenetworkstohaveverylargereceptiveﬁeldswithjustafewlay-ers,whilepreservingtheinputresolutionthroughoutthenetworkaswellascomputationalefﬁciency.Inthispaper,thedilationisdoubledforeverylayeruptoalimitandthenrepeated:e.g.1,2,4,...,512,1,2,4,...,512,1,2,4,...,512.Theintuitionbehindthisconﬁgurationistwo-fold.First,exponentiallyincreasingthedilationfactorresultsinexponentialreceptiveﬁeldgrowthwithdepth(Yu&Koltun,2016).Forexampleeach1,2,4,...,512blockhasreceptiveﬁeldofsize1024,andcanbeseenasamoreefﬁcientanddis-criminative(non-linear)counterpartofa1⇥1024convolution.Second,stackingtheseblocksfurtherincreasesthemodelcapacityandthereceptiveﬁeldsize.2.2SOFTMAXDISTRIBUTIONSOneapproachtomodelingtheconditionaldistributionsp(xt|x1,...,xt\\x001)overtheindividualaudiosampleswouldbetouseamixturemodelsuchasamixturedensitynetwork(Bishop,1994)ormixtureofconditionalGaussianscalemixtures(MCGSM)(Theis&Bethge,2015).However,vandenOordetal.(2016a)showedthatasoftmaxdistributiontendstoworkbetter,evenwhenthedataisimplicitlycontinuous(asisthecaseforimagepixelintensitiesoraudiosamplevalues).Oneofthereasonsisthatacategoricaldistributionismoreﬂexibleandcanmoreeasilymodelarbitrarydistributionsbecauseitmakesnoassumptionsabouttheirshape.Becauserawaudioistypicallystoredasasequenceof16-bitintegervalues(onepertimestep),asoftmaxlayerwouldneedtooutput65,536probabilitiespertimesteptomodelallpossiblevalues.Tomakethismoretractable,weﬁrstapplyaµ-lawcompandingtransformation(ITU-T,1988)tothedata,andthenquantizeitto256possiblevalues:f(xt)=sign(xt)ln(1+µ|xt|) ln(1+µ),3 Becausemodelswithcausalconvolutionsdonothaverecurrentconnections,theyaretypicallyfastertotrainthanRNNs,especiallywhenappliedtoverylongsequences.Oneoftheproblemsofcausalconvolutionsisthattheyrequiremanylayers,orlargeﬁlterstoincreasethereceptiveﬁeld.Forexample,inFig.2thereceptiveﬁeldisonly5(=#layers+ﬁlterlength-1).Inthispaperweusedilatedconvolutionstoincreasethereceptiveﬁeldbyordersofmagnitude,withoutgreatlyincreasingcomputationalcost.Adilatedconvolution(alsocalled`atrous,orconvolutionwithholes)isaconvolutionwheretheﬁlterisappliedoveranarealargerthanitslengthbyskippinginputvalueswithacertainstep.Itisequivalenttoaconvolutionwithalargerﬁlterderivedfromtheoriginalﬁlterbydilatingitwithzeros,butissigniﬁcantlymoreefﬁcient.Adilatedconvolutioneffectivelyallowsthenetworktooperateonacoarserscalethanwithanormalconvolution.Thisissimilartopoolingorstridedconvolutions,butheretheoutputhasthesamesizeastheinput.Asaspecialcase,dilatedconvolutionwithdilation1yieldsthestandardconvolution.Fig.3depictsdilatedcausalconvolutionsfordilations1,2,4,and8.Dilatedconvolutionshavepreviouslybeenusedinvariouscontexts,e.g.signalprocessing(Holschneideretal.,1989;Dutilleux,1989),andimagesegmentation(Chenetal.,2015;Yu&Koltun,2016). InputHidden LayerDilation = 1Hidden LayerDilation = 2Hidden LayerDilation = 4OutputDilation = 8 16.6 TTS 21 takes spectrograms as input and produces audio output represented as sequences of 8-bit mu-law (page ??). The probability of a waveform , a sequence of 8-bit mu-law values Y = y1, ..., yt , given an intermediate input mel spectrogram h is computed as: t (cid:89) p(Y ) = P(yt | y1, ..., yt 1, h1, ..., ht ) (16.23) − t=1 This probability distribution is modeled by a stack of special convolution layers, which include a speciﬁc convolutional structure called dilated convolutions, and a speciﬁc non-linearity function. A dilated convolution is a subtype of causal convolutional layer. Causal or masked convolutions look only at the past input, rather than the future; the pre- diction of yt+1 can only depend on y1, ..., yt , useful for autoregressive left-to-right processing. In dilated convolutions, at each successive layer we apply the convolu- tional ﬁlter over a span longer than its length by skipping input values. Thus at time t with a dilation value of 1, a convolutional ﬁlter of length 2 would see input values xt and xt 1. But a ﬁlter with a distillation value of 2 would skip an input, so would see input values xt and xt 1. Fig. 16.15 shows the computation of the output at time t with 4 dilated convolution layers with dilation values, 1, 2, 4, and 8. dilated convolutions − − Figure 16.15 Dilated convolutions, showing one dilation cycle size of 4, i.e., dilation values of 1, 2, 4, 8. Figure from van den Oord et al. (2016). The Tacotron 2 synthesizer uses 12 convolutional layers in two cycles with a dilation cycle size of 6, meaning that the ﬁrst 6 layers have dilations of 1, 2, 4, 8, 16, and 32. and the next 6 layers again have dilations of 1, 2, 4, 8, 16, and 32. Dilated convolutions allow the vocoder to grow the receptive ﬁeld exponentially with depth. WaveNet predicts mu-law audio samples. Recall from page ?? that this is a standard compression for audio in which the values at each sampling timestep are compressed into 8-bits. This means that we can predict the value of each sample with a simple 256-way categorical classiﬁer. The output of the dilated convolutions is thus passed through a softmax which makes this 256-way decision. The spectrogram prediction encoder-decoder and the WaveNet vocoder are trained separately. After the spectrogram predictor is trained, the spectrogram prediction network is run in teacher-forcing mode, with each predicted spectral frame condi- tioned on the encoded text input and the previous frame from the ground truth spec- trogram. This sequence of ground truth-aligned spectral features and gold audio output is then used to train the vocoder. This has been only a high-level sketch of the TTS process. There are numer- ous important details that the reader interested in going further with TTS may want 22 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH to look into. For example WaveNet uses a special kind of a gated activation func- In practice, tion as its non-linearity, and contains residual and skip connections. predicting 8-bit audio values doesn’t as work as well as 16-bit, for which a simple softmax is insufﬁcient, so decoders use fancier ways as the last step of predicting audio sample values, like mixtures of distributions. Finally, the WaveNet vocoder as we have described it would be so slow as to be useless; many different kinds of efﬁciency improvements are necessary in practice, for example by ﬁnding ways to do non-autoregressive generation, avoiding the latency of having to wait to generate each frame until the prior frame has been generated, and instead making predictions in parallel. We encourage the interested reader to consult the original papers and various version of the code. 16.6.4 TTS Evaluation Speech synthesis systems are evaluated by human listeners. (The development of a good automatic metric for synthesis evaluation, one that would eliminate the need for expensive and time-consuming human listening experiments, remains an open and exciting research topic.) MOS We evaluate the quality of synthesized utterances by playing a sentence to lis- teners and ask them to give a mean opinion score (MOS), a rating of how good the synthesized utterances are, usually on a scale from 1–5. We can then compare systems by comparing their MOS scores on the same sentences (using, e.g., paired t-tests to test for signiﬁcant differences). AB tests If we are comparing exactly two systems (perhaps to see if a particular change actually improved the system), we can use AB tests. In AB tests, we play the same sentence synthesized by two different systems (an A and a B system). The human listeners choose which of the two utterances they like better. We do this for say 50 sentences (presented in random order) and compare the number of sentences preferred for each system. 16.7 Other Speech Tasks While we have focused on speech recognition and TTS in this chapter, there are a wide variety of speech-related tasks. wake word The task of wake word detection is to detect a word or short phrase, usually in order to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant. The goal with wake words is build the detection into small devices at the computing edge, to maintain privacy by transmitting the least amount of user speech to a cloud- based server. Thus wake word detectors need to be fast, small footprint software that can ﬁt into embedded devices. Wake word detectors usually use the same frontend feature extraction we saw for ASR, often followed by a whole-word classiﬁer. speaker diarization Speaker diarization is the task of determining ‘who spoke when’ in a long multi-speaker audio recording, marking the start and end of each speaker’s turns in the interaction. This can be useful for transcribing meetings, classroom speech, or medical interactions. Often diarization systems use voice activity detection (VAD) to ﬁnd segments of continuous speech, extract speaker embedding vectors, and cluster the vectors to group together segments likely from the same speaker. More recent work is investigating end-to-end algorithms to map directly from input speech to a sequence of speaker labels for each frame. 16.8 SUMMARY speaker recognition language identiﬁcation Speaker recognition, is the task of identifying a speaker. We generally distin- guish the subtasks of speaker veriﬁcation, where we make a binary decision (is this speaker X or not?), such as for security when accessing personal information over the telephone, and speaker identiﬁcation, where we make a one of N decision trying to match a speaker’s voice against a database of many speakers . These tasks are related to language identiﬁcation, in which we are given a waveﬁle and must identify which language is being spoken; this is useful for example for automatically directing callers to human operators that speak appropriate languages. 16.8 Summary This chapter introduced the fundamental algorithms of automatic speech recognition (ASR) and text-to-speech (TTS). The task of speech recognition (or speech-to-text) is to map acoustic wave- forms to sequences of graphemes. The input to a speech recognizer is a series of acoustic waves. that are sam- pled, quantized, and converted to a spectral representation like the log mel spectrum. Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attention- based models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete. ASR is evaluated using the Word Error Rate; the edit distance between the hypothesis and the gold transcription. TTS systems are also based on the encoder-decoder architecture. The en- coder maps letters to an encoding, which is consumed by the decoder which generates mel spectrogram output. A neural vocoder then reads the spectro- gram and generates waveforms. TTS systems require a ﬁrst pass of text normalization to deal with numbers and abbreviations and other non-standard words. TTS is evaluated by playing a sentence to human listeners and having them give a mean opinion score (MOS) or by doing AB tests. Bibliographical and Historical Notes ASR A number of speech recognition systems were developed by the late 1940s and early 1950s. An early Bell Labs system could recognize any of the 10 digits from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent stored patterns, one for each digit, each of which roughly represented the ﬁrst two vowel formants in the digit. They achieved 97%–99% accuracy by choosing the pat- tern that had the highest relative correlation coefﬁcient with the input. Fry (1959) and Denes (1959) built a phoneme recognizer at University College, London, that recognized four vowels and nine consonants based on a similar pattern-recognition principle. Fry and Denes’s system was the ﬁrst to use phoneme transition probabili- ties to constrain the recognizer. 23 24 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH warping The late 1960s and early 1970s produced a number of important paradigm shifts. First were a number of feature-extraction algorithms, including the efﬁcient fast Fourier transform (FFT) (Cooley and Tukey, 1965), the application of cepstral pro- cessing to speech (Oppenheim et al., 1968), and the development of LPC for speech coding (Atal and Hanauer, 1971). Second were a number of ways of handling warp- ing; stretching or shrinking the input signal to handle differences in speaking rate and segment length when matching against stored patterns. The natural algorithm for solving this problem was dynamic programming, and, as we saw in Appendix A, the algorithm was reinvented multiple times to address this problem. The ﬁrst applica- tion to speech processing was by Vintsyuk (1968), although his result was not picked up by other researchers, and was reinvented by Velichko and Zagoruyko (1970) and Sakoe and Chiba (1971) (and 1984). Soon afterward, Itakura (1975) combined this dynamic programming idea with the LPC coefﬁcients that had previously been used only for speech coding. The resulting system extracted LPC features from incoming words and used dynamic programming to match them against stored LPC templates. The non-probabilistic use of dynamic programming to match a template against in- coming speech is called dynamic time warping. dynamic time warping The third innovation of this period was the rise of the HMM. Hidden Markov models seem to have been applied to speech independently at two laboratories around 1972. One application arose from the work of statisticians, in particular Baum and colleagues at the Institute for Defense Analyses in Princeton who applied HMMs to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967). James Baker learned of this work and applied the algorithm to speech processing (Baker, 1975) during his graduate work at CMU. Independently, Frederick Jelinek and collaborators (drawing from their research in information-theoretical models inﬂuenced by the work of Shannon (1948)) applied HMMs to speech at the IBM Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was the decoding algorithm; Baker’s DRAGON system used Viterbi (dynamic program- ming) decoding, while the IBM system applied Jelinek’s stack decoding algorithm (Jelinek, 1969). Baker then joined the IBM group for a brief time before founding the speech-recognition company Dragon Systems. bakeoff The use of the HMM, with Gaussian Mixture Models (GMMs) as the phonetic component, slowly spread through the speech community, becoming the dominant paradigm by the 1990s. One cause was encouragement by ARPA, the Advanced Research Projects Agency of the U.S. Department of Defense. ARPA started a ﬁve-year program in 1971 to build 1000-word, constrained grammar, few speaker speech understanding (Klatt, 1977), and funded four competing systems of which Carnegie-Mellon University’s Harpy system (Lowerre, 1968), which used a simpli- ﬁed version of Baker’s HMM-based DRAGON system was the best of the tested sys- tems. ARPA (and then DARPA) funded a number of new speech research programs, beginning with 1000-word speaker-independent read-speech tasks like “Resource Management” (Price et al., 1988), recognition of sentences read from the Wall Street Journal (WSJ), Broadcast News domain (LDC 1998, Graff 1997) (transcription of actual news broadcasts, including quite difﬁcult passages such as on-the-street inter- views) and the Switchboard, CallHome, CallFriend, and Fisher domains (Godfrey et al. 1992, Cieri et al. 2004) (natural telephone conversations between friends or strangers). Each of the ARPA tasks involved an approximately annual bakeoff at which systems were evaluated against each other. The ARPA competitions resulted in wide-scale borrowing of techniques among labs since it was easy to see which ideas reduced errors the previous year, and the competitions were probably an im- hybrid Kaldi ESPnet BIBLIOGRAPHICAL AND HISTORICAL NOTES portant factor in the eventual spread of the HMM paradigm. By around 1990 neural alternatives to the HMM/GMM architecture for ASR arose, based on a number of earlier experiments with neural networks for phoneme recognition and other speech tasks. Architectures included the time-delay neural network (TDNN)—the ﬁrst use of convolutional networks for speech— (Waibel et al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid HMM/MLP architecture in which a feedforward neural network is trained as a pho- netic classiﬁer whose outputs are used as probability estimates for an HMM-based architecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and Bourlard 1995). While the hybrid systems showed performance close to the standard HMM/GMM models, the problem was speed: large hybrid models were too slow to train on the CPUs of that era. For example, the largest hybrid system, a feedforward network, was limited to a hidden layer of 4000 units, producing probabilities over only a few dozen monophones. Yet training this model still required the research group to de- sign special hardware boards to do vector processing (Morgan and Bourlard, 1995). A later analytic study showed the performance of such simple feedforward MLPs for ASR increases sharply with more than 1 hidden layer, even controlling for the total number of parameters (Maas et al., 2017). But the computational resources of the time were insufﬁcient for more layers. Over the next two decades a combination of Moore’s law and the rise of GPUs allowed deep neural networks with many layers. Performance was getting close to traditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mo- hamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed traditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia). Originally it seemed that unsupervised pretraining of the networks using a tech- nique like deep belief networks was important, but by 2013, it was clear that for hybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data and enough layers, although a few other components did improve performance: us- ing log mel features instead of MFCCs, using dropout, and using rectiﬁed linear units (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013). Meanwhile early work had proposed the CTC loss function by 2006 (Graves et al., 2006), and by 2012 the RNN-Transducer was deﬁned and applied to phone recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recog- nition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015), (Our de- with advances such as specialized beam search (Hannun et al., 2014). scription of CTC in the chapter draws on Hannun (2017), which we encourage the interested reader to follow). The encoder-decoder architecture was applied to speech at about the same time by two different groups, in the Listen Attend and Spell system of Chan et al. (2016) and the attention-based encoder decoder architecture of Chorowski et al. (2014) and Bahdanau et al. (2016). By 2018 Transformers were included in this encoder- decoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Trans- formers in encoder-architectures for ASR, TTS, and speech-to-speech translation. Popular toolkits for speech processing include Kaldi (Povey et al., 2011) and ESPnet (Watanabe et al. 2018, Hayashi et al. 2020). TTS As we noted at the beginning of the chapter, speech synthesis is one of the earliest ﬁelds of speech and language processing. The 18th century saw a number of physical models of the articulation process, including the von Kempelen model mentioned above, as well as the 1773 vowel model of Kratzenstein in Copenhagen 25 26 CHAPTER 16 AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH using organ pipes. The early 1950s saw the development of three early paradigms of waveform synthesis: formant synthesis, articulatory synthesis, and concatenative synthesis. Modern encoder-decoder systems are distant descendants of formant synthesiz- ers. Formant synthesizers originally were inspired by attempts to mimic human speech by generating artiﬁcial spectrograms. The Haskins Laboratories Pattern Playback Machine generated a sound wave by painting spectrogram patterns on a moving transparent belt and using reﬂectance to ﬁlter the harmonics of a wave- form (Cooper et al., 1951); other very early formant synthesizers include those of Lawrence (1953) and Fant (1951). Perhaps the most well-known of the formant synthesizers were the Klatt formant synthesizer and its successor systems, includ- ing the MITalk system (Allen et al., 1987) and the Klattalk software used in Digital Equipment Corporation’s DECtalk (Klatt, 1982). See Klatt (1975) for details. A second early paradigm, concatenative synthesis, seems to have been ﬁrst pro- posed by Harris (1953) at Bell Laboratories; he literally spliced together pieces of magnetic tape corresponding to phones. Soon afterwards, Peterson et al. (1958) pro- posed a theoretical model based on diphones, including a database with multiple copies of each diphone with differing prosody, each labeled with prosodic features including F0, stress, and duration, and the use of join costs based on F0 and formant distance between neighboring units. But such diphone synthesis models were not actually implemented until decades later (Dixon and Maxey 1968, Olive 1977). The 1980s and 1990s saw the invention of unit selection synthesis, based on larger units of non-uniform length and the use of a target cost, (Sagisaka 1988, Sagisaka et al. 1992, Hunt and Black 1996, Black and Taylor 1994, Syrdal et al. 2000). A third paradigm, articulatory synthesizers attempt to synthesize speech by modeling the physics of the vocal tract as an open tube. Representative models include Stevens et al. (1953), Flanagan et al. (1975), and Fant (1986). See Klatt (1975) and Flanagan (1972) for more details. Most early TTS systems used phonemes as input; development of the text anal- ysis components of TTS came somewhat later, drawing on NLP. Indeed the ﬁrst true text-to-speech system seems to have been the system of Umeda and Teranishi (Umeda et al. 1968, Teranishi and Umeda 1968, Umeda 1976), which included a parser that assigned prosodic boundaries, as well as accent and stress. Exercises 16.1 Analyze each of the errors in the incorrectly recognized transcription of “um the phone is I left the. . . ” on page 15. For each one, give your best guess as to whether you think it is caused by a problem in signal processing, pronun- ciation modeling, lexicon size, language model, or pruning in the decoding search. Allen, J., M. S. Hunnicut, and D. H. Klatt. 1987. From Text to Speech: The MITalk system. Cambridge University Press. Atal, B. S. and S. Hanauer. 1971. Speech analysis and syn- thesis by prediction of the speech wave. JASA, 50:637– 655. Bahdanau, D., J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio. 2016. End-to-end attention-based large vo- cabulary speech recognition. ICASSP. Baker, J. K. 1975. The DRAGON system – An overview. IEEE Transactions on Acoustics, Speech, and Signal Pro- cessing, ASSP-23(1):24–29. Baum, L. E. and J. A. Eagon. 1967. An inequality with appli- cations to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. Bulletin of the American Mathematical Society, 73(3):360–363. Baum, L. E. and T. Petrie. 1966. Statistical inference for probabilistic functions of ﬁnite-state Markov chains. An- nals of Mathematical Statistics, 37(6):1554–1563. Black, A. W. and P. Taylor. 1994. CHATR: A generic speech synthesis system. COLING. Bourlard, H. and N. Morgan. 1994. Connectionist Speech Recognition: A Hybrid Approach. Kluwer. Bu, H., J. Du, X. Na, B. Wu, and H. Zheng. 2017. AISHELL- 1: An open-source Mandarin speech corpus and a speech recognition baseline. O-COCOSDA Proceedings. Canavan, A., D. Graff, and G. Zipperlen. 1997. CALL- HOME American English speech LDC97S42. Linguistic Data Consortium. Chan, W., N. Jaitly, Q. Le, and O. Vinyals. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. ICASSP. Chorowski, J., D. Bahdanau, K. Cho, and Y. Bengio. 2014. End-to-end continuous speech recognition us- ing attention-based recurrent NN: First results. NeurIPS Deep Learning and Representation Learning Workshop. Cieri, C., D. Miller, and K. Walker. 2004. The Fisher cor- pus: A resource for the next generations of speech-to-text. LREC. Cooley, J. W. and J. W. Tukey. 1965. An algorithm for the machine calculation of complex Fourier series. Mathe- matics of Computation, 19(90):297–301. Cooper, F. S., A. M. Liberman, and J. M. Borst. 1951. The interconversion of audible and visible patterns as a basis for research in the perception of speech. Proceedings of the National Academy of Sciences, 37(5):318–325. Dahl, G. E., T. N. Sainath, and G. E. Hinton. 2013. Im- proving deep neural networks for LVCSR using rectiﬁed linear units and dropout. ICASSP. Dahl, G. E., D. Yu, L. Deng, and A. Acero. 2012. Context- dependent pre-trained deep neural networks for large- vocabulary speech recognition. IEEE Transactions on au- dio, speech, and language processing, 20(1):30–42. David, Jr., E. E. and O. G. Selfridge. 1962. Eyes and ears for computers. Proceedings of the IRE (Institute of Radio Engineers), 50:1093–1101. Davis, K. H., R. Biddulph, and S. Balashek. 1952. Automatic recognition of spoken digits. JASA, 24(6):637–642. Exercises Davis, S. and P. Mermelstein. 1980. Comparison of para- metric representations for monosyllabic word recognition IEEE Transactions in continuously spoken sentences. on Acoustics, Speech, and Signal Processing, 28(4):357– 366. Demberg, V. 2006. Letter-to-phoneme conversion for a Ger- man text-to-speech system. Diplomarbeit Nr. 47, Univer- sit¨at Stuttgart. Denes, P. 1959. The design and operation of the mechanical speech recognizer at University College London. Journal of the British Institution of Radio Engineers, 19(4):219– 234. Appears together with companion paper (Fry 1959). Deng, L., G. Hinton, and B. Kingsbury. 2013. New types of deep neural network learning for speech recognition and related applications: An overview. ICASSP. Dixon, N. and H. Maxey. 1968. Terminal analog synthesis of continuous speech using the diphone method of segment assembly. IEEE Transactions on Audio and Electroacous- tics, 16(1):40–50. Du Bois, J. W., W. L. Chafe, C. Meyer, S. A. Thompson, R. Englebretson, and N. Martey. 2005. Santa Barbara cor- pus of spoken American English, Parts 1-4. Philadelphia: Linguistic Data Consortium. Ebden, P. and R. Sproat. 2015. The Kestrel TTS text normalization system. Natural Language Engineering, 21(3):333. van Esch, D. and R. Sproat. 2018. An expanded taxonomy of semiotic classes for text normalization. INTERSPEECH. Fant, G. M. 1951. Speech communication research. Vetenskaps Akad. Stockholm, Sweden, 24:331–337. Fant, G. M. 1986. Glottal ﬂow: Models and interaction. Journal of Phonetics, 14:393–399. Flanagan, J. L. 1972. Speech Analysis, Synthesis, and Per- ception. Springer. Flanagan, J. L., K. Ishizaka, and K. L. Shipley. 1975. Syn- thesis of speech from a dynamic model of the vocal cords and vocal tract. The Bell System Technical Jour- nal, 54(3):485–506. Fry, D. B. 1959. Theoretical aspects of mechanical speech recognition. Journal of the British Institution of Radio Engineers, 19(4):211–218. Appears together with com- panion paper (Denes 1959). Gillick, L. and S. J. Cox. 1989. Some statistical issues in the comparison of speech recognition algorithms. ICASSP. Godfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCH- BOARD: Telephone speech corpus for research and de- velopment. ICASSP. Graff, D. 1997. The 1996 Broadcast News speech and language-model corpus. Proceedings DARPA Speech Recognition Workshop. Graves, A. 2012. Sequence transduction with recurrent neu- ral networks. ICASSP. Graves, A., S. Fern´andez, F. Gomez, and J. Schmidhuber. 2006. Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural net- works. ICML. Graves, A. and N. Jaitly. 2014. Towards end-to-end speech recognition with recurrent neural networks. ICML. 27 Ing. 28 Chapter 16 Automatic Speech Recognition and Text-to-Speech Graves, A., A.-r. Mohamed, and G. Hinton. 2013. Speech recognition with deep recurrent neural networks. ICASSP. Hannun, A. 2017. Sequence modeling with CTC. Distill, Liu, Y., P. Fung, Y. Yang, C. Cieri, S. Huang, and D. Graff. 2006. HKUST/MTS: A very large scale Mandarin tele- phone speech corpus. International Conference on Chi- nese Spoken Language Processing. 2(11). Hannun, A. Y., A. L. Maas, D. Jurafsky, and A. Y. Ng. 2014. First-pass large vocabulary continuous speech recogni- tion using bi-directional recurrent DNNs. ArXiv preprint arXiv:1408.2873. Lowerre, B. T. 1968. The Harpy Speech Recognition System. Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA. Maas, A., Z. Xie, D. Jurafsky, and A. Y. Ng. 2015. Lexicon- free conversational speech recognition with neural net- works. NAACL HLT. Harris, C. M. 1953. A study of the building blocks in speech. JASA, 25(5):962–969. Hayashi, T., R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, Y. Zhang, and X. Tan. 2020. ESPnet-TTS: Uniﬁed, reproducible, and inte- gratable open source end-to-end text-to-speech toolkit. ICASSP. Maas, A. L., A. Y. Hannun, and A. Y. Ng. 2013. Rectiﬁer nonlinearities improve neural network acoustic models. ICML. Maas, A. L., P. Qi, Z. Xie, A. Y. Hannun, C. T. Lengerich, D. Jurafsky, and A. Y. Ng. 2017. Building dnn acoustic models for large vocabulary speech recognition. Com- puter Speech & Language, 41:195–213. Hunt, A. J. and A. W. Black. 1996. Unit selection in a con- catenative speech synthesis system using a large speech database. ICASSP. Mohamed, A., G. E. Dahl, and G. E. Hinton. 2009. Deep Belief Networks for phone recognition. NIPS Workshop on Deep Learning for Speech Recognition and Related Applications. Itakura, F. 1975. Minimum prediction residual principle ap- plied to speech recognition. IEEE Transactions on Acous- tics, Speech, and Signal Processing, ASSP-32:67–72. Ito, K. and L. Johnson. 2017. The LJ speech dataset. https: Morgan, N. and H. Bourlard. 1990. Continuous speech recognition using multilayer perceptrons with hidden markov models. ICASSP. //keithito.com/LJ-Speech-Dataset/. Jaitly, N., P. Nguyen, A. Senior, and V. Vanhoucke. 2012. Application of pretrained deep neural networks to large vocabulary speech recognition. INTERSPEECH. Jelinek, F. 1969. A fast sequential decoding algorithm us- ing a stack. IBM Journal of Research and Development, 13:675–685. Morgan, N. and H. A. Bourlard. 1995. Neural networks for statistical recognition of continuous speech. Proceedings of the IEEE, 83(5):742–772. NIST. 2005. Speech recognition scoring toolkit (sctk) ver- sion 2.1. http://www.nist.gov/speech/tools/. NIST. 2007. Matched Pairs Sentence-Segment Word Error (MAPSSWE) Test. Jelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a linguistic statistical decoder for the recognition of contin- uous speech. IEEE Transactions on Information Theory, IT-21(3):250–256. Karita, S., N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe, T. Yoshimura, and W. Zhang. 2019. A comparative study on transformer vs RNN in speech applications. IEEE ASRU-19. Olive, J. P. 1977. Rule synthesis of speech from dyadic units. ICASSP77. van den Oord, A., S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. 2016. WaveNet: A Generative Model ISCA Workshop on Speech Synthesis for Raw Audio. Workshop. Oppenheim, A. V., R. W. Schafer, and T. G. J. Stockham. 1968. Nonlinear ﬁltering of multiplied and convolved sig- nals. Proceedings of the IEEE, 56(8):1264–1291. Kendall, T. and C. Farrington. 2020. The Corpus of Regional African American Language. Version 2020.05. Eugene, OR: The Online Resources for African American Lan- guage Project. http://oraal.uoregon.edu/coraal. Klatt, D. H. 1975. Voice onset time, friction, and aspiration in word-initial consonant clusters. Journal of Speech and Hearing Research, 18:686–706. Panayotov, V., G. Chen, D. Povey, and S. Khudanpur. 2015. Librispeech: an ASR corpus based on public domain au- dio books. ICASSP. Peterson, G. E., W. S.-Y. Wang, and E. Sivertsen. 1958. JASA, Segmentation techniques in speech synthesis. 30(8):739–742. Klatt, D. H. 1977. Review of the ARPA speech understand- ing project. JASA, 62(6):1345–1366. Klatt, D. H. 1982. The Klattalk text-to-speech conversion Povey, D., A. Ghoshal, G. Boulianne, L. Burget, O. Glem- bek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsk´y, G. Stemmer, and K. Vesel´y. 2011. The Kaldi speech recognition toolkit. ASRU. system. ICASSP. Lang, K. J., A. H. Waibel, and G. E. Hinton. 1990. A time-delay neural network architecture for isolated word recognition. Neural networks, 3(1):23–43. Price, P. J., W. Fisher, J. Bernstein, and D. Pallet. 1988. The DARPA 1000-word resource management database for continuous speech recognition. ICASSP. Pundak, G. and T. N. Sainath. 2016. Lower frame rate neural Lawrence, W. 1953. The synthesis of speech from signals which have a low information rate. In W. Jackson, editor, Communication Theory, pages 460–469. Butterworth. LDC. 1998. LDC Catalog: Hub4 project. Univer- sity of Pennsylvania. www.ldc.upenn.edu/Catalog/ LDC98S71.html. network acoustic models. INTERSPEECH. Robinson, T. and F. Fallside. 1991. A recurrent error prop- agation network speech recognition system. Computer Speech & Language, 5(3):259–274. Sagisaka, Y. 1988. Speech synthesis by rule using an optimal selection of non-uniform synthesis units. ICASSP. Sagisaka, Y., N. Kaiki, N. Iwahashi, and K. Mimura. 1992. Atr – ν-talk speech synthesis system. ICSLP. Sakoe, H. and S. Chiba. 1971. A dynamic programming approach to continuous speech recognition. Proceedings of the Seventh International Congress on Acoustics, vol- ume 3. Akad´emiai Kiad´o. Sakoe, H. and S. Chiba. 1984. Dynamic programming al- gorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech, and Signal Process- ing, ASSP-26(1):43–49. Shannon, C. E. 1948. A mathematical theory of commu- nication. Bell System Technical Journal, 27(3):379–423. Continued in the following volume. Shen, J., R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu. 2018. Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. ICASSP. Sproat, R., A. W. Black, S. F. Chen, S. Kumar, M. Ostendorf, and C. Richards. 2001. Normalization of non-standard words. Computer Speech & Language, 15(3):287–333. Sproat, R. and K. Gorman. 2018. A brief summary of the Kaggle text normalization challenge. Stevens, K. N., S. Kasowski, and G. M. Fant. 1953. An elec- trical analog of the vocal tract. JASA, 25(4):734–742. Stevens, S. S. and J. Volkmann. 1940. The relation of pitch to frequency: A revised scale. The American Journal of Psychology, 53(3):329–353. Stevens, S. S., J. Volkmann, and E. B. Newman. 1937. A scale for the measurement of the psychological magni- tude pitch. JASA, 8:185–190. Syrdal, A. K., C. W. Wightman, A. Conkie, Y. Stylianou, M. Beutnagel, J. Schroeter, V. Strom, and K.-S. Lee. 2000. Corpus-based techniques in the AT&T NEXTGEN synthesis system. ICSLP. Taylor, P. 2009. Text-to-Speech Synthesis. Cambridge Uni- versity Press. Teranishi, R. and N. Umeda. 1968. Use of pronouncing dic- tionary in speech synthesis experiments. 6th International Congress on Acoustics. Umeda, N. 1976. Linguistic rules for text-to-speech synthe- sis. Proceedings of the IEEE, 64(4):443–451. Umeda, N., E. Matui, T. Suzuki, and H. Omura. 1968. Syn- thesis of fairy tale using an analog vocal tract. 6th Inter- national Congress on Acoustics. Velichko, V. M. and N. G. Zagoruyko. 1970. Automatic recognition of 200 words. International Journal of Man- Machine Studies, 2:223–234. Vintsyuk, T. K. 1968. Speech discrimination by dynamic programming. Cybernetics, 4(1):52–57. Russian Kiber- netika 4(1):81-88. 1968. Waibel, A., T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang. 1989. Phoneme recognition using time-delay neu- IEEE transactions on Acoustics, Speech, ral networks. and Signal Processing, 37(3):328–339. Wang, Y., R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous. 2017. Tacotron: Towards end-to-end speech synthesis. INTER- SPEECH. Exercises Watanabe, S., T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai. 2018. ESPnet: End-to-end speech processing toolkit. INTERSPEECH. Zhang, H., R. Sproat, A. H. Ng, F. Stahlberg, X. Peng, K. Gorman, and B. Roark. 2019. Neural models of text normalization for speech applications. Computational Linguistics, 45(2):293–337. 29', 'labels': ['Chapter name'], 'scores': [0.8939090967178345]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "    \"inputs\": text,\n",
        "    \"parameters\": {\"candidate_labels\": [\"person\", \"NLP\", \"Examples\"]}\n",
        "\n",
        "})"
      ],
      "metadata": {
        "id": "D83p1qLJrBSZ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfIok0ZArPQo",
        "outputId": "47207830-3d8a-4ab5-e930-91613e545f12"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'error': 'Service Unavailable'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ZSC"
      ],
      "metadata": {
        "id": "-pYCy3NBxxFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "def json_to_csv(json_data, csv_file, delimiter=',', escapechar='\\\\'):\n",
        "    # Open the CSV file in write mode\n",
        "    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        # Create a CSV writer object\n",
        "        writer = csv.writer(csvfile, delimiter=delimiter, quotechar='\"', quoting=csv.QUOTE_MINIMAL, escapechar=escapechar)\n",
        "\n",
        "        # Write the header\n",
        "        writer.writerow(json_data[0].keys())\n",
        "\n",
        "        # Write the data rows\n",
        "        for item in json_data:\n",
        "            writer.writerow(item.values())\n"
      ],
      "metadata": {
        "id": "9Nx0E80m8WRJ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_to_csv([output], 'output.csv')\n",
        "\n",
        "# Alternatively, you can convert to TSV\n",
        "json_to_csv([output], 'output.tsv', delimiter='\\t')"
      ],
      "metadata": {
        "id": "fXNHq90Z8VgY"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1hf8wLIkvp1",
        "outputId": "d85a9d44-6f5b-4455-9fbd-a9ea8998dc23"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Collecting huggingface-hub>=0.7.0 (from evaluate)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, huggingface-hub, datasets, evaluate\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 evaluate-0.4.1 huggingface-hub-0.22.2 multiprocess-0.70.16 responses-0.18.0 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmEPi_zp4Nym",
        "outputId": "7758ec3a-77cb-4457-aa40-98832b1b9705"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.4.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=cc5a31ee11c4179cb11fa2e1bcf3271c728048737f87a63b2c9f23444305cbe2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQCWlwH26QGR",
        "outputId": "bbafd555-e2e5-4551-e402-a33dafc23adb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score\n"
      ],
      "metadata": {
        "id": "M4mjG2jT4RmU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = f1_score\n"
      ],
      "metadata": {
        "id": "kO_OQHqC4Ytm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Load pre-trained NER model\n",
        "model_name = \"dslim/bert-base-NER\"\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "# Tokenize and prepare data\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example[\"tokens\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "dataset = dataset.map(tokenize_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = []\n",
        "true_labels = []\n",
        "for example in dataset[\"train\"]:\n",
        "    inputs = tokenizer(example[\"tokens\"], truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    predicted_labels = outputs.logits.argmax(dim=-1).tolist()\n",
        "    true_labels.extend(example[\"ner_tags\"])\n",
        "    predictions.extend(predicted_labels)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels, predictions, average=\"micro\")  # Use micro-averaging for NER\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(true_labels, predictions)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "828b865bb610413aa6fbe90f1a2c855e",
            "b2ac5fc8470947a9806b3dcbfe406335",
            "9dc2af6bd5dd477da8dd764f0e6f474f",
            "86fa167015894523a83e51a6dc252d15",
            "44676d4167f84c7183f5aa6d83d6799c",
            "3ba5c18c2bdd4a75aaa44985bc6eb394",
            "f36cdeb6471b49c3942e2c378bfbcbaa",
            "a121d945a8d94f4fb67e3a2c3175d4fc",
            "c59e291e21464c03a0251c7cff66cb12",
            "a0f7a8d8c51142f682f18321eff4d830",
            "d4c29b4c253342498796f3ef440b8712"
          ]
        },
        "id": "tesoNUp06Uew",
        "outputId": "87cbd409-40b4-480f-cc33-772b15f5399a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "828b865bb610413aa6fbe90f1a2c855e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "# from evaluate import load\n",
        "\n",
        "# # Replace with your model name\n",
        "# model_name = \"dslim/bert-base-NER\"\n",
        "\n",
        "# # Replace with path to your validation dataset\n",
        "# validation_file = \"path/to/your/validation_data.json\"  # Adjust format if needed\n",
        "\n",
        "# # Load tokenizer and model\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# # Define the metric (F1 score)\n",
        "# metric = load(\"seqeval_f1\")\n",
        "\n",
        "# # Function to read data in a suitable format for evaluation\n",
        "# def read_data(file_path):\n",
        "#   # Implement your data reading logic here\n",
        "#   # This example assumes a simple JSON format with \"tokens\" and \"ner_tags\" lists\n",
        "#   with open(file_path, \"r\") as f:\n",
        "#     data = json.load(f)\n",
        "#   return [(record[\"tokens\"], record[\"ner_tags\"]) for record in data]\n",
        "\n",
        "# # Read validation data\n",
        "# validation_data = read_data(validation_file)\n",
        "\n",
        "# # Prepare predictions\n",
        "# predictions, references = [], []\n",
        "# for tokens, labels in validation_data:\n",
        "#   encoded_input = tokenizer(tokens, return_tensors=\"pt\")\n",
        "#   with torch.no_grad():\n",
        "#     outputs = model(**encoded_input)\n",
        "#   predictions.extend(torch.argmax(outputs.logits, dim=-1).squeeze(0).tolist())\n",
        "#   references.extend(labels)\n",
        "\n",
        "# # Calculate F1 score\n",
        "# results = metric.compute(predictions=predictions, references=references)\n",
        "# print(f\"F1 Score: {results['overall_f1']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "3ddn7vcYncFK",
        "outputId": "ca22ca7a-4e9c-41a0-ac01-9e4ba2684fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find a module script at /content/seqeval_f1/seqeval_f1.py. Module 'seqeval_f1' doesn't exist on the Hugging Face Hub either.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-fc497f05f8cf>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Define the metric (F1 score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"seqeval_f1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Function to read data in a suitable format for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \"\"\"\n\u001b[1;32m    747\u001b[0m     \u001b[0mdownload_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     evaluation_module = evaluation_module_factory(\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/loading.py\u001b[0m in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m    682\u001b[0m                 \u001b[0;34mf\"Couldn't find a module script at {relative_to_absolute_path(combined_path)}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 \u001b[0;34mf\"Module '{path}' doesn't exist on the Hugging Face Hub either.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a module script at /content/seqeval_f1/seqeval_f1.py. Module 'seqeval_f1' doesn't exist on the Hugging Face Hub either."
          ]
        }
      ]
    }
  ]
}